<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Cheyenne Acevedo, Christine Albano, Ashley Eustis" />


<title>Quantile Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus2.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TimeSeries.html">Time-series analysis</a>
    </li>
    <li>
      <a href="SEM.html">Structural Equation Models</a>
    </li>
    <li>
      <a href="SpatialAutocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="BayesianNetworks.html">Bayesian Networks</a>
    </li>
    <li>
      <a href="MixedEffects.html">Mixed Effects Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Quantile Regression</h1>
<h4 class="author"><em>Cheyenne Acevedo, Christine Albano, Ashley Eustis</em></h4>
<h4 class="date"><em>November 2018</em></h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a href="QuantileRegression.R">click here</a> for the companion R script for this lecture.</p>
<p>In this tutorial we provide an overview of quantile regression- a form of regression analysis that is used to estimate the conditional median or other quantiles of a response variable of interest.</p>
<pre class="r"><code># install.packages(&quot;quantreg&quot;)
# install.packages(&quot;Qtools&quot;)
# install.packages(&quot;ggplot2&quot;)
library(quantreg)
library(Qtools)
library(ggplot2)</code></pre>
<div id="when-to-use-quantile-regression-vs.ordinary-least-squares-regression" class="section level1">
<h1>When to use Quantile Regression vs. Ordinary Least Squares Regression</h1>
<pre class="r"><code># Simulate data
# generate some data to represent a dataset with a non-constant variance
x &lt;- rep(1:195, 2) # our independent variable
b_0 &lt;- 0 # intercept
b_1 &lt;- 1 # slope
sigma &lt;- 0.1 + 0.5*x # our non-constant variance
eps &lt;- rnorm(x, mean = , sd = sigma)
y &lt;- b_0 + b_1*x + eps


#plot the data to show heteroscedascity
plot(y~x)</code></pre>
<p><img src="QuantileRegression_files/figure-html/check%20data-1.png" width="672" /></p>
<p>From the scatterplot you can see that as x increases, y becomes more variable. This violates a key assumption of linear regression that there are normal errors with constant variance. Linear regression would have limited value for examining the relationship between our x and y, especially as x gets larger. If we did use linear regression here, we would see that it provides a good estimate for y when x is close to zero. However, as x increases, the mean of y given x becomes less meaningful.</p>
</div>
<div id="simple-example-of-the-rq-function-in-the-quantreg-package-using-simulated-data" class="section level1">
<h1>Simple example of the rq() function in the quantreg package using simulated data</h1>
<p>Reminder: Quantile regression models the relationship between the independent variable and the conditional mean for different quantiles of the dependent variable. Now we will use the quantile regression function <a href="https://www.rdocumentation.org/packages/quantreg/versions/2.0-2/topics/rq">rq</a>. It is important to note that the rq() function just produces coefficient estimates and the summary() function is needed to evaluate the precision of the estimates. This allows us to examine whether covariates are significant at that particular quantile.</p>
<pre class="r"><code># OLS regression
# The standard OLS (Ordinary Least Squares) model explains the relationship between independent variables and the conditional mean of the dependent variable.

# Run OLS regression and get the output:
OLSreg &lt;- lm(y~x)
summary(OLSreg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -227.405  -29.031    0.912   24.196  198.785 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.47716    6.09477  -0.571    0.569    
## x            1.03667    0.05393  19.223   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 59.95 on 388 degrees of freedom
## Multiple R-squared:  0.4878, Adjusted R-squared:  0.4865 
## F-statistic: 369.5 on 1 and 388 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># Now we can build some quantile regression models for different quantiles

# estimate the model at the median (this is also known as the 2nd quantile)
qr1 &lt;- rq(y ~ x) # our default tau is 0.50 
summary(qr1) # get the output</code></pre>
<pre><code>## 
## Call: rq(formula = y ~ x)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.62166     -0.22067  3.11693
## x            0.94944      0.87182  0.98767</code></pre>
<pre class="r"><code># Now lets fit a model for the remaining quantiles

# estimate the model for the first quartile
qr2 &lt;- rq(y ~ x, tau = 0.25) # tau = 0.25 is the first quantile
summary(qr2) # get the output</code></pre>
<pre><code>## 
## Call: rq(formula = y ~ x, tau = 0.25)
## 
## tau: [1] 0.25
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) -0.24758     -5.80011  1.18703
## x            0.66691      0.62217  0.74525</code></pre>
<pre class="r"><code># estimate the model for the third quartile
qr3 &lt;- rq( y~ x, tau = 0.75) # third quantile
summary(qr3) # get the output</code></pre>
<pre><code>## 
## Call: rq(formula = y ~ x, tau = 0.75)
## 
## tau: [1] 0.75
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) -0.85367     -2.00720  0.83762
## x            1.37785      1.33215  1.45218</code></pre>
<pre class="r"><code># estimate the model for the 95th quantile
Qreg95 &lt;- rq(y~x, tau=0.95)
summary(Qreg95, se = &quot;rank&quot;) # get the output</code></pre>
<pre><code>## 
## Call: rq(formula = y ~ x, tau = 0.95)
## 
## tau: [1] 0.95
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) -2.08063     -3.31586 -1.38067
## x            1.93970      1.87679  1.98568</code></pre>
</div>
<div id="plotting-results" class="section level1">
<h1>Plotting results</h1>
<pre class="r"><code># plot the data
plot(x,y, type = &quot;n&quot;)
points(x,y,cex=.5,col=&quot;blue&quot;)  # add the data points
taus &lt;- c(.05,.1,.25,.75,.9,.95) # choose the quantiles you want to plot
f &lt;- coef(rq((y)~(x),tau=taus)) # make your coefficents from your quantile model
xx &lt;- seq(min(x),max(x),190) # sequence the min and max values
yy &lt;- cbind(1,xx)%*%f 
for(i in 1:length(taus)){  # loop through your quantile values to make a line for each quantile
  lines(xx,yy[,i],col = &quot;gray&quot;) # this will show the slope of th eline for each quantile
}
abline(lm(y ~ x),col=&quot;red&quot;,lty = 2) # plot the mean Least Squares Estimate fit line
abline(rq(y ~ x), col=&quot;blue&quot;) # plot the meadian
legend(&quot;topleft&quot;,legend = c(&quot;mean (LSE) fit&quot;, &quot;median (LAE) fit&quot;), col = c(&quot;red&quot;,&quot;blue&quot;),lty = c(2,1)) # make the legend!</code></pre>
<p><img src="QuantileRegression_files/figure-html/rq%20plots-1.png" width="672" /></p>
<p>The above scatterplot from the simulated data shows the relationship between our dependent variable (y) and our independent variable (x). The gray lines are for the 5th, 10th, 50th, 75th, 90th and 95th quantile regression and the linear regression coefficient estimates. The solid blue line is the median fit. The red dashed line is the least squares estimate of the conditional mean function. The intercept estimate doesn’t change much, but the slopes for each quantile increase.</p>
</div>
<div id="plotting-model-coefficients-by-quantile" class="section level1">
<h1>Plotting Model Coefficients by Quantile</h1>
<pre class="r"><code>#Example of plotting of coefficients and their confidence bands
plot(summary(rq(y~x,tau = 1:49/50)))</code></pre>
<p><img src="QuantileRegression_files/figure-html/rq%20plot2-1.png" width="768" style="display: block; margin: auto;" /> The solid red line is the OLS regression coefficient and the dashed red lines are the confidence intervals around the OLS. Each black dot is the slope coefficient for the quantile indicated on the x-axis. The light gray area around the black dots is the confidence interval around the quantile. The lower quantiles have significant difference below the OLS and the upper quantiles have significant difference above the OLS.</p>
</div>
<div id="comparing-quantiles-with-anova" class="section level1">
<h1>Comparing quantiles with ANOVA</h1>
<p>If we want to test that the slopes were the same at two or more quantiles, then we need to use anova. Before you do this you need to have computed summary statistics for two or more quantile regression fits (We did this above).</p>
<p>Anova allows us to compute test statistics for two or more quantiles regression fits, and determines whether covariates are significant at particular quantiles.</p>
<pre class="r"><code># use anova to compare the 1st and 3rd quantiles
anova(qr2, qr3)</code></pre>
<pre><code>## Quantile Regression Analysis of Deviance Table
## 
## Model: y ~ x
## Joint Test of Equality of Slopes: tau in {  0.25 0.75  }
## 
##   Df Resid Df F value    Pr(&gt;F)    
## 1  1      779  141.69 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># compare the 1st, 2nd, and 3rd quantiles
anova(qr1, qr2, qr3)</code></pre>
<pre><code>## Quantile Regression Analysis of Deviance Table
## 
## Model: y ~ x
## Joint Test of Equality of Slopes: tau in {  0.5 0.25 0.75  }
## 
##   Df Resid Df F value    Pr(&gt;F)    
## 1  2     1168   72.57 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="comparing-nested-models-with-anova" class="section level1">
<h1>Comparing nested models with ANOVA</h1>
<pre class="r"><code># Let&#39;s use the Barro Data to explore quantile regression models with a larger dataset with more covarites. 
# This is a regression data set consisting of 161 observations on determinants of cross country GDP growth rates with 14 covariates.

data(barro) # load the Barro Data
# ?barro # for more info on this data

names(barro)</code></pre>
<pre><code>##  [1] &quot;y.net&quot;   &quot;lgdp2&quot;   &quot;mse2&quot;    &quot;fse2&quot;    &quot;fhe2&quot;    &quot;mhe2&quot;    &quot;lexp2&quot;  
##  [8] &quot;lintr2&quot;  &quot;gedy2&quot;   &quot;Iy2&quot;     &quot;gcony2&quot;  &quot;lblakp2&quot; &quot;pol2&quot;    &quot;ttrad2&quot;</code></pre>
<pre class="r"><code># our dependent variable is &quot;y.net&quot; the Annal Change Per Capita GDP
# our independent variables are everything else

# Now let&#39;s build a nested model where we compare the effect of Initial Per Capita GDP (lgdp2), Female Secondary Education (fse2), and Education/GDP (gedy2) with our dependent variable (Annual Change Per Capita GDP)

# estimate this model for the 2nd quantile (or the meadian) 
fit0 &lt;- rq(y.net ~  lgdp2 + fse2 + gedy2 , data = barro)  
summary(fit0) # the default is the meadian since we didn&#39;t specify tau</code></pre>
<pre><code>## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2, data = barro)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) -0.00747     -0.03006  0.04209
## lgdp2        0.00465     -0.00309  0.00725
## fse2         0.00159     -0.00162  0.00947
## gedy2       -0.36620     -0.55943  0.17807</code></pre>
<pre class="r"><code># Let&#39;s build another nested model where we compare the effect of Initial Per Capita GDP (lgdp2), Female Secondary Education (fse2), Education/GDP (gedy2), Investment GDP (Iy2), and Public Consumption/GDP (gcony2) with our dependent variable (Annual Change Per Capita GDP)

fit1 &lt;- rq(y.net ~  lgdp2 + fse2 + gedy2 + Iy2 + gcony2, data = barro) 
summary(fit1) # this model will also output the 2nd quantile</code></pre>
<pre><code>## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, data = barro)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.12501      0.05563  0.16746
## lgdp2       -0.01510     -0.02077 -0.00527
## fse2         0.00439      0.00189  0.00927
## gedy2       -0.19698     -0.50469  0.16207
## Iy2          0.14906      0.08542  0.20689
## gcony2      -0.14366     -0.26171 -0.02612</code></pre>
<pre class="r"><code># Now compare the two models
anova(fit1,fit0) </code></pre>
<pre><code>## Quantile Regression Analysis of Deviance Table
## 
## Model 1: y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2
## Model 2: y.net ~ lgdp2 + fse2 + gedy2
##   Df Resid Df F value    Pr(&gt;F)    
## 1  2      155  18.879 4.596e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="multiple-quantile-regression-example-barro" class="section level1">
<h1>Multiple quantile regression example (Barro)</h1>
<pre class="r"><code># Still using the barro data
# Let&#39;s examine different quantiles from our second nested model above


# estimate the model for the third quantile
fit2 &lt;- rq(y.net ~  lgdp2 + fse2 + gedy2 + Iy2 + gcony2, data = barro,tau=.75)
summary(fit2) # this model gives us the output for the 3rd quantile because we specified tau as 0.75</code></pre>
<pre><code>## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, tau = 0.75, 
##     data = barro)
## 
## tau: [1] 0.75
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.13103      0.09312  0.16521
## lgdp2       -0.01489     -0.01898 -0.01012
## fse2        -0.00026     -0.00148  0.00425
## gedy2        0.00400     -0.30995  0.45653
## Iy2          0.14527      0.10049  0.19170
## gcony2      -0.13505     -0.19123 -0.02922</code></pre>
<pre class="r"><code># estimate the model for the first quantile
fit3 &lt;- rq(y.net ~  lgdp2 + fse2 + gedy2 + Iy2 + gcony2, data = barro,tau=.25) 
summary(fit3) # this model outpus the 1st quantile</code></pre>
<pre><code>## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, tau = 0.25, 
##     data = barro)
## 
## tau: [1] 0.25
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.06086      0.02271  0.16148
## lgdp2       -0.00884     -0.02255 -0.00446
## fse2         0.00246      0.00081  0.00902
## gedy2       -0.14962     -0.54266  0.26080
## Iy2          0.15592      0.09664  0.21300
## gcony2      -0.15862     -0.26571 -0.03083</code></pre>
<pre class="r"><code># Test whether the coefficients are significantly different for the quantiles using ANOVA
# remember: use the anova function after you have computed test statistics for two or more quantile regression fits

# Now compare the two quantiles
anova(fit2,fit3) </code></pre>
<pre><code>## Quantile Regression Analysis of Deviance Table
## 
## Model: y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2
## Joint Test of Equality of Slopes: tau in {  0.75 0.25  }
## 
##   Df Resid Df F value Pr(&gt;F)  
## 1  5      317  2.7904 0.0175 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># lets compare the 1st, 2nd and 3rd quantiles
anova(fit1,fit2,fit3,joint=FALSE) </code></pre>
<pre><code>## Quantile Regression Analysis of Deviance Table
## 
## Model: y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2
## Tests of Equality of Distinct Slopes: tau in {  0.5 0.75 0.25  }
## 
##        Df Resid Df F value  Pr(&gt;F)  
## lgdp2   2      481  1.0656 0.34535  
## fse2    2      481  2.6398 0.07241 .
## gedy2   2      481  0.7862 0.45614  
## Iy2     2      481  0.0447 0.95632  
## gcony2  2      481  0.0653 0.93675  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># Alternatively, fitting can be done in one call:
fit &lt;- rq(y.net ~  lgdp2 + fse2 + gedy2 + Iy2 + gcony2, method = &quot;fn&quot;, tau = 1:4/5, data = barro)
summary(fit) # this looks that the 20th, 40th, 60th, and 80th percentiles</code></pre>
<pre><code>## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, tau = 1:4/5, 
##     data = barro, method = &quot;fn&quot;)
## 
## tau: [1] 0.2
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.08961      0.01263  0.18331
## lgdp2       -0.01353     -0.02193 -0.00442
## fse2         0.00640      0.00082  0.01099
## gedy2       -0.17913     -0.63151  0.27242
## Iy2          0.17935      0.07162  0.21062
## gcony2      -0.17463     -0.26228 -0.03154
## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, tau = 1:4/5, 
##     data = barro, method = &quot;fn&quot;)
## 
## tau: [1] 0.4
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.08563      0.05458  0.14135
## lgdp2       -0.01143     -0.02061 -0.00649
## fse2         0.00552     -0.00077  0.00669
## gedy2       -0.22571     -0.42905  0.27365
## Iy2          0.15339      0.09836  0.18022
## gcony2      -0.10467     -0.23439 -0.04114
## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, tau = 1:4/5, 
##     data = barro, method = &quot;fn&quot;)
## 
## tau: [1] 0.6
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.11733      0.07871  0.17083
## lgdp2       -0.01355     -0.02145 -0.00914
## fse2         0.00226      0.00095  0.00684
## gedy2       -0.13172     -0.43128  0.20007
## Iy2          0.14351      0.09458  0.18621
## gcony2      -0.12887     -0.23311 -0.03235
## 
## Call: rq(formula = y.net ~ lgdp2 + fse2 + gedy2 + Iy2 + gcony2, tau = 1:4/5, 
##     data = barro, method = &quot;fn&quot;)
## 
## tau: [1] 0.8
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  0.13637      0.10915  0.18545
## lgdp2       -0.01557     -0.02155 -0.01196
## fse2        -0.00017     -0.00255  0.00595
## gedy2        0.01005     -0.23406  0.41058
## Iy2          0.14504      0.08562  0.19427
## gcony2      -0.08853     -0.22945 -0.03840</code></pre>
<pre class="r"><code>plot(summary(fit)) # see what it looks like</code></pre>
<p><img src="QuantileRegression_files/figure-html/rq%20multiple%20regression-1.png" width="672" /></p>
</div>
<div id="alternatives-for-calculating-standard-errors" class="section level1">
<h1>Alternatives for calculating Standard Errors</h1>
<p>Standard errors and p-values for slope estimates are not usually generated automatically in the summary.rq function for “bigger” problems. Thus, you often need to specify that you want this output. There are several options, some of which are based on assumptions about data distribution, and others of which are not. Ultimately, Hao and Naiman 2007 recommend the bootstrap method as it does not require any a-priori assumptions of the data distributions.</p>
<div id="iid---conditional-distributions-are-independent-and-identically-distributed-iid." class="section level3">
<h3>iid - conditional distributions are independent and identically distributed (iid).</h3>
<p>Location shift model, where changes among the different QRs show up only in the intercept.</p>
<pre class="r"><code># iid - conditional distributions are independent and identically distributed (iid).

# generate data with constant variance 
x &lt;- seq(0,100,length.out = 100)        # independent variable
int &lt;- 6                                # true intercept of mean
slope &lt;- 0.1                            # true slope
set.seed(1)                             # make the next line reproducible
e &lt;- rnorm(100,mean = 0, sd = 5)        # normal random error with constant variance
y &lt;- int + slope*x + e                    # dependent variable
dat &lt;- data.frame(x,y)                  # make into datafame
plot(dat$x, dat$y)                      # plot data</code></pre>
<p><img src="QuantileRegression_files/figure-html/iid-1.png" width="672" /></p>
<pre class="r"><code># note that summary gives confidence intervals but not standard errors
summary(rq(y~x, tau=c(0.1, 0.5, 0.9)))</code></pre>
<pre><code>## 
## Call: rq(formula = y ~ x, tau = c(0.1, 0.5, 0.9))
## 
## tau: [1] 0.1
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  1.86238     -3.66174  2.83085
## x            0.07994      0.06152  0.16305
## 
## Call: rq(formula = y ~ x, tau = c(0.1, 0.5, 0.9))
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 6.92531      4.79222  9.18450 
## x           0.09297      0.05043  0.12938 
## 
## Call: rq(formula = y ~ x, tau = c(0.1, 0.5, 0.9))
## 
## tau: [1] 0.9
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 11.54926      9.71369 13.92503
## x            0.10533      0.07774  0.15515</code></pre>
<pre class="r"><code># add SE option to statement
#iid - assumes iid distribution
plot(summary(rq(y~x, tau=c(0.1, 0.5, 0.9)), se=&quot;iid&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/iid-2.png" width="672" /></p>
<pre class="r"><code>#nid - assumes non-identical conditional distribution
plot(summary(rq(y~x, tau=c(0.1, 0.5, 0.9)), se=&quot;nid&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/iid-3.png" width="672" /></p>
<pre class="r"><code>#boot - no distribution assumption. Note that this is most similar to the iid, which is the appropriate choice for data with this distribution
plot(summary(rq(y~x, tau=c(0.1, 0.5, 0.9)), se=&quot;boot&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/iid-4.png" width="672" /></p>
</div>
<div id="nid---non-identical-conditional-distributions-nid." class="section level3">
<h3>nid - non-identical conditional distributions (nid.):</h3>
<p>the error densities have the same shape but differ from one another in dispersion.This is the location and scale shift model, where QRs vary in both intercepts and slopes. Use for heteroscedastic data</p>
<pre class="r"><code># generate data with non-constant variance (adapted from https://data.library.virginia.edu/getting-started-with-quantile-regression/)
#generate data with nonconstant variance
x &lt;- seq(0,100,length.out = 100)        # independent variable
varfunc &lt;- 0.1 + 0.1*x                  # non-constant variance
int &lt;- 6                                # true intercept
slope &lt;- 0.1                            # true slope
set.seed(1)                             # make the next line reproducible
e &lt;- rnorm(100,mean = 0, sd = varfunc)  # normal random error with non-constant variance
y &lt;- int + slope*x + e                    # dependent variable
dat &lt;- data.frame(x,y)                  # make into datafame
plot(dat$x, dat$y)                      # plot data</code></pre>
<p><img src="QuantileRegression_files/figure-html/nid-1.png" width="672" /></p>
<pre class="r"><code># add SE option to statement
# note that summary gives confidence intervals but not standard errors
summary(rq(y~x, tau=c(0.1, 0.5, 0.9)))</code></pre>
<pre><code>## 
## Call: rq(formula = y ~ x, tau = c(0.1, 0.5, 0.9))
## 
## tau: [1] 0.1
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept)  5.99747      3.86295  6.15738
## x           -0.02367     -0.02906  0.03772
## 
## Call: rq(formula = y ~ x, tau = c(0.1, 0.5, 0.9))
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 6.02969      5.70159  6.52311 
## x           0.10715      0.07854  0.13329 
## 
## Call: rq(formula = y ~ x, tau = c(0.1, 0.5, 0.9))
## 
## tau: [1] 0.9
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 5.97238      5.89246  6.63652 
## x           0.22240      0.21226  0.25554</code></pre>
<pre class="r"><code># add SE option to statement
#iid - assumes iid distribution
plot(summary(rq(y~x, tau=c(0.1, 0.5, 0.9)), se=&quot;iid&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/nid-2.png" width="672" /></p>
<pre class="r"><code>#nid - assumes non-identical conditional distribution
plot(summary(rq(y~x, tau=c(0.1, 0.5, 0.9)), se=&quot;nid&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/nid-3.png" width="672" /></p>
<pre class="r"><code>#boot - no distribution assumption. Note that this is most similar to the nid, which is the appropriate choice for data with this distribution
plot(summary(rq(y~x, tau=c(0.1, 0.5, 0.9)), se=&quot;boot&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/nid-4.png" width="672" /></p>
</div>
</div>
<div id="evaluating-goodness-of-fit" class="section level1">
<h1>Evaluating Goodness-of-Fit</h1>
<p>Because quantile regression models are fit based on minimizing the absolute values of weighted residuals, as opposed to minimizing the sum of squared errors, the R2 is not an applicable goodness-of-fit metric. A couple of methods are available for evaluating goodness-of-fit, but these are not tests, per-se. Rather, they provide a means for comparing model fit among nested models. Of note, because different conditional quantile models are modeling different responses, these tests cannot be used to compare GOF among quantiles.</p>
<pre class="r"><code>####
####  Evaluating Goodness-of-Fit
####

# generate data with non-constant variance (adapted from https://data.library.virginia.edu/getting-started-with-quantile-regression/)

x &lt;- seq(0,100,length.out = 100)        # independent variable
varfunc &lt;- 0.1 + 0.1*x                  # non-constant variance
int &lt;- 6                                # true intercept
slope &lt;- 0.1                            # true slope
set.seed(1)                             # make the next line reproducible
e &lt;- rnorm(100,mean = 0, sd = varfunc)  # normal random error with non-constant variance
y &lt;- int + slope*x + e                    # dependent variable
dat &lt;- data.frame(x,y)                  # make into datafame
plot(dat$x, dat$y)                      # plot data</code></pre>
<p><img src="QuantileRegression_files/figure-html/gof-1.png" width="672" /></p>
<div id="r1-metric" class="section level2">
<h2>R1 Metric</h2>
<p>The R1 metric is calculated as one minus the ratio of the sum of absolute weighted residuals for the fitted model to the sum of absolute weighted residuals for the null model. As with an R2, values range between 0 and 1. However, the R1 metric will always be smaller than the R2 metric.</p>
<p>Koenker, R., Machado, J.A.F. 1999. Goodness of fit and related inference processes for quantile regression. Journal of the American Statistical Association. 94(448):1296-1310.</p>
<pre class="r"><code>####  R1 metric

# fitted model for median quantile
fitmod5 &lt;- rq(y ~ x, tau = .5, data = dat)


# null model for median quantile
nullmod5&lt;-rq(y~1, data=dat, tau=0.5)

#weighted residuals
nullres5&lt;-resid(nullmod5)
fitres5 &lt;- resid(fitmod5)

# calculate R1 metric as 1 minus ratio of summed absolute residuals from fitted and null model
1-(sum(abs(fitres5))/sum(abs(nullres5)))</code></pre>
<pre><code>## [1] 0.1842132</code></pre>
<pre class="r"><code># or more simply, pull these sums from the model object
1 - fitmod5$rho/nullmod5$rho</code></pre>
<pre><code>## [1] 0.1842132</code></pre>
<pre class="r"><code># compare to R2
summary(lm(y ~ x, data = dat))$r.squared</code></pre>
<pre><code>## [1] 0.2520498</code></pre>
<pre class="r"><code># plot data with mean and median regression lines. note in this case they are the same line but goodness of fit differs (QR &lt; OLS)
plot(dat$x, dat$y)
abline(fitmod5, col=&quot;red&quot;)
abline(lm(y ~ x, data = dat), col=&quot;blue&quot;)</code></pre>
<p><img src="QuantileRegression_files/figure-html/r1metric-1.png" width="672" /></p>
</div>
<div id="difference-in-aic-from-null-model" class="section level2">
<h2>Difference in AIC from null model</h2>
<p>Another option for assessing relative model goodness-of-fit is to compare AIC values of the fitted model to the null model. Typically, an AIC difference &gt; 10 suggests the model has some explanatory power.</p>
<pre class="r"><code>####  difference in AIC

# delta AIC method for QR model
AIC(nullmod5)[1]-AIC(fitmod5)[1]</code></pre>
<pre><code>## [1] 38.72044</code></pre>
<pre class="r"><code>#compare to OLS
AIC(lm(y ~ 1, data = dat))[1]-AIC(lm(y ~ x, data = dat))[1]</code></pre>
<pre><code>## [1] 27.04189</code></pre>
<pre class="r"><code># notice the dAIC for the QR model vs. null is larger than that for the OLS vs. null</code></pre>
<div id="a-side-note-on-likelihood-in-quantile-regression" class="section level3">
<h3>A side note on likelihood in quantile regression</h3>
<p>From Koeneker: <a href="http://www.econ.uiuc.edu/~roger/research/rq/FAQ" class="uri">http://www.econ.uiuc.edu/~roger/research/rq/FAQ</a></p>
<p>FAQ question: In quantreg of R package, rq.object can compute AIC (Akaile’s An Information Criterion); AIC=-2L/n+2k/n ,where L is log-likelihood, k represents the number of parameters in the model ,n is the number of observations. We know that likelihood is based on distribution, but quantile regression can not assume specific distribution, thus….Why can an rq.object compute AIC?</p>
<p>FAQ answer: This is discussed many places in the literature. Yes, it is not a “real” likelihood, only a pseudo likelihood, but then the Gaussian likelihood is usually not a real likelihood either most of the time….</p>
</div>
</div>
<div id="cumulative-sum-method" class="section level2">
<h2>Cumulative sum method</h2>
<p>A third goodness of fit test was proposed that is based on the cumulative sum of the gradient vector and is available in the Qtools package, but it is not particularly well-documented. The aim is to minimize the test statistic, but it is not clear what test statistic qualifies as a ‘good’ fit. This test is also computationally expensive.</p>
<p>He XM, Zhu LX. A lack-of-fit test for quantile regression. Journal of the American Statistical Association (2003);98:1013-1022.</p>
<pre class="r"><code># cumulative sum of the gradient vector (eigenvector) (Qtools package)

# specify QR model object, significance level, B= number of montecarlo samples
GOFTest(fitmod5, alpha = 0.05, B = 1000)</code></pre>
<pre><code>## Goodness-of-fit test for quantile regression based on the cusum process 
## Quantile 0.5: Test statistic = 0.0323; p-value = 0.287</code></pre>
<pre class="r"><code>#try with dataset with smaller variance
x &lt;- seq(0,100,length.out = 100)        # independent variable
varfunc &lt;- 0.1 + 0.01*x                  # non-constant variance
int &lt;- 6                                # true intercept
slope &lt;- 0.1                            # true slope
set.seed(1)                             # make the next line reproducible
e &lt;- rnorm(100 ,mean = 0, sd = varfunc)  # normal random error with non-constant variance
y &lt;- int + slope*x + e                    # dependent variable
dat &lt;- data.frame(x,y)                  # make into datafame
plot(x,y)</code></pre>
<p><img src="QuantileRegression_files/figure-html/cumsum-1.png" width="672" /></p>
<pre class="r"><code>fitmod5b &lt;- rq(y ~ x, tau = .5, data = dat)
GOFTest(fitmod5b, alpha = 0.05, B = 1000)</code></pre>
<pre><code>## Goodness-of-fit test for quantile regression based on the cusum process 
## Quantile 0.5: Test statistic = 0.0139; p-value = 0.716</code></pre>
<pre class="r"><code>#try with dataset with smaller variance and larger sample size
x &lt;- seq(0,1000,length.out = 1000)        # independent variable
varfunc &lt;- 0.1 + 0.01*x                  # non-constant variance
int &lt;- 6                                # true intercept
slope &lt;- 0.1                            # true slope
set.seed(1)                             # make the next line reproducible
e &lt;- rnorm(1000 ,mean = 0, sd = varfunc)  # normal random error with non-constant variance
y &lt;- int + slope*x + e                    # dependent variable
dat &lt;- data.frame(x,y)                  # make into datafame
plot(x,y)</code></pre>
<p><img src="QuantileRegression_files/figure-html/cumsum-2.png" width="672" /></p>
<pre class="r"><code>fitmod5c &lt;- rq(y ~ x, tau = .5, data = dat)
GOFTest(fitmod5c, alpha = 0.05, B = 1000)</code></pre>
<pre><code>## Goodness-of-fit test for quantile regression based on the cusum process 
## Quantile 0.5: Test statistic = 0.068; p-value = 0.076</code></pre>
</div>
</div>
<div id="plotting-quantile-regression-in-ggplot" class="section level1">
<h1>Plotting Quantile Regression in ggplot</h1>
<pre class="r"><code>####
#### Plotting Quantile Regression in ggplot
####

data(engel)
m &lt;- ggplot(engel, aes(income, foodexp)) + geom_point()
m + geom_quantile()</code></pre>
<p><img src="QuantileRegression_files/figure-html/other%20viz-1.png" width="672" /></p>
<pre class="r"><code>m + geom_quantile(quantiles = 0.5)</code></pre>
<p><img src="QuantileRegression_files/figure-html/other%20viz-2.png" width="672" /></p>
<pre class="r"><code>q10 &lt;- seq(0.05, 0.95, by = 0.05)
m + geom_quantile(aes(colour = ..quantile..),quantiles = q10)</code></pre>
<p><img src="QuantileRegression_files/figure-html/other%20viz-3.png" width="672" /></p>
</div>
<div id="other-types-of-quantile-regression" class="section level1">
<h1>Other Types of Quantile Regression</h1>
<div id="non-linear-quantile-regression" class="section level2">
<h2>Non-linear Quantile Regression</h2>
<pre class="r"><code>#  Nonlinear Quantile Regression  (adapted from: https://rpubs.com/MarkusLoew/10676)

data(Mammals) #Observations on the maximal running speed of mammal species and their body mass.
attach(Mammals)
x &lt;- log(weight)
y &lt;- log(speed)
mamdat&lt;-as.data.frame(cbind(x,y))

# plot data
plot(x,y, xlab=&quot;Weight in log(Kg)&quot;, ylab=&quot;Speed in log(Km/hour)&quot;)</code></pre>
<p><img src="QuantileRegression_files/figure-html/mammaldata-1.png" width="672" /></p>
<p>Start with OLS non-linear regression</p>
<pre class="r"><code># start with just a simple quadratic approach
# y ~ a * x^2 + b * x  + c

my.equation &lt;- y ~ a * x^2 + b * x  + c

# fit the equation to the data via &quot;non-linear least squares&quot;
# choose some good starting values for parameter estimation
nls.fit &lt;- nls(my.equation,
               data = mamdat,
               start = list(a = 2, b = 3, c = 5))

# look at the result
summary(nls.fit)</code></pre>
<pre><code>## 
## Formula: y ~ a * x^2 + b * x + c
## 
## Parameters:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## a -0.024342   0.005389  -4.517 1.66e-05 ***
## b  0.252607   0.026214   9.636 4.37e-16 ***
## c  3.341114   0.090890  36.760  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6343 on 104 degrees of freedom
## 
## Number of iterations to convergence: 1 
## Achieved convergence tolerance: 1.354e-07</code></pre>
<pre class="r"><code># create a dummy range of that we use to predict speed from our fitted model
predict_range &lt;- data.frame(x = seq(-5, 9, length = 250))

# calculate for each x-range value the corresponding y-range
my.line &lt;- within(predict_range, y &lt;- predict(nls.fit, newdata = predict_range))

# add the line to the existing graph
# This line represents the &quot;mean&quot; fit, no quantile regression involved
# plot data
plot(x,y, xlab=&quot;Weight in log(Kg)&quot;, ylab=&quot;Speed in log(Km/hour)&quot;)
lines(y ~ x, data = my.line, col = &quot;red&quot;)</code></pre>
<p><img src="QuantileRegression_files/figure-html/nlinr-1.png" width="672" /></p>
<p>Now try with quantile regression</p>
<pre class="r"><code># Non-linear quantile regression
# aiming for the upper 99% quantile
my.rq &lt;- nlrq(my.equation,
           data = mamdat,
           start = list(a = 2, b = 2, c = 5),
           tau = .99)
summary(my.rq)</code></pre>
<pre><code>## 
## Call: nlrq(formula = my.equation, data = mamdat, start = list(a = 2, 
##     b = 2, c = 5), tau = 0.99, control = list(maxiter = 100, 
##     k = 2, InitialStepSize = 1, big = 1e+20, eps = 1e-07, beta = 0.97), 
##     trace = FALSE)
## 
## tau: [1] 0.99
## 
## Coefficients:
##   Value    Std. Error t value  Pr(&gt;|t|)
## a -0.03159  0.00605   -5.22479  0.00000
## b  0.18846  0.04341    4.34100  0.00003
## c  4.45252  0.14018   31.76311  0.00000</code></pre>
<pre class="r"><code># calculating the values from the model
my.line95 &lt;- within(predict_range, 
             y &lt;- predict(my.rq, 
                            newdata = predict_range))
plot(x,y, xlab=&quot;Weight in log(Kg)&quot;, ylab=&quot;Speed in log(Km/hour)&quot;)
lines(y ~ x, data = my.line, col = &quot;red&quot;)
lines(y ~ x, data = my.line95, col = &quot;blue&quot;)</code></pre>
<p><img src="QuantileRegression_files/figure-html/nlinrq-1.png" width="672" /></p>
</div>
<div id="count-data" class="section level2">
<h2>Count Data</h2>
<p>Need to create an artificial continuous distribution that approximates the discrete count data.</p>
<p>The classic way to do this is to artificial smooth (jitter) the data by adding random error</p>
<p>Machado JAF, Santos Silva JMC. Quantiles for counts. Journal of the American Statistical Association. 2005;100(472):1226-37.</p>
<p>In this case, the response is log-transformed to achieve the continuous distribution</p>
<pre class="r"><code># Esterase data
data(esterase)

# Fit quantiles 0.25 and 0.75 - number of dithered samples
fit1 &lt;- rq.counts(Count ~ Esterase, tau = 0.25, data = esterase)
coef(fit1)</code></pre>
<pre><code>## (Intercept)    Esterase 
##  4.19869934  0.05324318</code></pre>
<pre class="r"><code>fit2 &lt;- rq.counts(Count ~ Esterase, tau = 0.75, data = esterase)
coef(fit2)</code></pre>
<pre><code>## (Intercept)    Esterase 
##  4.85205309  0.04943474</code></pre>
<pre class="r"><code># Plot
with(esterase, plot(Count ~ Esterase))
lines(esterase$Esterase, fit1$fitted.values, col = &quot;blue&quot;)
lines(esterase$Esterase, fit2$fitted.values, col = &quot;red&quot;)
legend(8, 1000, lty = c(1,1), col = c(&quot;blue&quot;, &quot;red&quot;), legend = c(&quot;tau = 0.25&quot;,&quot;tau = 0.75&quot;))</code></pre>
<p><img src="QuantileRegression_files/figure-html/count-1.png" width="672" /></p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<div id="advantages-of-quantile-regression" class="section level2">
<h2>Advantages of Quantile Regression</h2>
<ul>
<li><p>allows a comprehensive look at the data – identify limiting factors, floors, ceilings</p></li>
<li><p>robust to outliers</p></li>
<li><p>is distribution agnostic</p></li>
</ul>
</div>
<div id="disadvantages-of-quantile-regression" class="section level2">
<h2>Disadvantages of Quantile Regression:</h2>
<ul>
<li><p>computationally less-efficient than OLS</p></li>
<li><p>needs sufficient data</p></li>
<li><p>non-traditional goodness-of-fit metrics make communication of model fit challenging</p></li>
</ul>
</div>
</div>
<div id="resources" class="section level1">
<h1>Resources</h1>
<ul>
<li><p>Quantreg package: <a href="https://cran.r-project.org/web/packages/quantreg/quantreg.pdf" class="uri">https://cran.r-project.org/web/packages/quantreg/quantreg.pdf</a></p></li>
<li><p>A gentle introduction to quantile regression (Cade and Noon 2003): <a href="https://pdfs.semanticscholar.org/8c0d/c7f7362479553ac7747b66933a425b0840c8.pdf" class="uri">https://pdfs.semanticscholar.org/8c0d/c7f7362479553ac7747b66933a425b0840c8.pdf</a></p></li>
<li><p>Quantile Regression: Theory and Applications (Davino et al.) <a href="http://domenicovistocco.it/conferma-associato/pubblicazioni/7-davino-furno-vistocco-bookQR-wiley-2013.pdf" class="uri">http://domenicovistocco.it/conferma-associato/pubblicazioni/7-davino-furno-vistocco-bookQR-wiley-2013.pdf</a></p></li>
<li><p>Quantile Regression (Hao and Naiman 2007) <a href="http://methods.sagepub.com/book/quantile-regression" class="uri">http://methods.sagepub.com/book/quantile-regression</a></p></li>
</ul>
</div>
<div id="advanced-topicspackages-in-r" class="section level1">
<h1>Advanced Topics/Packages in R</h1>
<ul>
<li><p>Mixed Effects Modeling : <a href="https://cran.r-project.org/web/packages/lqmm/lqmm.pdf" class="uri">https://cran.r-project.org/web/packages/lqmm/lqmm.pdf</a></p></li>
<li><p>Random Forests : <a href="https://cran.r-project.org/web/packages/quantregForest/quantregForest.pdf" class="uri">https://cran.r-project.org/web/packages/quantregForest/quantregForest.pdf</a></p></li>
<li><p>Bayesian Quantile Regression : <a href="https://cran.r-project.org/web/packages/bayesQR/bayesQR.pdf" class="uri">https://cran.r-project.org/web/packages/bayesQR/bayesQR.pdf</a></p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
