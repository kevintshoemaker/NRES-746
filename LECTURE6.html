<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-27" />

<title>Bayesian Analysis #1: Concepts</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="Generalized Additive Models (GAMs).pdf">GAMs</a>
    </li>
    <li>
      <a href="RMarkdown_FigureDemo.html">Publication-quality figures in R</a>
    </li>
    <li>
      <a href="Bayesian Networks.pptx">Bayesian Networks</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Bayesian Analysis #1: Concepts</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 27, 2016</em></h4>

</div>


<p><strong>Bayesian analysis</strong> is likelihood-based, and follows naturally from our previous discussions. The difference is that we are no longer interested in the maximum likelhood estimate and the properties of maximum likelhood estimators. We are now interested in using the likelihood function to update our degree of belief in all possible parameter sets. Effectively, we want a <em>joint probability distribution</em> that represents our degree of belief in all possible sets of parameter values after we have observed some relevant data.</p>
<div id="play-with-binomialbeta-conjugate-prior" class="section level3">
<h3>Play with binomial/beta (conjugate prior)</h3>
<p>Let’s imagine we know <em>N</em> (<em>N</em>, the number of independent trials, is fixed), but we want to estimate <em>p</em>. Let’s assume we have no prior information, so that any value of <em>p</em> is equally likely.</p>
<div id="set-the-prior" class="section level4">
<h4>Set the prior</h4>
<p>To set the prior, let’s assume a uniform distribution between 0 and 1:</p>
<pre class="r"><code>curve(dunif(x),ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>#hist(runif(10000),freq=F,ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p>An alternative way to specify this uniform (flat) prior is to use the beta distribution, with both shape parameters set to 1</p>
<pre class="r"><code>curve(dbeta(x,1,1),ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>#hist(rbeta(10000,1,1),freq=F,ylim=c(0,2),col=&quot;red&quot;)</code></pre>
</div>
</div>
<div id="conjugate-prior" class="section level3">
<h3>Conjugate prior</h3>
<p>Why choose the beta distribution here? The answer is that the beta is the <strong>conjugate prior</strong> for the <em>p</em> parameter in the binomial distribution. This makes Bayesian estimation easy and straightforward, as we will see!</p>
<div id="definition-conjugate-prior" class="section level4">
<h4>Definition: conjugate prior</h4>
<p>A conjugate prior is a distribution that matches the data-generating model- that is, it has the same form as the likelihood function. In this way, the distributional form of the posterior distribution for a parameter is the same as the prior distribution for that parameter (although the shape of the distribution will change). We will come back to this!</p>
</div>
</div>
<div id="worked-example" class="section level3">
<h3>Worked example</h3>
<p>Let’s work through an example. Let’s imagine the same frog-call survey we have imagined before. We know the site is occupied. After visiting the site 10 times, we detected the frog (heard its call) 3 times out of 10 total visits. We are interested in determining the detection probability.</p>
<p>We know the likelihood of the data across parameter space. So now we have both a prior distribution for our parameter of interest, as well as a likelihood surface.</p>
<pre class="r"><code>data = 3
param.space &lt;- seq(0,1,by=0.001)
likelihood &lt;- dbinom(data,size=10,prob=param.space)
par(mai=c(1,1,0,1))
curve(dbeta(x,1,1),ylim=c(0,2),col=&quot;blue&quot;,ylab=&quot;Probability density&quot;,xlab=&quot;param.space&quot;)
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=2)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Recall that the likelihood curve is NOT a probability distribution. It does not generally sum to 1! In Bayesian analyses, we translate the likelihood to a probability distribution using Bayes rule!!</p>
<p><span class="math inline">\(Prob(Model|Data) = \frac{Prob(Data|Model)\cdot Prob(Model))}{Prob(Data)}\)</span></p>
<p>The likelihood is just the <span class="math inline">\(Prob(Data|Model)\)</span> term…</p>
<p>What is the probability of the data? Well, it’s just the sum of the probability of the data across all possible parameter values (parameter space). Really, <span class="math inline">\(Prob(Data)\)</span> can be seen as a normalizing constant that is used to convert the numerator of Bayes rule into a probability distribution. As always, let’s do it first by brute force…</p>
<pre class="r"><code>prior &lt;- dbeta(param.space,shape1=1,shape2=1)
#prior

## weight the data likelihood by the prior

weighted.likelihood &lt;- likelihood*prior      # Numerator for Bayes rule

## compute normalization constant

normalization.constant &lt;- sum(weighted.likelihood)

## Posterior!!
posterior &lt;- weighted.likelihood/normalization.constant   # this is Bayes&#39; rule!

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,5),type=&quot;l&quot;,lwd=1,lty=2,col=&quot;blue&quot;,ylab=&quot;Probability Density&quot;,xlab=&quot;param.space&quot;)
points(param.space,posterior*length(param.space),type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2,lty=1)  # convert posterior to probability density
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Notice that the shape of the posterior looks a lot like the shape of the likelhood surface. What this says to us is that the prior has been <em>overwhelmed</em> by the information content of the data (as summarized by the likelihood surface).</p>
<p>What if we have a more informative prior?</p>
<pre class="r"><code>prior &lt;- dbeta(param.space,shape1=15,shape2=5)
#prior

## weight the data likelihood by the prior

weighted.likelihood &lt;- likelihood*prior

## compute normalization constant

normalization.constant &lt;- sum(weighted.likelihood)

## Posterior!!

posterior &lt;- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,5),type=&quot;l&quot;,lwd=1,lty=2,col=&quot;blue&quot;,ylab=&quot;Probability Density&quot;,xlab=&quot;param.space&quot;)
points(param.space,posterior*length(param.space),type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2,lty=1)
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>What does this tell us?</p>
<p>What about if we have more data? Let’s imagine we visited 10 occupied ponds and observed the following data:</p>
<blockquote>
<p>3, 1, 6, 2, 3, 2, 6, 1, 3, 3</p>
</blockquote>
<pre class="r"><code>moredata &lt;- c(3, 1, 6, 2, 3, 2, 6, 1, 3, 3)

## prior
prior &lt;- dbeta(param.space,shape1=15,shape2=5)

## likelihood
likelihood &lt;- sapply(param.space,function(t) prod(dbinom(moredata,size=10,prob=t)))

## weight the data likelihood by the prior
weighted.likelihood &lt;- likelihood*prior

## compute normalization constant

normalization.constant &lt;- sum(weighted.likelihood)

## Posterior!!

posterior &lt;- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,10),type=&quot;l&quot;,lwd=1,lty=2,col=&quot;blue&quot;,ylab=&quot;Probability Density&quot;,xlab=&quot;param.space&quot;)
points(param.space,posterior*length(param.space),type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2,lty=1)
points(param.space,likelihood*1e9,type=&quot;l&quot;,col=&quot;red&quot;,lwd=1)
axis(4,at=seq(0,6,by=1),labels = seq(0,6e-9,by=1e-9))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>What about a super informative prior??</p>
<pre class="r"><code>prior &lt;- dbeta(param.space,shape1=150,shape2=50)
#prior

## weight the data likelihood by the prior

weighted.likelihood &lt;- likelihood*prior

## compute normalization constant

normalization.constant &lt;- sum(weighted.likelihood)

## Posterior!!

posterior &lt;- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,15),type=&quot;l&quot;,lwd=1,lty=2,col=&quot;blue&quot;,ylab=&quot;Probability Density&quot;,xlab=&quot;param.space&quot;)
points(param.space,posterior*length(param.space),type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2,lty=1)
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Okay, now let’s do it the more mathematically elegant way! When we work with a conjugate prior, the updating process is easy. The posterior distribution for the <em>p</em> term in the above example can be computed by:</p>
<p><span class="math inline">\(Beta(shape1=prior+k,shape2=prior+(N-k))\)</span></p>
<p>Let’s do the same thing, now using the conjugate prior method…</p>
<pre class="r"><code>### PRIOR

prior_beta &lt;- c(shape1=1,shape2=1)
curve(dbeta(x,prior_beta[&#39;shape1&#39;],prior_beta[&#39;shape2&#39;]),ylim=c(0,5),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=1,lty=2,xlab=&quot;param.space&quot;)

### POSTERIOR

curve(dbeta(x,prior_beta[&#39;shape1&#39;]+data,prior_beta[&#39;shape2&#39;]+(10-data)),ylim=c(0,4),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,lty=1,xlab=&quot;param.space&quot;,add=T)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>And again, this time with an informative prior!</p>
<pre class="r"><code>### PRIOR
prior_beta &lt;- c(shape1=15,shape2=5)
curve(dbeta(x,prior_beta[&#39;shape1&#39;],prior_beta[&#39;shape2&#39;]),ylim=c(0,5),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=1,lty=2,xlab=&quot;param.space&quot;)


### POSTERIOR

curve(dbeta(x,prior_beta[&#39;shape1&#39;]+data,prior_beta[&#39;shape2&#39;]+(10-data)),ylim=c(0,4),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,lty=1,xlab=&quot;param.space&quot;,add=T)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>And the super informative prior??</p>
<pre class="r"><code>### PRIOR
prior_beta &lt;- c(shape1=150,shape2=50)
curve(dbeta(x,prior_beta[&#39;shape1&#39;],prior_beta[&#39;shape2&#39;]),ylim=c(0,15),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=1,lty=2,xlab=&quot;param.space&quot;)


### POSTERIOR
curve(dbeta(x,prior_beta[&#39;shape1&#39;]+data,prior_beta[&#39;shape2&#39;]+(10-data)),ylim=c(0,15),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,lty=1,xlab=&quot;param.space&quot;,add=T)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="bayesian-point-estimate" class="section level3">
<h3>Bayesian point estimate</h3>
<p>One of the differences between the MLE and the Bayesian paradigm (although both use likelihood as a way to summarize the information content of the data) is that the point estimate is not the maximum of the posterior distribution (in MLE, we by definition try to find the parameter value that maximizes the likelihood function) but the <strong>mean</strong> of the posterior distribution. That is, it is the expected value of that parameter…</p>
<p>Imagine you had a skewed posterior distribution that looked something like this:</p>
<pre class="r"><code>curve(dlnorm(x,4,1),from=0.001,to=200,ylab=&quot;prob density&quot;)  # use a lognormal distribution for example of skewed dist...</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Where is the mode? Where is the mean?</p>
<pre class="r"><code>param.space2 &lt;- seq(0.001,200,length=10000)
skewed.posterior &lt;- dlnorm(param.space2,4,1)
mean &lt;- mean(rlnorm(10000,4,1))
mode &lt;- param.space2[which.max(skewed.posterior)]
plot(param.space2,skewed.posterior,type=&quot;l&quot;,ylab=&quot;prob density&quot;)
abline(v=c(mean,mode),col=gray(0.5),lwd=3,lty=2)   # add to plot</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>What about for the first worked example? Is there a big difference between the mean and the mode?</p>
<pre class="r"><code>graphics.off()
### POSTERIOR
posterior &lt;- dbeta(param.space,1+data,1+(10-data))
mean &lt;- mean(rbeta(10000,1+data,1+(10-data)))
mode &lt;- param.space[which.max(posterior)]
plot(param.space,posterior,type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2)
abline(v=c(mean,mode),col=gray(0.5),lwd=3,lty=2)   # add to plot</code></pre>
<div id="discussion-question" class="section level4">
<h4>Discussion question</h4>
<p>In MLE, we by definition try to find the parameter value that maximizes the likelihood function. Why don’t we use the mean of the likelihood function? In Bayesian analysis, why don’t we use the mode of the posterior distribution?</p>
</div>
</div>
<div id="bayesian-parameter-uncertainty" class="section level3">
<h3>Bayesian parameter uncertainty</h3>
<p>We often call Bayesian confidence intervals <em>credible intervals</em> to distinguish from their frequentist analog. Bayesian (e.g., 95%) credible intervals can be interpreted in the way you probably have always wanted to interpret frequentist (95%) confidence intervals. It will probably feel satisfying, but a little dirty at the same time!</p>
<blockquote>
<p>You are 95% sure that the true parameter value is between the lower and upper bound!!!</p>
</blockquote>
<p>Let’s try this:</p>
<pre class="r"><code>### POSTERIOR

curve(dbeta(x,1+data,1+(10-data)),ylim=c(0,4),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,lty=1,xlab=&quot;param.space&quot;)

### CREDIBLE INTERVAL

credible.interval &lt;- qbeta(c(0.025,0.975),1+data,1+(10-data))     # get the credible interval

abline(v=credible.interval,col=gray(0.5),lwd=3,lty=2)   # add to plot</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="what-if-there-is-no-nice-easy-conjugate-prior" class="section level3">
<h3>What if there is no nice easy conjugate prior?</h3>
<p>One of the reasons Bayesian analysis was less common historically was that there were no mathematically straightforward ways to do the analysis. <em>There still are not</em> BUT we have fast computers and computational algorithms. Basically, we can use various forms of more-or-less brute force computation to do most Bayesian analyses.</p>
<p>Let’s start with the most brute of brute-force methods:</p>
</div>
<div id="the-brute-force-method" class="section level3">
<h3>The brute force method</h3>
<p>For continuity we will continue the myxomatosis dataset. Recall that we are estimating the shape and scale parameters of the Gamma distribution that describes the virus titer in Australian rabbits for the lowest-grade infections.</p>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>Recall the histogram looks like this:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>And we are trying to fit a Gamma distribution to these data:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>We already have a likelihood function for this problem! Note that we are now looking at real likelihoods and not log likelihoods!</p>
<pre class="r"><code>GammaLikelihoodFunction &lt;- function(params){
  prod(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;]))
}

params &lt;- c(40,0.15) 
names(params) &lt;- c(&quot;shape&quot;,&quot;scale&quot;)
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] 2.906766e-22</code></pre>
<p>And we recall that the 2D likelihood surface looks something like this:</p>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Now let’s use this 2-D likelihood surface as a jumping-off point for a brute-force Bayesian solution to this problem!</p>
<p>First we need to set priors for the shape and scale parameters… For example, we could set uniform distributions for these parameters. For this example, let’s imagine a prior in which all pixels in the above image are equally likely:</p>
<pre class="r"><code>pixelArea &lt;- 0.0001  # for determining probability densities

##############
# define the prior probability surface across this grid within parameter space
##############

prior2D &lt;- matrix(1, nrow=length(shapevec),ncol=length(scalevec))   # initialize prior
prior2D &lt;- prior2D/length(prior2D)

############
# Visualize the 2-D prior distribution
############

image(x=shapevec,y=scalevec,z=prior2D,zlim=c(0,0.001),col=rainbow(10))</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Okay, not very interesting!</p>
<p>But now we have the raw information we need to apply Bayes’ rule!</p>
<pre class="r"><code>weighted.likelihood &lt;- prior2D * likelihood2D    # numerator of Bayes rule
normalization.constant &lt;- sum(weighted.likelihood)    # denominator of Bayes rule

posterior2D &lt;- weighted.likelihood/normalization.constant

############
# Visualize the 2-D posterior distribution
############

image(x=shapevec,y=scalevec,z=(posterior2D/pixelArea),zlim=c(0,5),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=(posterior2D/pixelArea),levels=c(1:4),add=T,drawlabels=FALSE)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Now we can use this posterior distribution to get our point estimates and parameter uncertainty estimates. We could just take the 2d posterior distribution surface and draw the contour containing 95% of the probability density:</p>
<p>First let’s find the contour line below which only 5% of the probability density is contained:</p>
<pre class="r"><code>possible.contours &lt;- data.frame(contour = seq(0.13e-4,1e-4,length=100), quantile = NA)
i=1
for(i in 1:nrow(possible.contours)){
  ndx &lt;- which(posterior2D&lt;possible.contours$contour[i],arr.ind = T)
  possible.contours$quantile[i] &lt;- sum(posterior2D[ndx])
}

head(possible.contours,10)</code></pre>
<pre><code>##         contour   quantile
## 1  1.300000e-05 0.03484278
## 2  1.387879e-05 0.03803099
## 3  1.475758e-05 0.04110702
## 4  1.563636e-05 0.04417352
## 5  1.651515e-05 0.04751733
## 6  1.739394e-05 0.05046922
## 7  1.827273e-05 0.05369474
## 8  1.915152e-05 0.05674380
## 9  2.003030e-05 0.05980036
## 10 2.090909e-05 0.06315809</code></pre>
<p>From here we can see that the posterior probability 1.739394e-05 encloses 95% of the probability density</p>
<pre class="r"><code>q95 &lt;- 1.739394e-05
image(x=shapevec,y=scalevec,z=posterior2D,zlim=c(0.5e-11,5e-4),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=posterior2D,levels=q95,add=T,lwd=3,col=&quot;red&quot;,drawlabels=FALSE)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Where is our point estimate? Let’s find the posterior mean and mode!</p>
<pre class="r"><code>image(x=shapevec,y=scalevec,z=posterior2D,zlim=c(0.5e-11,5e-4),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=posterior2D,levels=q95,add=T,lwd=3,col=&quot;red&quot;,drawlabels=FALSE)
meanshape &lt;- sum(shapevec*posterior2D) 
meanscale &lt;- sum(scalevec*posterior2D)
points(meanshape,meanscale,pch=20,cex=2,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>We can also plot out the marginal posterior distributions for the parameters separately. For example, the probability that the shape parameter falls within a given range, regardless of the other variable:</p>
<pre class="r"><code>marginal.dist.shape &lt;- apply(posterior2D,1,mean)
plot(shapevec,(marginal.dist.shape/sum(marginal.dist.shape))/0.1,type=&quot;l&quot;,lwd=2,col=&quot;blue&quot;,ylab=&quot;probability density&quot;,main=&quot;Posterior probability&quot;)
abline(v=meanshape)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code>marginal.dist.scale &lt;- apply(posterior2D,2,mean)
plot(scalevec,(marginal.dist.scale/sum(marginal.dist.scale))/0.001,type=&quot;l&quot;,lwd=2,col=&quot;blue&quot;,ylab=&quot;probability density&quot;,main=&quot;Posterior probability&quot;)
abline(v=meanscale)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-29-2.png" width="672" /></p>
<pre class="r"><code>meanshape</code></pre>
<pre><code>## [1] 48.08924</code></pre>
<pre class="r"><code>meanscale</code></pre>
<pre><code>## [1] 0.1549478</code></pre>
<p>And from here we can compute the posterior mean and Bayesian credible intervals.</p>
<p>How do our Bayesian estimates compare with the maximum likelihood estimates?? (shape = 49.3666607 scale = 0.1402629)</p>
<p>We can also <em>sample directly from this 2-D posterior distribution</em>:</p>
<pre class="r"><code>SampleFromPosterior &lt;- function(n){
  shape &lt;- rep(shapevec,times=length(scalevec))
  scale &lt;- rep(scalevec,each=length(shapevec))
  jointparams &lt;- data.frame(shape=shape,scale=scale)
  probs &lt;- as.vector(posterior2D)
  samples &lt;- sample(c(1:length(probs)),size=n,replace=TRUE,prob=probs)
  jointparams[samples,]
}

samples&lt;-SampleFromPosterior(n=10000)
par(mfrow=c(3,2))
plot(samples,col=1:10000)
plot(samples,type=&quot;l&quot;)
plot(ts(samples[,1]))
plot(ts(samples[,2]))
hist(samples[,1],40)
hist(samples[,2],40)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
</div>
<div id="markov-chain-monte-carlo" class="section level3">
<h3>Markov Chain Monte Carlo</h3>
<p>Now in many cases, we simply won’t have the computational power to partition our parameter space into discrete pixels and completely evaluate the posterior probability for all <em>n</em>-dimensional pixels in that space. In these cases, we tend to harness ingenious algorithms known as Markov-Chain Monte Carlo (MCMC). This is the focus of the next lecture.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
