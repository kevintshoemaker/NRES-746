<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-27" />

<title>Bayesian Analysis #1: Concepts</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="Generalized Additive Models (GAMs).pdf">GAMs</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Bayesian Analysis #1: Concepts</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 27, 2016</em></h4>

</div>


<p><strong>Bayesian analysis</strong> is also likelihood-based, and follows naturally from our previous discussions. The difference is that we are no longer interested in the maximum likelhood estimate and the properties of maximum likelhood estimators. We are now interested in computing our degree of believe in all possible values across parameter space. Effectively, we want a <em>probability distribution</em> that gives us a probability density (or mass) for all possible parameter combinations, that gives us our degree of belief in a particular model (in this case, the set of parameter values) given the observed data.</p>
<div id="play-with-binomialbeta-conjugate-prior" class="section level3">
<h3>Play with binomial/beta (conjugate prior)</h3>
<p>One of the most intuitive ways to get into Bayesian inference is to start with the binomial distribution. Let’s imagine we know <em>N</em> (<em>N</em>, the number of independent trials, is fixed), but we want to estimate <em>p</em>. Let’s assume we have no prior information, so that any value of p is equally likely.</p>
<div id="set-the-prior" class="section level4">
<h4>Set the prior</h4>
<p>To set the prior, let’s assume a uniform distribution between 0 and 1:</p>
<pre class="r"><code>curve(dunif(x),ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>hist(runif(10000),freq=F,,ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<p>An alternative way to specify this uniform (flat) prior is to use the beta distribution, with both shape parameters set to 1</p>
<pre class="r"><code>curve(dbeta(x,1,1),ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>hist(rbeta(10000,1,1),freq=F,,ylim=c(0,2),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
</div>
</div>
<div id="conjugate-prior" class="section level3">
<h3>Conjugate prior</h3>
<p>Why choose the beta distribution here? The answer is that the beta is the <strong>conjugate prior</strong> for the <em>p</em> parameter in the binomial distribution. This makes Bayesian estimation easy, as we will see!</p>
<div id="definition-conjugate-prior" class="section level4">
<h4>Definition: conjugate prior</h4>
<p>A conjugate prior is a distribution that matches the data-generating model such that it has the same form as the likelihood function. In this way, the distributional form of the posterior distribution for a parameter is the same as the prior distribution for that parameter (although the shape of the distribution will change). We will come back to this!</p>
</div>
<div id="worked-example" class="section level4">
<h4>Worked example</h4>
<p>Let’s work through an example. Let’s imagine the same frog-call survey we have imagined before. We know the site is occupied. After visiting the site 10 times, we detected the frog (heard its call) 3 times out of 10. We are interested in determining the detection probability.</p>
<p>We know the likelihood of the data across parameter space</p>
<pre class="r"><code>data = 3
param.space &lt;- seq(0,1,by=0.001)
likelihood &lt;- dbinom(data,size=10,prob=param.space)
par(mai=c(1,1,0,1))
curve(dbeta(x,1,1),ylim=c(0,2),col=&quot;blue&quot;,ylab=&quot;Probability density&quot;,xlab=&quot;param.space&quot;)
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=2)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Recall that the likelihood curve is NOT a probability distribution. It does not necessarily sum to 1! In Bayesian analyses, we translate the likelihood to a probability using Bayes rule!!</p>
<p><span class="math inline">\(Prob(Model|Data) = \frac{Prob(Data|Model)\cdot Prob(Model))}{Prob(Data)}\)</span></p>
<p>The likelihood is just the <span class="math inline">\(Prob(Data|Model)\)</span> term…</p>
<p>What is the probability of the data? well, it’s just the sum of the probability of the data across parameter space. Really, <span class="math inline">\(Prob(Data)\)</span> can be seen as a normalizing constant that is used to convert the numerator of Bayes rule into a probability distribution. Let’s do it first by brute force…</p>
<pre class="r"><code>prior &lt;- dbeta(param.space,shape1=1,shape2=1)
#prior

## weight the data likelihood by the prior

weighted.likelihood &lt;- likelihood*prior

## compute normalization constant

normalization.constant &lt;- sum(weighted.likelihood)

## Posterior!!

posterior &lt;- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,5),type=&quot;l&quot;,lwd=2,col=&quot;blue&quot;,ylab=&quot;Probability&quot;,xlab=&quot;param.space&quot;)
points(param.space,posterior*length(param.space),type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2,lty=2)
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Notice that the shape of the posterior looks a lot like the shape of the likelhood surface. What this says to us is that the prior has been overwhelmed by the information content of the data (as summarized by the likelihood surface)</p>
<p>What if we have a more informative prior?</p>
<pre class="r"><code>prior &lt;- dbeta(param.space,shape1=15,shape2=5)
#prior

## weight the data likelihood by the prior

weighted.likelihood &lt;- likelihood*prior

## compute normalization constant

normalization.constant &lt;- sum(weighted.likelihood)

## Posterior!!

posterior &lt;- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,5),type=&quot;l&quot;,lwd=2,col=&quot;blue&quot;,ylab=&quot;Probability&quot;,xlab=&quot;param.space&quot;)
points(param.space,posterior*length(param.space),type=&quot;l&quot;,col=&quot;blue&quot;,lwd=2,lty=2)
points(param.space,likelihood*5,type=&quot;l&quot;,col=&quot;red&quot;,lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext(&quot;Likelihood&quot;, side=4, col=&quot;red&quot;,line=3)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>What does this tell us?</p>
<p>Okay, now let’s do it the more mathematically elegant way! When we work with a conjugate prior, the updating process is easy. The posterior distribution for the <em>p</em> term in the above example can be computed by:</p>
<p><span class="math inline">\(Beta(shape1=prior+k,shape2=prior+(N-k))\)</span></p>
<p>Let’s do the same thing, now using the conjugate prior method…</p>
<pre class="r"><code>### PRIOR

curve(dbeta(x,1,1),ylim=c(0,5),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,xlab=&quot;param.space&quot;)

### POSTERIOR

curve(dbeta(x,1+data,1+(10-data)),ylim=c(0,4),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,lty=2,xlab=&quot;param.space&quot;,add=T)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>And again, this time with an informative prior!</p>
<pre class="r"><code>### PRIOR

curve(dbeta(x,15,5),ylim=c(0,5),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,xlab=&quot;param.space&quot;)

### POSTERIOR

curve(dbeta(x,15+data,5+(10-data)),ylim=c(0,4),ylab=&quot;Prob Density&quot;,col=&quot;blue&quot;,lwd=2,lty=2,xlab=&quot;param.space&quot;,add=T)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="what-if-there-is-no-nice-easy-conjugate-prior" class="section level3">
<h3>What if there is no nice easy conjugate prior?</h3>
<p>One of the reasons Bayesian analysis was less common historically was that there were no mathematically straightforward ways to do the analysis. <em>There still are not</em> BUT we have fast computers and computational algorithms. Basically, we can use various forms of brute force computation to do Bayesian analyses.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
