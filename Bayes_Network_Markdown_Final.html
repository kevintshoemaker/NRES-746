<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="James Simmons and Tim Caldwell" />


<title>BayesNet Lab</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus2.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">BayesNet Lab</h1>
<h4 class="author"><em>James Simmons and Tim Caldwell</em></h4>

</div>


<p><strong>Note: much of this lab was created using the <a href="www.bnlearn.com">bnlearn</a> site.</strong></p>
<div id="background" class="section level1">
<h1>Background</h1>
<p>Element/concepts of Bayesian Networks have been around since the early 1900s. However, modern Bayesian network theory emerged out of artificial intelligence research in the 1980s, with the term ‘Bayesian Networks’ coined by Judea Pearl in 1985.</p>
<p>These networks are termed Bayesian because:</p>
<ul>
<li>Input or starting information can be subjective</li>
<li>May use Bayes Rule to update model information</li>
<li>Distinguishes between causal and evidential modes</li>
</ul>
<p>Also known as:</p>
<p>belief networks, Bayesian belief networks, Bayes nets, causal probabilistic networks, influence diagrams, and BNs.</p>
<p>Note: BNs have the same network requirements and components as SEMs (Structural Equation Models). Refer to the SEM lecture for more details. However, BNs are different because they replace the SEM correlation coefficients and r<sup>2</sup>s with conditional probability tables at each node (defined below).</p>
</div>
<div id="basic-bn-graph-componentsterminology" class="section level1">
<h1>Basic BN graph components/terminology</h1>
<ul>
<li>node: an endogenous, exogenous, or latent variable of the system</li>
<li>arc: directional arrow depicting the casual relationship between two nodes. Arcs can also be non-directional when the casual relationship between two nodes is unknown or unable to be determined.</li>
</ul>
</div>
<div id="when-can-bns-be-used" class="section level1">
<h1>When can BNs be used?</h1>
<p>Anytime one can model a system/process AND<br />
Meet the DAG requirements AND<br />
Satisfactorily address the ‘Challenges’ listed below</p>
<p>DAG (directed acyclic graph):</p>
<ul>
<li>each node to have a directional relationship with at least one other node</li>
<li>no cycles</li>
<li>no loops</li>
<li>d-separation</li>
</ul>
</div>
<div id="advantages" class="section level1">
<h1>Advantages</h1>
<ul>
<li>Various knowledge sources/types: categorical, ordinal, continuous, discrete, ‘expert knowledge’, or mixed data</li>
<li>Small/missing data: no sample size requirements</li>
<li>Structural learning: can estimate unknown node relationships and entire network structure</li>
<li>Uncertainty/decision analysis: ‘influence diagrams’ can quantify uncertainty and decision outcomes</li>
<li>Omni-directional inference: can make inferences in any direction within the network</li>
<li>Fast analytical solutions possible: with conjugate priors, fast analytical answers possible without having to run time consuming numerical approximation analyses</li>
</ul>
</div>
<div id="challenges" class="section level1">
<h1>Challenges</h1>
<ul>
<li>Expert/prior knowledge: ‘expert’ knowledge may not be very good</li>
<li>Small data sets/missing data: more data is always preferable to less</li>
<li>Discretizing continuous variables: information is lost in this process</li>
<li>Feedback loops - acyclic: hard to model systems with these features</li>
</ul>
</div>
<div id="five-main-steps-to-bayesian-network-analysis." class="section level1">
<h1>Five main steps to Bayesian Network analysis.</h1>
<p><strong>1. Network setup:</strong> manually creating nodes and arcs.</p>
<p><strong>2. Network structure learning:</strong> algorithmically creating nodes and arcs, and selecting the ‘best’ network.</p>
<p><strong>3. ‘Parameter learning’ or ‘training’ the network:</strong> creating conditional probability tables at each node.</p>
<p><strong>4. Model validation:</strong> validating that the model/network fits the data.</p>
<p><strong>5. Inference:</strong> estimating network outcomes, given a starting value(s).</p>
</div>
<div id="bayesian-network-example" class="section level1">
<h1>Bayesian Network Example</h1>
<p>We will setup a Bayesian network structure from beginning to end using <strong>bnlearn</strong>.</p>
<p><strong>bnlearn</strong> is an R package for performing Bayesian Network analysis: manually creating network structure, learning network graphical structure, estimating node joint conditional probability tables, validating models, and performing inference.</p>
<p>First, install the bnlearn package: install.packages(“bnlearn”).</p>
<p>We will be using the ‘ASIA’ (sometimes called Lung Cancer) dataset from the Bayesian Network Repository on the bnlearn site. “‘ASIA’ is a synthetic data set from Lauritzen and Spiegelhalter (1988) about lung diseases (tuberculosis, lung cancer or bronchitis) and visits to Asia. Shortness-of-breath (dyspnoea) may be due to tuberculosis, lung cancer or bronchitis, or none of them, or more than one of them. A recent visit to Asia increases the chances of tuberculosis, while smoking is known to be a risk factor for both lung cancer and bronchitis. The results of a single chest X-ray do not discriminate between lung cancer and tuberculosis, as neither does the presence or absence of dyspnoea.”</p>
<p>Now, call up the ‘Asia’ dataset in the bnlearn library.</p>
<pre class="r"><code>library(bnlearn)</code></pre>
<pre><code>## Warning: package &#39;bnlearn&#39; was built under R version 3.3.2</code></pre>
<pre><code>## 
## Attaching package: &#39;bnlearn&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     sigma</code></pre>
<pre class="r"><code>head(asia)</code></pre>
<pre><code>##    A   S   T  L   B   E   X   D
## 1 no yes  no no yes  no  no yes
## 2 no yes  no no  no  no  no  no
## 3 no  no yes no  no yes yes yes
## 4 no  no  no no yes  no  no yes
## 5 no  no  no no  no  no  no yes
## 6 no yes  no no  no  no  no yes</code></pre>
<p>The ASIA dataset contains the following variables and data:</p>
<ul>
<li>D (dyspnoea), a two-level factor with levels yes and no.</li>
<li>T (tuberculosis), a two-level factor with levels yes and no.</li>
<li>L (lung cancer), a two-level factor with levels yes and no.</li>
<li>B (bronchitis), a two-level factor with levels yes and no.</li>
<li>A (visit to Asia), a two-level factor with levels yes and no.</li>
<li>S (smoking), a two-level factor with levels yes and no.</li>
<li>X (chest X-ray), a two-level factor with levels yes and no.</li>
<li>E (tuberculosis versus lung cancer/bronchitis), a two-level factor with levels yes and no.</li>
</ul>
<p>And for later reference, the ‘true’ network structure is shown below: <img src="BNimage.png" alt="Asia Data Set Structure" /></p>
<p>It is important note here that although this example uses on two-level factor data, Bayesian networks can use categorical, ordinal, discrete, continuous, and mixed data (combination of the previously listed data types).</p>
<div id="network-setup" class="section level2">
<h2>1. Network setup</h2>
<p>Before we work with the ‘Asia’ dataset, however, we will show an example of how to create a simple network structure from scratch. One can create an ‘empty’ (no arcs) or ‘random’ (randomly assigned arcs between the nodes) networks, but we will not cover these here. Examples of how to setup these networks can be found under Examples on the bnlearn site. We will focus on creating a specific network structure. You may want to use this method when you are fairly confident in the ‘true’ network structure.</p>
<pre class="r"><code>dag= empty.graph(LETTERS[c(1,19,20,12,2,5,24,4)]) #create an empty DAG with nodes
asia.structure = matrix(c(&quot;A&quot;, &quot;S&quot;, &quot;S&quot;, &quot;T&quot;, &quot;T&quot;,&quot;L&quot;, &quot;L&quot;,&quot;B&quot;, &quot;B&quot;, &quot;E&quot;,&quot;E&quot;, &quot;X&quot;,&quot;X&quot;,&quot;D&quot;),
                        ncol = 2, byrow = TRUE,
                        dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;))) #assign the DAG structure
asia.structure</code></pre>
<pre><code>##      from to 
## [1,] &quot;A&quot;  &quot;S&quot;
## [2,] &quot;S&quot;  &quot;T&quot;
## [3,] &quot;T&quot;  &quot;L&quot;
## [4,] &quot;L&quot;  &quot;B&quot;
## [5,] &quot;B&quot;  &quot;E&quot;
## [6,] &quot;E&quot;  &quot;X&quot;
## [7,] &quot;X&quot;  &quot;D&quot;</code></pre>
<pre class="r"><code>arcs(dag) &lt;- asia.structure #now asign the structure to the empty graph using arcs, which makes it a bnlearn object
dag</code></pre>
<pre><code>## 
##   Random/Generated Bayesian network
## 
##   model:
##    [A][S|A][T|S][L|T][B|L][E|B][X|E][D|X] 
##   nodes:                                 8 
##   arcs:                                  7 
##     undirected arcs:                     0 
##     directed arcs:                       7 
##   average markov blanket size:           1.75 
##   average neighbourhood size:            1.75 
##   average branching factor:              0.88 
## 
##   generation algorithm:                  Empty</code></pre>
<pre class="r"><code>plot(dag)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Also, one can introduce undirected arcs, by including both directions of an arc in the arc set (e.g. A to B and B to A), as long as no cycle will be introduced in either direction. For example:</p>
<pre class="r"><code>dag2 = empty.graph(LETTERS[c(1,19,20,12)])
asia.structure2 = matrix(c(&quot;A&quot;, &quot;S&quot;, &quot;S&quot;, &quot;A&quot;, &quot;T&quot;,&quot;L&quot;, &quot;L&quot;, &quot;T&quot;),
                      ncol = 2, byrow = TRUE,
                       dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;)))
arcs(dag2) = asia.structure2
plot(dag2)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Performing these commands also conducts automatic checks for violations in the network structure requirements. Detected structure violations will be communicated via error messages. The main violation checks are for missing nodes, cycles, and loops.</p>
<p>In addition, here is an example of a network structure with ‘expert’ opinion, where we plug estimates right into the node joint conditional probability distribution.</p>
<pre class="r"><code>Expert1 = matrix(c(0.4, 0.6), ncol = 2, dimnames = list(NULL, c(&quot;LOW&quot;, &quot;HIGH&quot;)))
Expert1</code></pre>
<pre><code>##      LOW HIGH
## [1,] 0.4  0.6</code></pre>
<pre class="r"><code>Expert2 = c(0.5, 0.5, 0.4, 0.6, 0.3, 0.7, 0.2, 0.8)
dim(Expert2) = c(2, 2, 2)
dimnames(Expert2) = list(&quot;C&quot; = c(&quot;TRUE&quot;, &quot;FALSE&quot;), &quot;A&quot; =  c(&quot;LOW&quot;, &quot;HIGH&quot;), &quot;B&quot; = c(&quot;GOOD&quot;, &quot;BAD&quot;))
Expert2</code></pre>
<pre><code>## , , B = GOOD
## 
##        A
## C       LOW HIGH
##   TRUE  0.5  0.4
##   FALSE 0.5  0.6
## 
## , , B = BAD
## 
##        A
## C       LOW HIGH
##   TRUE  0.3  0.2
##   FALSE 0.7  0.8</code></pre>
</div>
</div>
<div id="network-structure-learning" class="section level1">
<h1>2. Network structure learning</h1>
<p>In addition to manually creating a network structure (example above), network structures can be created by the data via structure learning algorithms.</p>
<p>There are three main types of structure learning algorithms: constraint-based, score-based, and hybrid (mixture of constraint-based and score-based). The user can specify either (Akaike Information Criterion), BIC (Bayesian Information Criterion), or BDE (Bayesian Dirichlet) scoring to determine the best network structure. The algorithms use different techniques to cycle through various network structures, and then chooses as the ‘best’ network the structure with best score. The default scoring method for score-based and hybrid algorithms is BIC.</p>
<div id="constraint-based" class="section level3">
<h3>Constraint-based:</h3>
<p>No beginning/starting model structure is used with these algorithms. The algorithms build the structure by searching for conditional dependencies between the variables. bnlearn includes the following constraint-based algorithms:</p>
<ul>
<li>Grow-Shrink (GS)</li>
<li>Incremental Association Markov Blanket (IAMB)</li>
<li>Fast Incremental Association (Fast-IAMB)</li>
<li>Interleaved Incremental Association (Inter-IAMB)</li>
<li>Max-Min Parents &amp; Children (MMPC)</li>
<li>Semi-Interleaved Hiton-PC (SI-HITON-PC)</li>
</ul>
</div>
<div id="score-based" class="section level3">
<h3>Score-based:</h3>
<p>User leverages their knowledge of the system to create a network, codes his/her confidence in the network, and inputs the data. The algorithm then estimates the most likely model structure. bnlearn includes the following scored-based algorithms:</p>
<ul>
<li>Hill Climbing (HC)</li>
<li>Tabu Search (Tabu)</li>
</ul>
</div>
<div id="hybrid" class="section level3">
<h3>Hybrid:</h3>
<p>Mixture of constraint-based and score-based methods. bnlearn includes the following hybrid algorithms:</p>
<ul>
<li>Max-Min Hill Climbing (MMHC)</li>
<li>General 2-Phase Restricted Maximization (RSMAX2)</li>
</ul>
<p>First, an example of the contraint-based learning using the Incremental Association Markov Blanket (IAMB) algorithm:</p>
<pre class="r"><code>iambex &lt;- iamb(asia) #structure learning
iambex</code></pre>
<pre><code>## 
##   Bayesian network learned via Constraint-based methods
## 
##   model:
##    [A][S][T][L][X][D][B|S:D][E|T:L] 
##   nodes:                                 8 
##   arcs:                                  4 
##     undirected arcs:                     0 
##     directed arcs:                       4 
##   average markov blanket size:           1.50 
##   average neighbourhood size:            1.00 
##   average branching factor:              0.50 
## 
##   learning algorithm:                    IAMB 
##   conditional independence test:         Mutual Information (disc.) 
##   alpha threshold:                       0.05 
##   tests used in the learning procedure:  111 
##   optimized:                             TRUE</code></pre>
<pre class="r"><code>plot(iambex)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Now, an example of score-based learning using the Hill Climbing (HC) algorithm:</p>
<pre class="r"><code>hcex &lt;- hc(asia)
hcex</code></pre>
<pre><code>## 
##   Bayesian network learned via Score-based methods
## 
##   model:
##    [A][S][T][L|S][B|S][E|T:L][X|E][D|B:E] 
##   nodes:                                 8 
##   arcs:                                  7 
##     undirected arcs:                     0 
##     directed arcs:                       7 
##   average markov blanket size:           2.25 
##   average neighbourhood size:            1.75 
##   average branching factor:              0.88 
## 
##   learning algorithm:                    Hill-Climbing 
##   score:                                 BIC (disc.) 
##   penalization coefficient:              4.258597 
##   tests used in the learning procedure:  77 
##   optimized:                             TRUE</code></pre>
<pre class="r"><code>plot(hcex)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>And a hybrid learning example with the Max-Min Hill Climbing (MMHC) algorthim:</p>
<pre class="r"><code>mmex &lt;- mmhc(asia)
mmex</code></pre>
<pre><code>## 
##   Bayesian network learned via Hybrid methods
## 
##   model:
##    [A][S][T][X][L|S][B|S][E|T:L][D|B:X] 
##   nodes:                                 8 
##   arcs:                                  6 
##     undirected arcs:                     0 
##     directed arcs:                       6 
##   average markov blanket size:           2.00 
##   average neighbourhood size:            1.50 
##   average branching factor:              0.75 
## 
##   learning algorithm:                    Max-Min Hill-Climbing 
##   constraint-based method:               Max-Min Parent Children 
##   conditional independence test:         Mutual Information (disc.) 
##   score-based method:                    Hill-Climbing 
##   score:                                 BIC (disc.) 
##   alpha threshold:                       0.05 
##   penalization coefficient:              4.258597 
##   tests used in the learning procedure:  65 
##   optimized:                             TRUE</code></pre>
<pre class="r"><code>plot(mmex)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="network-scores" class="section level2">
<h2>Network Scores</h2>
<p>Below are examples of the AIC and BDE scores for the best network in the Hill Climbing (HC) algorithm (score-based learning) example shown above.</p>
<pre class="r"><code>score(hcex,asia,type=&quot;aic&quot;) #getting aic value for full network</code></pre>
<pre><code>## [1] -11051.9</code></pre>
<pre class="r"><code>score(hcex,asia,type=&quot;bde&quot;) #getting bde value for full network</code></pre>
<pre><code>## [1] -11147.65</code></pre>
<p>The above algorithm results also provide a good example of what happens when the ‘best’ network structure doesn’t contain arcs for all of the nodes. The network from the score-based algorithm is the closest to the ‘true’ network, but the A node is unconnected to the network. We can investigate why this is the case with the A node. For example, from the true model we know that node A influences node T. Let us calculate the score from A to T, and then from T to A.</p>
<pre class="r"><code>eq.net = set.arc(hcex, &quot;A&quot;, &quot;T&quot;) #setting arcs to get actual scores from individual relationships
eq.net1 = set.arc(hcex,&quot;T&quot;, &quot;A&quot;) #setting arcs to get actual scores from individual relationships
score(eq.net,asia,type=&quot;aic&quot;) #retriveing score</code></pre>
<pre><code>## [1] -11051.09</code></pre>
<pre class="r"><code>score(eq.net1,asia,type=&quot;aic&quot;) #retriving score</code></pre>
<pre><code>## [1] -11051.09</code></pre>
<p>We see that when we set the arc from A to T, or from T to A, we get the same network score (-11051.09). Thus, the relationship between A and T is termed ‘score equivalent’, since either direction provides the same/equivalent network score - changing the node direction does not change the network score.</p>
<p>Alternatively, if we change the arrow direction between two other nodes, we will see the network score change. For example, if we change the relationship between nodes L and E:</p>
<pre class="r"><code>eq.net = set.arc(hcex, &quot;L&quot;, &quot;E&quot;)
eq.net1 = set.arc(hcex,&quot;E&quot;, &quot;L&quot;)
score(eq.net,asia,type=&quot;aic&quot;)</code></pre>
<pre><code>## [1] -11051.9</code></pre>
<pre class="r"><code>score(eq.net1,asia,type=&quot;aic&quot;)</code></pre>
<pre><code>## [1] -11271.59</code></pre>
<p>We see that the network score decreases when we reverse the direction between nodes L and E.</p>
<p>At this point, since the algorithms have not been able determine the relationship of A to T (or other nodes), we may want to rely on literature, ‘expert’ opinion, ecology theory, etc. to argue for the best relationship of node A to node T or the rest of the structure.</p>
</div>
</div>
<div id="parameter-learning-or-training-the-network" class="section level1">
<h1>3. ‘Parameter learning’ or ‘training’ the network</h1>
<p>The bn.fit command generates parameter estimates for the conditional probability tables at each node. However, the bn.fit command requires that the network structure represent a DAG (directed acyclic graph); <strong>“otherwise their parameters cannot be estimated because the factorization of the global probability distribution of the data into the local ones (one for each variable in the model) is not completely known.”</strong> Thus, undirected arcs must be set prior to parameter estimation. We see in the above estimated structure that the ‘A’ node is not connected to the network structure. Thus, before we apply bn.fit, we must set a directional arc for A. Due to ‘expert’ opinion, knowledge of the system, or previous studies, let’s set an arc between A and T. The default method for the parameter estimation is maximum likelihood (mle).</p>
<pre class="r"><code>hcex1 = set.arc(hcex, from  = &quot;A&quot;, to = &quot;T&quot;) #creating a new DAG with the A to T relationship(based on our previous knowledge that goint to asia effects having tuberculosis)
plot(hcex1)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Now we can run bn.fit to get the node parameter estimates:</p>
<pre class="r"><code>fit = bn.fit(hcex1, asia) # fitting the network with conditoinal probability tables
fit </code></pre>
<pre><code>## 
##   Bayesian network parameters
## 
##   Parameters of node A (multinomial distribution)
## 
## Conditional probability table:
##      no    yes 
## 0.9916 0.0084 
## 
##   Parameters of node S (multinomial distribution)
## 
## Conditional probability table:
##     no   yes 
## 0.497 0.503 
## 
##   Parameters of node T (multinomial distribution)
## 
## Conditional probability table:
##  
##      A
## T              no         yes
##   no  0.991528842 0.952380952
##   yes 0.008471158 0.047619048
## 
##   Parameters of node L (multinomial distribution)
## 
## Conditional probability table:
##  
##      S
## L             no        yes
##   no  0.98631791 0.88230616
##   yes 0.01368209 0.11769384
## 
##   Parameters of node B (multinomial distribution)
## 
## Conditional probability table:
##  
##      S
## B            no       yes
##   no  0.7006036 0.2823062
##   yes 0.2993964 0.7176938
## 
##   Parameters of node E (multinomial distribution)
## 
## Conditional probability table:
##  
## , , L = no
## 
##      T
## E     no yes
##   no   1   0
##   yes  0   1
## 
## , , L = yes
## 
##      T
## E     no yes
##   no   0   0
##   yes  1   1
## 
## 
##   Parameters of node X (multinomial distribution)
## 
## Conditional probability table:
##  
##      E
## X              no         yes
##   no  0.956587473 0.005405405
##   yes 0.043412527 0.994594595
## 
##   Parameters of node D (multinomial distribution)
## 
## Conditional probability table:
##  
## , , E = no
## 
##      B
## D             no        yes
##   no  0.90017286 0.21373057
##   yes 0.09982714 0.78626943
## 
## , , E = yes
## 
##      B
## D             no        yes
##   no  0.27737226 0.14592275
##   yes 0.72262774 0.85407725</code></pre>
<p>We can retrieve the conditional probability of a specific node using the common $ operator, for example:</p>
<pre class="r"><code>fit$L</code></pre>
<pre><code>## 
##   Parameters of node L (multinomial distribution)
## 
## Conditional probability table:
##  
##      S
## L             no        yes
##   no  0.98631791 0.88230616
##   yes 0.01368209 0.11769384</code></pre>
<pre class="r"><code>fit$D</code></pre>
<pre><code>## 
##   Parameters of node D (multinomial distribution)
## 
## Conditional probability table:
##  
## , , E = no
## 
##      B
## D             no        yes
##   no  0.90017286 0.21373057
##   yes 0.09982714 0.78626943
## 
## , , E = yes
## 
##      B
## D             no        yes
##   no  0.27737226 0.14592275
##   yes 0.72262774 0.85407725</code></pre>
<p>And we can visualize the node conditoinal probability tables with barcharts:</p>
<pre class="r"><code>bn.fit.barchart(fit$D)</code></pre>
<pre><code>## Loading required namespace: lattice</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Or alternatively with a dot plot:</p>
<pre class="r"><code>bn.fit.dotplot(fit$D)</code></pre>
<p><img src="Bayes_Network_Markdown_Final_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>In addition to maximum likelihood, parameter estimation can be done performed with Bayesian methods - but currently only with discrete data. Below is an example with the Asia dataset. The same bn.fit command line is used, but the method is specified as ‘bayes’.</p>
<pre class="r"><code>fit1 = bn.fit(hcex1, asia, method = &quot;bayes&quot;)
fit1</code></pre>
<pre><code>## 
##   Bayesian network parameters
## 
##   Parameters of node A (multinomial distribution)
## 
## Conditional probability table:
##           no         yes 
## 0.990618762 0.009381238 
## 
##   Parameters of node S (multinomial distribution)
## 
## Conditional probability table:
##        no      yes 
## 0.497006 0.502994 
## 
##   Parameters of node T (multinomial distribution)
## 
## Conditional probability table:
##  
##      A
## T              no         yes
##   no  0.991033649 0.904255319
##   yes 0.008966351 0.095744681
## 
##   Parameters of node L (multinomial distribution)
## 
## Conditional probability table:
##  
##      S
## L             no        yes
##   no  0.98534137 0.88154762
##   yes 0.01465863 0.11845238
## 
##   Parameters of node B (multinomial distribution)
## 
## Conditional probability table:
##  
##      S
## B            no       yes
##   no  0.7002008 0.2827381
##   yes 0.2997992 0.7172619
## 
##   Parameters of node E (multinomial distribution)
## 
## Conditional probability table:
##  
## , , L = no
## 
##      T
## E               no          yes
##   no  0.9997301673 0.0294117647
##   yes 0.0002698327 0.9705882353
## 
## , , L = yes
## 
##      T
## E               no          yes
##   no  0.0038051750 0.1923076923
##   yes 0.9961948250 0.8076923077
## 
## 
##   Parameters of node X (multinomial distribution)
## 
## Conditional probability table:
##  
##      E
## X             no        yes
##   no  0.95609493 0.01200000
##   yes 0.04390507 0.98800000
## 
##   Parameters of node D (multinomial distribution)
## 
## Conditional probability table:
##  
## , , E = no
## 
##      B
## D            no       yes
##   no  0.8997410 0.2140392
##   yes 0.1002590 0.7859608
## 
## , , E = yes
## 
##      B
## D            no       yes
##   no  0.2813620 0.1496815
##   yes 0.7186380 0.8503185</code></pre>
</div>
<div id="model-validation" class="section level1">
<h1>4. Model Validation</h1>
<p>Now that we have a network structure and node conditional probability tables, the next step is to validate the model, or assess model fit to the data. <strong>“Cross-validation is a standard way to obtain unbiased estimates of a model’s goodness of fit. By comparing such measures for different learning strategies (different combinations of learning algorithms, fitting techniques and the respective parameters) we can choose the optimal one for the data at hand in a principled way.”</strong></p>
<p>bnlearn contains 3 methods for cross-validation: k-fold(default), custom, and hold out. We will compare the first two, k-fold and custom.</p>
<div id="heres-an-example-for-the-k-fold-method" class="section level3">
<h3>Heres an example for the K fold method</h3>
<p>The data are randomly partitioned into k subsets of equal size. Each subset is used in turn to validate the model fitted on the remaining k - 1 subsets.</p>
<p>A lower expected loss value is better. Here we will cross-validate two learning algorithms - Max-Min Hill-Climb (mmhc) and Hill-Climb (hc). And the BDE scoring method will be used, which requires an iss (‘imaginery sample size’ used for bde scores) term.</p>
<pre class="r"><code>bn.cv(asia, bn = &quot;mmhc&quot;, algorithm.args = list(score = &quot;bde&quot;, iss = 1))</code></pre>
<pre><code>## 
##   k-fold cross-validation for Bayesian networks
## 
##   target learning algorithm:             Max-Min Hill-Climbing 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (disc.) 
##   expected loss:                         2.405467</code></pre>
<pre class="r"><code>bn.cv(asia, bn = &quot;hc&quot;, algorithm.args = list(score = &quot;bde&quot;, iss = 1))</code></pre>
<pre><code>## 
##   k-fold cross-validation for Bayesian networks
## 
##   target learning algorithm:             Hill-Climbing 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (disc.) 
##   expected loss:                         2.209596</code></pre>
<p>We can specify the number of runs, standard is to do 10 runs.</p>
<pre class="r"><code>bn.cv(asia, bn = &quot;hc&quot;, runs = 10, algorithm.args = list(score = &quot;bde&quot;, iss = 1))</code></pre>
<pre><code>## Warning in entropy.loss(fitted = fitted, data = data, debug = debug): 2
## observations were dropped because the corresponding probabilities for node
## D were 0 or NaN.</code></pre>
<pre><code>## 
##   k-fold cross-validation for Bayesian networks
## 
##   target learning algorithm:             Hill-Climbing 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (disc.) 
##   number of runs:                        10 
##   average loss over the runs:            2.21016 
##   standard deviation of the loss:        0.0004719826</code></pre>
<pre class="r"><code>bn.cv(asia, bn = &quot;mmhc&quot;, runs = 10, algorithm.args = list(score = &quot;bde&quot;, iss = 1))</code></pre>
<pre><code>## 
##   k-fold cross-validation for Bayesian networks
## 
##   target learning algorithm:             Max-Min Hill-Climbing 
##   number of folds:                       10 
##   loss function:                         Log-Likelihood Loss (disc.) 
##   number of runs:                        10 
##   average loss over the runs:            2.405559 
##   standard deviation of the loss:        0.000418334</code></pre>
<p>So, based on these cross-validation results, it appears that the Hill-Climb algorithm produces a model/network structure that fits the data better - because its loss is approximately 2.2 compared to the Max-Min Hill-Climb loss at 2.4.</p>
</div>
</div>
<div id="inference" class="section level1">
<h1>5. Inference</h1>
<p>Finally, now that we have the structure and parameter estimates, we can make inferences from the network. One advantage of Bayesian networks is that inferences can be omni-directional, from the beginning to end, end to beginning, or middle to end or beginning of the process/system. We will show 2 examples here:</p>
<p><strong>Begining to end: From Asia to Xray:</strong></p>
<pre class="r"><code>cpquery(fit1, event = (X==&quot;yes&quot;), evidence = ( A==&quot;yes&quot;))</code></pre>
<pre><code>## [1] 0.1875</code></pre>
<p>The probability is about 17% that your xray is ‘yes’ when you have been to Asia (results may vary).</p>
<p><strong>End to beginning: From Xray to Asia:</strong></p>
<pre class="r"><code>cpquery(fit1, event = (A==&quot;yes&quot;), evidence = ( X==&quot;yes&quot;))</code></pre>
<pre><code>## [1] 0.01247772</code></pre>
<p>The probability is about 2% that you have been to Asia if your Xray is yes (results may vary).</p>
</div>
<div id="additional-resources" class="section level1">
<h1>Additional Resources</h1>
<p><a href="www.bnlern.com">bnlearn website</a></p>
<p><strong>Books</strong></p>
<p>Bayesian Networks: With Examples in R. 2014. Marco Scutari, Jean-Baptiste Denis.</p>
<p>Bayesian Networks in R: with Applications in Systems Biology. 2013. R. Nagarajan, M. Scutari and S. L?bre.</p>
<p>Bayesian Artificial Intelligence, Second Edition. 2010. Kevin B. Korb and Ann E. Nicholson.</p>
<p><strong>Journal Articles</strong></p>
<p>L. Uusitalo. Advantages and challenges of Bayesian networks in environmental modelling. 2007. Ecological Modeling (203): 312-318.</p>
<p>Gupta, S., Kim, H.W. Linking structural equation modeling to Bayesian networks: Decision support for customer retention in virtual communities. 2007. European Journal of Operational Research (190): 881-833.</p>
<p>Anderson, R A., Vasta, G. Causal modeling alternatives in operation research: Overview and application. 2004. European Journal of Operational Research (156): 92-109.</p>
<p>McCann et al. Bayesian belief networks: applications in ecology and natural resource management. 2006. Canadian Journal of Forest Research (36): 3053 - 3062.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
