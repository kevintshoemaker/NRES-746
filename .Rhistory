a <- params[1]      # a parameters
b <- params[2]      # b parameter (not a function of wave/nonwave)
k <- params[3]      # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a=1,b=1,k=1)
NegBinomLik_nowave(params)
### Find the MLE
MLE_nowave <- optim(fn=NegBinomLik_nowave,par=params,method="L-BFGS-B")
MLE_nowave$par
MLE_nowave$value
#########
# Perform LRT -- this time with three fewer free parameters in the reduced model
ms_full <- 2*MLE_full$value
ms_nowave <- 2*MLE_nowave$value
Deviance <- ms_nowave - ms_full
Deviance
Chisq.crit <- qchisq(0.95,df=3)   # now three additional params in the more complex model!
Chisq.crit
Deviance>=Chisq.crit
1-pchisq(Deviance,df=3)   # p-value
###### Visualize the likelihood ratio test
curve(dchisq(x,df=3),0,15)
abline(v=Deviance,col="red",lwd=4)
##############
# Information-theoretic metrics for model-selection
##############
#########
# Akaike's Information Criterion (AIC)
#### First, let's build another likelihood function: whereby only the "b" parameter differs by "wave" sites
NegBinomLik_constak <- function(params){
wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
a <- params[1]                             # a parameters
b <- c(params[2],params[3])[wave.code]                              # b parameter (not a function of wave/nonwave)
k <- params[4]                               # dispersion parameters
expcones <- a*fir$DBH^b
-sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}
params <- c(a=1,b.n=1,b.w=1,k=1)
NegBinomLik_constak(params)
### Fit the new model
MLE_constak <- optim(fn=NegBinomLik_constak,par=params)
MLE_constak$par
MLE_constak$value
ms_constak <- 2*MLE_constak$value
###########
### Now, let's build and fit one more final model- this time with no wave effect and a Poisson error distribution
PoisLik_nowave <- function(params){
a <- params[1]      # a parameters
b <- params[2]      # b parameter (not a function of wave/nonwave)
expcones <- a*fir$DBH^b
-sum(dpois(fir$TOTCONES,lambda=expcones,log=TRUE))
}
params <- c(a=1,b=1)
PoisLik_nowave(params)
MLE_pois <- optim(fn=PoisLik_nowave,par=params)
MLE_pois$par
MLE_pois$value
ms_pois <- 2*MLE_pois$value
###########
# Compare all five models using AIC!
AIC_constak <- ms_constak + 2*4
AIC_full <- ms_full + 2*6
AIC_constb <- ms_constb + 2*5
AIC_nowave <- ms_nowave + 2*3
AIC_pois <- ms_pois + 2*2
AICtable <- data.frame(
Model = c("Full","Constant b","Constant a and k","All constant","Poisson"),
AIC = c(AIC_full,AIC_constb,AIC_constak,AIC_nowave,AIC_pois),
LogLik = c(ms_full/-2,ms_constb/-2,ms_constak/-2,ms_nowave/-2,ms_pois/-2),
params = c(6,5,4,3,2),
stringsAsFactors = F
)
AICtable$DeltaAIC <- AICtable$AIC-AICtable$AIC[which.min(AICtable$AIC)]
AICtable$Weights <- round(exp(-0.5*AICtable$DeltaAIC) / sum(exp(-0.5*AICtable$DeltaAIC)),3)
AICtable$AICc <- AICtable$AIC + ((2*AICtable$params)*(AICtable$params+1))/(nrow(fir)-AICtable$params-1)
AICtable[order(AICtable$AIC),c(1,7,2,5,6,4,3)]
###########
# Bayes factor example
###########
##### take a basic binomial distribution with parameter p fixed at 0.5:
probs1 <- dbinom(0:10,10,0.5)
names(probs1) = 0:10
barplot(probs1,ylab="probability")
## Q: What is the *marginal likelihood* under this simple model for an observation of 2 mortalities out of 10?
## A:
dbinom(2,10,0.5)
## Now we can consider a model whereby "p" is a free parameter
curve(dbeta(x,1,1))  # uniform prior on "p"
###########
# Compute the marginal likelihood of observing 2 mortalities
# ?integrate
binom2 <- function(x) dbinom(x=2,size=10,prob=x)
marginal_likelihood <- integrate(f=binom2,0,1)$value    # use "integrate" function in R
marginal_likelihood  # equal to 0.0909 = 1/11
###########
# Compute the marginal likelihood of observing 3 mortalities
binom3 <- function(x) dbinom(x=3,size=10,prob=x)
marginal_likelihood <- integrate(f=binom3,0,1)$value    # use "integrate" function
marginal_likelihood   # equal to 0.0909 = 1/11
#########
# simulate data from the model across all possible values of the parameter "p"
lots=1000000
a_priori_data <- rbinom(lots,10,prob=rbeta(lots,1,1))   # no particular observation is favored
for_hist <- table(a_priori_data)/lots
barplot(for_hist,xlab="Potential Observation",ylab="Marginal likelihood")
#########
# Visualize the marginal likelihood of all possible observations
probs2 <- rep(1/11,times=11)
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))
###########
# Overlay the marginal likelihood of the simpler model, with p fixed at 0.5
probs2 <- rep(1/11,times=11)
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))
probs1 <- dbinom(0:10,10,0.5)
names(probs1) = 0:10
barplot(probs1,ylab="probability",add=T,col="red",density=20)
############
# Finally, compute the bayes factor given that we observed 2 mortalities. Which model is better?
probs2 <- rep(1/11,times=11)
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))
probs1 <- dbinom(0:10,10,0.5)
names(probs1) = 0:10
barplot(probs1,ylab="probability",add=T,col="red",density=20)
abline(v=3,col="green",lwd=4 )
BayesFactor = (1/11)/dbinom(2,10,0.5)
BayesFactor
############
# Compute the bayes factor given that we observed 3 mortalities. Which model is better now?
probs2 <- rep(1/11,times=11)
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))
probs1 <- dbinom(0:10,10,0.5)
names(probs1) = 0:10
barplot(probs1,ylab="probability",add=T,col="red",density=20)
abline(v=4.3,col="green",lwd=4 )
BayesFactor = dbinom(3,10,0.5)/(1/11)
BayesFactor
#############
# Visualize the likelihood ratio
# probs2 <- rep(1/11,times=11)
# names(probs2) = 0:10
# barplot(probs2,ylab="probability",ylim=c(0,1))
probs1 <- dbinom(0:10,10,0.5)
names(probs1) = 0:10
barplot(probs1,ylab="probability",col="red",density=20,ylim=c(0,1))
probs3 <- dbinom(0:10,10,0.3)
names(probs3) = 0:10
barplot(probs3,ylab="probability",add=T,col="green",density=10,angle = -25)
abline(v=4.3,col="green",lwd=4 )
#########
# LRT: simple model (p fixed at 0.5) vs complex model (p is free parameter)
Likelihood_simple <- dbinom(3,10,0.5)
Likelihood_complex <- dbinom(3,10,0.3)
Likelihood_simple
Likelihood_complex
-2*log(Likelihood_simple)--2*log(Likelihood_complex)
qchisq(0.95,1)
pchisq(1.64,1)    # very high p value, simpler model is preferred
#########
# AIC: simple model (p fixed at 0.5) vs complex model (p is free parameter)
AIC_simple <- -2*log(Likelihood_simple) + 2*0
AIC_complex <-  -2*log(Likelihood_complex) + 2*1
AIC_simple
AIC_complex
### Alternatively, use AICc
AICc_simple <- -2*log(Likelihood_simple) + 0 + 0
AICc_complex <-  -2*log(Likelihood_complex) + 1 + ((2*2)/(3-1-1))
AICc_simple
AICc_complex
######
# Alternatively, try BIC
BIC_simple <- -2*log(Likelihood_simple) + log(10)*0
BIC_complex <-  -2*log(Likelihood_complex) + log(10)*1
BIC_simple
BIC_complex
##############
# Bayesian model selection: Bolker's fir dataset
cat("
model  {
### Likelihood
for(i in 1:n.obs){
expected.cones[i] <- a[wave[i]]*pow(DBH[i],b[wave[i]])   # power function: a*DBH^b
p[i] <- r[wave[i]] / (r[wave[i]] + expected.cones[i])
observed.cones[i] ~ dnegbin(p[i],r[wave[i]])
}
### Priors
for(j in 1:2){   # estimate separately for wave and non-wave
a[j] ~ dunif(0.001,2)
b[j] ~ dunif(0.5,4)
r[j] ~ dunif(0.5,5)
}
}
",file="BUGS_fir.txt")
#######
# Package the data for JAGS
data.package1 <- list(
observed.cones = fir$TOTCONES,
n.obs = nrow(fir),
wave = as.numeric(fir$WAVE_NON),
DBH = fir$DBH
)
#data.package
##########
# Make a function for generating initial guesses
init.generator1 <- function(){ list(
a = runif(2, 0.2,0.5),
b = runif(2, 2,3),
r = runif(2, 1,2)
)
}
init.generator1()
###########
# Run the model in JAGS
library(jagsUI)    # load packages
library(coda)
library(lattice)
params.to.monitor <- c("a","b","r")
jags.fit1 <- jags(data=data.package1,inits=init.generator1,parameters.to.save=params.to.monitor,n.adapt=1000, n.iter=10000,model.file="BUGS_fir.txt",n.chains = 2,n.burnin = 2000,n.thin=5,parallel=TRUE )
jagsfit1.mcmc <- jags.fit1$samples   # extract "MCMC" object (coda package)
summary(jagsfit1.mcmc)
#plot(jagsfit1.mcmc)
plot(jagsfit1.mcmc)
########
# Visualize the model fit
plot(jagsfit1.mcmc)
lattice::densityplot(jagsfit1.mcmc)
########
# Visualize the model fit
plot(jagsfit1.mcmc)
lattice::densityplot(jagsfit1.mcmc)
lattice::densityplot(jags.fit2)
lattice::densityplot(jagsfit2.mcmc)
rmarkdown::render('LECTURE8.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE9.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE10.Rmd',rmarkdown::pdf_document())
rmarkdown::render("neuralNet.Rmd",rmarkdown::pdf_document())
rmarkdown::render("SEM.RMarkdown.Rmd",rmarkdown::pdf_document())
rmarkdown::render("Occupancy.Rmd",rmarkdown::pdf_document())
rmarkdown::render("TimeSeries_all.Rmd",rmarkdown::pdf_document())
rmarkdown::render('INTRO.Rmd',rmarkdown::pdf_document())
rmd2rscript <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
# rmd2rscript_labanswers <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts: see NRESlabs folder!!
#   outfile <- gsub(".Rmd",".R",infile)
#   close( file( outfile, open="w" ) )   # clear output file
#   con1 <- file(infile,open="r")
#   con2 <- file(outfile,"w")
#   stringToFind <- "```{r*"
#   stringToFind2 <- c("answer","test","solution")
#   isrblock <- FALSE
#   #count=0
#   blocknum=0
#
#   while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
#     isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
#     showit <- grepl(input, pattern = stringToFind2[1], perl = TRUE) | grepl(input, pattern = stringToFind2[2])
#     if(isrblock){
#       blocknum=blocknum+1
#       while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
#         if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#         #count=count+1
#       }
#       isrblock=FALSE
#     }
#   }
#   closeAllConnections()
# }
rmd2rscript("LECTURE1.Rmd")
rmd2rscript("LECTURE1.Rmd")
rmd2rscript("LECTURE2.Rmd")
rmd2rscript("LECTURE1.Rmd")
rmd2rscript("LECTURE2.Rmd")
rmd2rscript("LECTURE3.Rmd")
rmd2rscript("LECTURE3.Rmd")
rmd2rscript("LECTURE4.Rmd")
rmd2rscript("LECTURE3.Rmd")
rmd2rscript("LECTURE4.Rmd")
rmd2rscript("LECTURE5.Rmd")
rmd2rscript("LECTURE6.Rmd")
rmd2rscript("LECTURE6.Rmd")
rmd2rscript("LECTURE7.Rmd")  ##
rmd2rscript("LECTURE7.Rmd")  ##
rmd2rscript("LECTURE8.Rmd")
rmd2rscript("LECTURE8.Rmd")
rmd2rscript("LECTURE9.Rmd")
rmd2rscript("LECTURE9.Rmd")
rmd2rscript("LECTURE10.Rmd")
rmd2rscript("FigureDemo.Rmd")
rmd2rscript("LAB3demo.Rmd")
?kable
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  NRES 746, Lecture 1
#   University of Nevada, Reno
#   Computational algorithms vs standard statistics
# SALMON EXAMPLE (made-up!) ------------------
population.mean = 4.5
population.sd = 0.9
my.sample = c(3.14,3.27,2.56,3.77,3.34,4.32,3.84,2.19,5.24,3.09)
sample.size <- length(my.sample)     # determine sample size
obs.samplemean = mean(my.sample)     # note the equal sign as assignment operator
## visualize the population of conventional-raised salmon  -------------------
curve(dnorm(x,population.mean,population.sd),0,10,
xlab="Body mass (kg)",ylab="Probability density")
### now overlay this on the observed data  --------------------
hist(my.sample,freq=F,
xlab="Body mass (kg)",ylab="Probability density",main="",
xlim=c(0,10))
curve(dnorm(x,population.mean,population.sd),0,10,
col="red",lwd=2,add=T)
abline(v=obs.samplemean,col="blue",lwd=3)
# Perform "canned" z-test  ----------------------------
library(BSDA)
z.test(x=my.sample,mu=population.mean, sigma.x=population.sd,alternative = "less")
# alternative "canned" z-test  -----------------------
std.err = population.sd/sqrt(sample.size)
curve(dnorm(x,population.mean,std.err),0,10,     # visualize the sampling distribution under null hypothesis
xlab="Body mass (kg)",ylab="Probability density")     # versus the observed sample mean
abline(v=obs.samplemean,col="blue",lwd=3)
p.val = pnorm(obs.samplemean,population.mean,std.err)
p.val     # this is the same as the p value from the z-test above...
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  NRES 746, Lecture 2  ---------------------------
##  University of Nevada, Reno
##  Working with Probabilities
# Classic Urn Example ------------------------
n_red <- 104
n_blue <- 55
n_green <- 30
allSpheres <- c(n_red,n_blue,n_green)           # create vector of urn contents
names(allSpheres) <- c("red","blue","green")    # make it a named vector, for convenience!
P_blue <- allSpheres["blue"]/sum(allSpheres)     # probability of drawing a blue sphere
P_blue
Prob <- allSpheres/sum(allSpheres)    # probability of drawing each type of sphere
Prob
as.numeric( Prob["blue"] + Prob["red"] )     # probability of drawing a blue or red sphere
as.numeric( Prob["blue"] + Prob["red"] + Prob["green"] )      # P(blue OR green)
# Question: What is the probability of drawing a blue **AND THEN** a red sphere?
#[your command here]    # P(blue AND THEN red)
#### Question: What is the probability of drawing a blue and a red sphere in two consecutive draws (but in no particular order)?
#[your command here]    # P(blue AND THEN red)
as.numeric( (Prob["blue"] * Prob["red"]) + (Prob["red"] * Prob["blue"]) )    # P(blue then red OR red then blue)
Prob["blue"] * Prob["red"]) + (Prob["red"] * Prob["blue"])
as.numeric( (Prob["blue"] * Prob["red"]) + (Prob["red"] * Prob["blue"]) )    # P(blue then red OR red then blue)
(Prob["blue"]+Prob["red"])^2
(Prob["blue"]+Prob["red"])^2-Prob["blue"]^2-Prob["red"]^2
as.numeric( (Prob["blue"] * ) + (Prob["red"] * Prob["blue"]) )    # P(blue then red OR red then blue)
as.numeric( (Prob["blue"] * Prob["red"]) + (Prob["red"] * Prob["blue"]) )    # P(blue then red OR red then blue)
(Prob["blue"]+Prob["red"])^2-Prob["blue"]^2-Prob["red"]^2
n_red_sphere <- 39       # contents of new urn
n_blue_sphere <- 76
n_red_cube <- 101
n_blue_cube <- 25
allSpheres <- c(n_red_sphere,n_blue_sphere)         # build up matrix from vectors
allCubes <- c(n_red_cube,n_blue_cube)
allTypes <- c(allSpheres,allCubes)
allTypes <- matrix(allTypes,nrow=2,ncol=2,byrow=T)     # matrix of urn contents
rownames(allTypes) <- c("sphere","cube")               # name rows and columns
colnames(allTypes) <- c("red","blue")
allTypes
Prob_Shape <- apply(allTypes,1,sum)/sum(allTypes)  # marginal probabilities of shape
Prob_Shape
Prob_Color <- apply(allTypes,2,sum)/sum(allTypes)    # marginal probabilities of color
Prob_Color
allprobs <- (allTypes/sum(allTypes))
allprobs
allTypes
Prob_Shape <- apply(allTypes,1,sum)/sum(allTypes)  # marginal probabilities of shape
Prob_Shape
Prob_Color <- apply(allTypes,2,sum)/sum(allTypes)    # marginal probabilities of color
Prob_Color
allprobs <- (allTypes/sum(allTypes))
allprobs
source("~/.active-rstudio-document", echo=TRUE)
citation()
remotes::install_github("rstudio/gradethis")
rmarkdown::render('index.Rmd', 'word_document')
rmarkdown::render('schedule.Rmd', 'word_document')
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  NRES 746, Lecture 1
#   University of Nevada, Reno
#   Computational algorithms vs standard statistics
# SALMON EXAMPLE (made-up!) ------------------
population.mean = 4.5
population.sd = 0.9
my.sample = c(3.14,3.27,2.56,3.77,3.34,4.32,3.84,2.19,5.24,3.09)
sample.size <- length(my.sample)     # determine sample size
obs.samplemean = mean(my.sample)     # note the equal sign as assignment operator
## visualize the population of conventional-raised salmon  -------------------
curve(dnorm(x,population.mean,population.sd),0,10,
xlab="Body mass (kg)",ylab="Probability density")
### now overlay this on the observed data  --------------------
hist(my.sample,freq=F,
xlab="Body mass (kg)",ylab="Probability density",main="",
xlim=c(0,10))
curve(dnorm(x,population.mean,population.sd),0,10,
col="red",lwd=2,add=T)
abline(v=obs.samplemean,col="blue",lwd=3)
library(BSDA)
z.test(x=my.sample,mu=population.mean, sigma.x=population.sd,alternative = "less")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  NRES 746, Lecture 1
#   University of Nevada, Reno
#   Computational algorithms vs standard statistics
# SALMON EXAMPLE (made-up!) ------------------
population.mean = 4.5
population.sd = 0.9
my.sample = c(3.14,3.27,2.56,3.77,3.34,4.32,3.84,2.19,5.24,3.09)
sample.size <- length(my.sample)     # determine sample size
obs.samplemean = mean(my.sample)     # note the equal sign as assignment operator
## visualize the population of conventional-raised salmon  -------------------
curve(dnorm(x,population.mean,population.sd),0,10,
xlab="Body mass (kg)",ylab="Probability density")
### now overlay this on the observed data  --------------------
hist(my.sample,freq=F,
xlab="Body mass (kg)",ylab="Probability density",main="",
xlim=c(0,10))
curve(dnorm(x,population.mean,population.sd),0,10,
col="red",lwd=2,add=T)
abline(v=obs.samplemean,col="blue",lwd=3)
# Perform "canned" z-test  ----------------------------
library(BSDA)
z.test(x=my.sample,mu=population.mean, sigma.x=population.sd,alternative = "less")
# alternative "canned" z-test  -----------------------
std.err = population.sd/sqrt(sample.size)
curve(dnorm(x,population.mean,std.err),0,10,     # visualize the sampling distribution under null hypothesis
xlab="Body mass (kg)",ylab="Probability density")     # versus the observed sample mean
abline(v=obs.samplemean,col="blue",lwd=3)
p.val = pnorm(obs.samplemean,population.mean,std.err)
p.val     # this is the same as the p value from the z-test above...
std.err = population.sd/sqrt(sample.size)
std.err
curve(dnorm(x,population.mean,std.err),0,10,     # visualize the sampling distribution under null hypothesis
xlab="Body mass (kg)",ylab="Probability density")     # versus the observed sample mean
abline(v=obs.samplemean,col="blue",lwd=3)
curve(dnorm(x,population.mean,std.err),0,10,     # visualize the sampling distribution under null hypothesis
xlab="Body mass (kg)",ylab="Probability density")     # versus the observed sample mean
abline(v=obs.samplemean,col="blue",lwd=3)
p.val = pnorm(obs.samplemean,population.mean,std.err)
p.val     # this is the same as the p value from the z-test above...
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  NRES 746, Lecture 1
#   University of Nevada, Reno
#   Computational algorithms vs standard statistics
# SALMON EXAMPLE (made-up!) ------------------
population.mean = 4.5
population.sd = 0.9
my.sample = c(3.14,3.27,2.56,3.77,3.34,4.32,3.84,2.19,5.24,3.09)
sample.size <- length(my.sample)     # determine sample size
obs.samplemean = mean(my.sample)     # note the equal sign as assignment operator
## visualize the population of conventional-raised salmon  -------------------
curve(dnorm(x,population.mean,population.sd),0,10,
xlab="Body mass (kg)",ylab="Probability density")
### now overlay this on the observed data  --------------------
hist(my.sample,freq=F,
xlab="Body mass (kg)",ylab="Probability density",main="",
xlim=c(0,10))
curve(dnorm(x,population.mean,population.sd),0,10,
col="red",lwd=2,add=T)
abline(v=obs.samplemean,col="blue",lwd=3)
# Perform "canned" z-test  ----------------------------
library(BSDA)
z.test(x=my.sample,mu=population.mean, sigma.x=population.sd,alternative = "less")
# alternative "canned" z-test  -----------------------
std.err = population.sd/sqrt(sample.size)
curve(dnorm(x,population.mean,std.err),0,10,     # visualize the sampling distribution under null hypothesis
xlab="Body mass (kg)",ylab="Probability density")     # versus the observed sample mean
abline(v=obs.samplemean,col="blue",lwd=3)
p.val = pnorm(obs.samplemean,population.mean,std.err)
p.val     # this is the same as the p value from the z-test above...
# ALTERNATIVE ALGORITHMIC Z-TEST! ----------------------
## Simulate the STATISTICAL POPULATION under the null hypothesis -----------------
infinity <- 1000000  # large number approximating infinity
popData_null <- rnorm(n=infinity,mean=population.mean,sd=population.sd)    # the statistical "population" of interest (under null model w no 'treatment' effect)
## Draw a SAMPLE from that null data ----------------
null.sample <- sample(popData_null,size=sample.size)    # use R's native "sample()" function to sample from the null distribution
round(null.sample,2)
null.samplemean <- mean(null.sample)
null.samplemean    # here is one sample mean that we can generate under the null hypothesis
exp(0.01)
exp(-10)
exp(-10/100)
exp(-10/10)
exp(-10/1)
exp(-2)
exp(-2/100)
