#summary(jagsfit.mcmc)
#plot(jagsfit.mcmc)
cat("
model  {
### Likelihood for model 1: full
for(i in 1:n.obs){
expected.cones[i,1] <- a1[wave[i]]*pow(DBH[i],b1[wave[i]])       # a*DBH^b
spread.cones[i,1] <- r1[wave[i]]
p[i,1] <- spread.cones[i,1] / (spread.cones[i,1] + expected.cones[i,1])
observed.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
predicted.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
SE_obs[i,1] <- pow(observed.cones[i,1]-expected.cones[i,1],2)
SE_pred[i,1] <- pow(predicted.cones[i,1]-expected.cones[i,1],2)
}
### Priors, model 1
for(j in 1:2){   # estimate separately for wave and non-wave
a1[j] ~ dunif(0.001,2)
b1[j] ~ dunif(0.5,4)
r1[j] ~ dunif(0.5,5)
}
### Likelihood for model 2: reduced
for(i in 1:n.obs){
expected.cones[i,2] <- a2*pow(DBH[i],b2)       # a*DBH^b
spread.cones[i,2] <- r2
p[i,2] <- spread.cones[i,2] / (spread.cones[i,2] + expected.cones[i,2])
observed.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
predicted.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
SE_obs[i,2] <- pow(observed.cones[i,2]-expected.cones[i,2],2)
SE_pred[i,2] <- pow(predicted.cones[i,2]-expected.cones[i,2],2)
}
### Priors, model 2
a2 ~ dunif(0.001,2)
b2 ~ dunif(0.5,4)
r2 ~ dunif(0.5,5)
### Likelihood for model 3: constant a and b
for(i in 1:n.obs){
expected.cones[i,3] <- a3*pow(DBH[i],b3)       # a*DBH^b
spread.cones[i,3] <- r3[wave[i]]
p[i,3] <- spread.cones[i,3] / (spread.cones[i,3] + expected.cones[i,3])
observed.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
predicted.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
SE_obs[i,3] <- pow(observed.cones[i,3]-expected.cones[i,3],2)
SE_pred[i,3] <- pow(predicted.cones[i,3]-expected.cones[i,3],2)
}
SSE_obs[1] <- sum(SE_obs[,1])
SSE_pred[1] <- sum(SE_pred[,1])
SSE_obs[2] <- sum(SE_obs[,2])
SSE_pred[2] <- sum(SE_pred[,2])
SSE_obs[3] <- sum(SE_obs[,3])
SSE_pred[3] <- sum(SE_pred[,3])
### Priors, model 3
for(j in 1:2){   # estimate separately for wave and non-wave
r3[j] ~ dunif(0.5,5)
}
a3 ~ dunif(0.001,2)
b3 ~ dunif(0.5,4)
#####################
### SELECT THE BEST MODEL!!!
#####################
for(i in 1:n.obs){
observed.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])
predicted.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])     # for posterior predictive check!
SE2_obs[i] <- pow(observed.cones2[i]-expected.cones[i,selected])
SE2_pred[i] <- pow(predicted.cones2[i]-expected.cones[i,selected])
}
### Priors
# model selection...
prior[1] <- 1/5
prior[2] <- 3/5     # put substantially more weight because fewer parameters (there are more rigorous ways to do this!!)
prior[3] <- 1/5
selected ~ dcat(prior[])
}
",file="BUGS_fir_modelselection.txt")
cat("
model  {
### Likelihood for model 1: full
for(i in 1:n.obs){
expected.cones[i,1] <- a1[wave[i]]*pow(DBH[i],b1[wave[i]])       # a*DBH^b
spread.cones[i,1] <- r1[wave[i]]
p[i,1] <- spread.cones[i,1] / (spread.cones[i,1] + expected.cones[i,1])
observed.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
predicted.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
SE_obs[i,1] <- pow(observed.cones[i,1]-expected.cones[i,1],2)
SE_pred[i,1] <- pow(predicted.cones[i,1]-expected.cones[i,1],2)
}
### Priors, model 1
for(j in 1:2){   # estimate separately for wave and non-wave
a1[j] ~ dunif(0.001,2)
b1[j] ~ dunif(0.5,4)
r1[j] ~ dunif(0.5,5)
}
### Likelihood for model 2: reduced
for(i in 1:n.obs){
expected.cones[i,2] <- a2*pow(DBH[i],b2)       # a*DBH^b
spread.cones[i,2] <- r2
p[i,2] <- spread.cones[i,2] / (spread.cones[i,2] + expected.cones[i,2])
observed.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
predicted.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
SE_obs[i,2] <- pow(observed.cones[i,2]-expected.cones[i,2],2)
SE_pred[i,2] <- pow(predicted.cones[i,2]-expected.cones[i,2],2)
}
### Priors, model 2
a2 ~ dunif(0.001,2)
b2 ~ dunif(0.5,4)
r2 ~ dunif(0.5,5)
### Likelihood for model 3: constant a and b
for(i in 1:n.obs){
expected.cones[i,3] <- a3*pow(DBH[i],b3)       # a*DBH^b
spread.cones[i,3] <- r3[wave[i]]
p[i,3] <- spread.cones[i,3] / (spread.cones[i,3] + expected.cones[i,3])
observed.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
predicted.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
SE_obs[i,3] <- pow(observed.cones[i,3]-expected.cones[i,3],2)
SE_pred[i,3] <- pow(predicted.cones[i,3]-expected.cones[i,3],2)
}
SSE_obs[1] <- sum(SE_obs[,1])
SSE_pred[1] <- sum(SE_pred[,1])
SSE_obs[2] <- sum(SE_obs[,2])
SSE_pred[2] <- sum(SE_pred[,2])
SSE_obs[3] <- sum(SE_obs[,3])
SSE_pred[3] <- sum(SE_pred[,3])
### Priors, model 3
for(j in 1:2){   # estimate separately for wave and non-wave
r3[j] ~ dunif(0.5,5)
}
a3 ~ dunif(0.001,2)
b3 ~ dunif(0.5,4)
#####################
### SELECT THE BEST MODEL!!!
#####################
for(i in 1:n.obs){
observed.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])
predicted.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])     # for posterior predictive check!
SE2_obs[i] <- pow(observed.cones2[i]-expected.cones[i,selected])
SE2_pred[i] <- pow(predicted.cones2[i]-expected.cones[i,selected])
}
SSE2_obs <- sum(SE2_obs[])
SSE2_pred <- sum(SE2_pred[])
### Priors
# model selection...
prior[1] <- 1/5
prior[2] <- 3/5     # put substantially more weight because fewer parameters (there are more rigorous ways to do this!!)
prior[3] <- 1/5
selected ~ dcat(prior[])
}
",file="BUGS_fir_modelselection.txt")
params.to.monitor <- c("a1","b1","r1","a2","b2","r2","a3","b3","r3","selected","predicted.cones2","predicted.cones","SSE_obs","SSE_pred","SSE2_obs","SSE2_pred")
jags.fit <- jags(data=data.package,parameters.to.save=params.to.monitor,n.iter=5000,model.file="BUGS_fir_modelselection.txt",n.chains = 2,n.burnin = 1000,n.thin=2 )
cat("
model  {
### Likelihood for model 1: full
for(i in 1:n.obs){
expected.cones[i,1] <- a1[wave[i]]*pow(DBH[i],b1[wave[i]])       # a*DBH^b
spread.cones[i,1] <- r1[wave[i]]
p[i,1] <- spread.cones[i,1] / (spread.cones[i,1] + expected.cones[i,1])
observed.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
predicted.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
SE_obs[i,1] <- pow(observed.cones[i,1]-expected.cones[i,1],2)
SE_pred[i,1] <- pow(predicted.cones[i,1]-expected.cones[i,1],2)
}
### Priors, model 1
for(j in 1:2){   # estimate separately for wave and non-wave
a1[j] ~ dunif(0.001,2)
b1[j] ~ dunif(0.5,4)
r1[j] ~ dunif(0.5,5)
}
### Likelihood for model 2: reduced
for(i in 1:n.obs){
expected.cones[i,2] <- a2*pow(DBH[i],b2)       # a*DBH^b
spread.cones[i,2] <- r2
p[i,2] <- spread.cones[i,2] / (spread.cones[i,2] + expected.cones[i,2])
observed.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
predicted.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
SE_obs[i,2] <- pow(observed.cones[i,2]-expected.cones[i,2],2)
SE_pred[i,2] <- pow(predicted.cones[i,2]-expected.cones[i,2],2)
}
### Priors, model 2
a2 ~ dunif(0.001,2)
b2 ~ dunif(0.5,4)
r2 ~ dunif(0.5,5)
### Likelihood for model 3: constant a and b
for(i in 1:n.obs){
expected.cones[i,3] <- a3*pow(DBH[i],b3)       # a*DBH^b
spread.cones[i,3] <- r3[wave[i]]
p[i,3] <- spread.cones[i,3] / (spread.cones[i,3] + expected.cones[i,3])
observed.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
predicted.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
SE_obs[i,3] <- pow(observed.cones[i,3]-expected.cones[i,3],2)
SE_pred[i,3] <- pow(predicted.cones[i,3]-expected.cones[i,3],2)
}
SSE_obs[1] <- sum(SE_obs[,1])
SSE_pred[1] <- sum(SE_pred[,1])
SSE_obs[2] <- sum(SE_obs[,2])
SSE_pred[2] <- sum(SE_pred[,2])
SSE_obs[3] <- sum(SE_obs[,3])
SSE_pred[3] <- sum(SE_pred[,3])
### Priors, model 3
for(j in 1:2){   # estimate separately for wave and non-wave
r3[j] ~ dunif(0.5,5)
}
a3 ~ dunif(0.001,2)
b3 ~ dunif(0.5,4)
#####################
### SELECT THE BEST MODEL!!!
#####################
for(i in 1:n.obs){
observed.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])
predicted.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])     # for posterior predictive check!
SE2_obs[i] <- pow(observed.cones2[i]-expected.cones[i,selected],2)
SE2_pred[i] <- pow(predicted.cones2[i]-expected.cones[i,selected],2)
}
SSE2_obs <- sum(SE2_obs[])
SSE2_pred <- sum(SE2_pred[])
### Priors
# model selection...
prior[1] <- 1/5
prior[2] <- 3/5     # put substantially more weight because fewer parameters (there are more rigorous ways to do this!!)
prior[3] <- 1/5
selected ~ dcat(prior[])
}
",file="BUGS_fir_modelselection.txt")
params.to.monitor <- c("a1","b1","r1","a2","b2","r2","a3","b3","r3","selected","predicted.cones2","predicted.cones","SSE_obs","SSE_pred","SSE2_obs","SSE2_pred")
jags.fit <- jags(data=data.package,parameters.to.save=params.to.monitor,n.iter=5000,model.file="BUGS_fir_modelselection.txt",n.chains = 2,n.burnin = 1000,n.thin=2 )
jagsfit.mcmc <- as.mcmc(jags.fit)   # convert to "MCMC" object (coda package)
BUGSlist <- as.data.frame(jags.fit$BUGSoutput$sims.list)
#summary(jagsfit.mcmc)
#plot(jagsfit.mcmc)
plot(jagsfit.mcmc[,"a1[1]"])
plot(jagsfit.mcmc[,"r1[2]"])
n.iterations <- length(jags.fit$BUGSoutput$sims.list$selected)
selected <- table(jags.fit$BUGSoutput$sims.list$selected)
names(selected) <- c("Full model","No wave","Fixed a&b")
selected
barplot(selected/n.iterations,ylab="Degree of belief")
n.data <- length(fir$DBH)
plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)
for(d in 1:n.data){
tofind <- sprintf("predicted.cones[%s,1]",d)
model1 <- as.vector(jagsfit.mcmc[,tofind])
points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col="gray",cex=0.4)
}
plot(jagsfit.mcmc[,"SSE_pred[,1]"][[1]]~jagsfit.mcmc[,"SSE_obs[,1]"][[1]],xlab="SSE, real data",ylab="SSE, perfect data")
jagsfit.mcmc[,"SSE_pred[,1]"]
plot(jagsfit.mcmc[,"SSE_pred[1]"][[1]]~jagsfit.mcmc[,"SSE_obs[1]"][[1]],xlab="SSE, real data",ylab="SSE, perfect data")
plot(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[1]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data")
?abline
plot(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[1]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data")
abline(0,1,col="red")
plot(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[2]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data")
abline(0,1,col="red")
plot(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[2]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
plot(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[3]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[3]"][[1]]))/length(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[3]"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])
)
p.value
p.value
plot(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[2]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[2]"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]]))
p.value
p.value
plot(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[1]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[1]"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]]))
p.value
plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)
for(d in 1:n.data){
tofind <- sprintf("predicted.cones2[%s]",d)
model1 <- as.vector(jagsfit.mcmc[,tofind])
points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col="gray",cex=0.4)
}
plot(as.vector(jagsfit.mcmc[,"SSE2_pred"][[1]])~as.vector(jagsfit.mcmc[,"SSE2_obs"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE2_pred"][[1]])>as.vector(jagsfit.mcmc[,"SSE2_obs"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE2_pred"][[1]]))
p.value
install.packages("NetIndices")
install.packages("igraphdata")
install.packages(c("mgcv", "nlme"))
data("airquality")
airquality
ozone.data<-  airquality  #read.csv("ozone.csv", header = T)
head(ozone.data) # ozone is the response variable while the others are predictors
pairs(ozone.data, panel = function(x,y){
points(x,y)
lines(lowess(x,y),lwd=2,col="red")
})
library(mgcv)
library(nlme)
data("airquality")
#setwd("C:\\Users\\tbisrael\\Documents\\Academic Classes\\Fall 2016\\NRES 746\\GAMs")
ozone.data<-  airquality  #read.csv("ozone.csv", header = T)
head(ozone.data) # ozone is the response variable while the others are predictors
pairs(ozone.data, panel = function(x,y){
points(x,y)
lines(lowess(x,y),lwd=2,col="red")
})
names(ozone.data)
ozone.data$rad <- ozone.data$Solar.R
ozone.data$rad <- ozone.data$Solar.R
ozone.data$temp <- ozone.data$temp
ozone.data$wind <- ozone.data$Wind
ozone.data$ozone <- ozone.data$Ozone
pairs(ozone.data, panel = function(x,y){
points(x,y)
lines(lowess(x,y),lwd=2,col="red")
})
pairs(ozone.data)
ozone.data <- ozone.data(,c("ozone","rad","wind","temp"))
ozone.data <- ozone.data[,c("ozone","rad","wind","temp")]
ozone.data$rad <- ozone.data$Solar.R
ozone.data$temp <- ozone.data$temp
ozone.data$wind <- ozone.data$Wind
ozone.data$ozone <- ozone.data$Ozone
ozone.data[,c("ozone","rad","wind","temp")]
ozone.data[,c("temp")]
head(ozone.data)
ozone.data$temp <- ozone.data$temp
ozone.data
ozone.data$rad <- ozone.data$Solar.R
ozone.data$wind <- ozone.data$Wind
ozone.data <- ozone.data[,c("ozone","rad","wind","temp")]
ozone.data$temp
ozone.data
ozone.data$Temp
ozone.data$temp <- ozone.data$Temp
ozone.data <- ozone.data[,c("ozone","rad","wind","temp")]
pairs(ozone.data)
plot(ozone.data$ozone)
install.packages("forecast")
install.packages("TTR")
PPT_mth<-read.csv("PRISM_tmean_189511_201610.csv", head=T, skip=10)
str(PPT_mth) # looking at the structure of the data
ts(PPT_mth$ppt..inches,start=c(1895,1), frequency =12, end=c(1968)) ->pptimeseries # reading in the data in a time series format
PPT_mth$ppt..inches
PPT_mth
AIC.func<-function(model.list,n,modelnames) {
output<-NULL
for (i in 1:length(model.list)) {
cur.model<-model.list[[i]]
LL<- -cur.model$minimum
K<-length(cur.model$estimate)
AIC<- -2*LL + 2*K
AICc<-AIC + 2*K*(K+1)/(n-K-1)
output<-rbind(output,c(LL,K,AIC,AICc))
}
colnames(output)<-c('LogL','K','AIC','AICc')
minAICc<-min(output[,"AICc"])
deltai<-output[,"AICc"]-minAICc
rel.like<-exp(-deltai/2)
wi<-round(rel.like/sum(rel.like),3)
out<-data.frame(modelnames,output,deltai,wi)[order(deltai,decreasing = F),]
out
}
AIC.func(models,80,model.names)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
slugs<-read.table( 'http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/slugsurvey.txt', header=TRUE)
head(slugs)
table(slugs$slugs,slugs$field)
out <- table(slugs$slugs,slugs$field)
barplot(out)
barplot(t(out), beside=TRUE, angle=c(45,135), density=c(20,20), col=c('black','red'), legend.text=TRUE, xlab='# of slugs', ylab='frequency')
coords<-barplot(t(out), beside=TRUE, angle=c(45,135), density=c(20,20), ylim=c(0,27), col=c('black','red'), xlab='# of slugs', ylab='frequency')
box()
legend(coords[1,8], 26, c('nursery','rookery'), density=c(20,20), angle=c(45,135), fill=c('black','red'), cex=c(.8,.8),bty='n')
poi.1<-function(data,p) -sum(log(dpois(data$slugs,lambda=p)))
mean(slugs$slugs)
out1 <- nlm(function(p) poi.1(slugs,p),2)
out1
as.numeric(slugs$field)
poi.2<-function(data,p) {
field.dummy<-as.numeric(data$field)-1
mylambda<-p[1]+p[2]*field.dummy
negloglike<- -sum(log(dpois(data$slugs,lambda=mylambda)))
negloglike
}
tapply(slugs$slugs,slugs$field,mean)
out2 <- nlm(function(p) poi.2(slugs,p),c(1.2,1))
out2
out1$minimum
out2$minimum
my.aic<-function(output) -2*(-output$minimum) + 2*length(output$estimate)
my.aic(out1)
my.aic(out2)
#common lambda and theta
zip1<-function(data,p) {
lambda<-p[1]
theta<-p[2]
zero.term<-sum(log(theta+(1-theta)* dpois(data$slugs[data$slugs==0], lambda)))
nonzero.term<-sum(log((1- theta)* dpois(data$slugs[data$slugs>0], lambda)))
negloglike<- -(zero.term+nonzero.term)
negloglike
}
mean(slugs$slugs[slugs$slugs>0])
table(slugs$slugs)[1]/sum(table(slugs$slugs))
out7 <- nlm(function(p) zip1(slugs,p),c(3,.4))
out7
my.aic(out7)
# different lambda, same theta
zip2<-function(data,p) {
field.dummy<-as.numeric(slugs$field)-1
mylambda<-p[1]+p[3]*field.dummy
theta<-p[2]
zero.term<-sum(ifelse(data$slugs==0,log(theta+(1-theta)* dpois(data$slugs,lambda=mylambda)),0))
nonzero.term<-sum(ifelse(data$slugs>0,log((1-theta)* dpois(data$slugs,lambda=mylambda)),0))
negloglike<- -(zero.term+nonzero.term)
negloglike
}
zip2.alt<-function(data,p) {
field.dummy<-as.numeric(slugs$field)-1
mylambda<-p[1]+p[3]*field.dummy
theta<-p[2]
negloglike<- -sum(ifelse(data$slugs==0,log(theta+(1-theta)* dpois(data$slugs,lambda=mylambda)),log((1-theta)*dpois(data$slugs,lambda=mylambda))))
negloglike
}
tapply(slugs$slugs[slugs$slugs>0],slugs$field[slugs$slugs>0],mean)
out8 <- nlm(function(p) zip2(slugs,p),c(3.4,.42,-.4))
out8
#different theta
zip3<-function(data,p){
field.dummy<-as.numeric(data$field)-1
mylambda<-p[1]
theta<-p[2]+p[3]*field.dummy
zero.term<-sum(ifelse(data$slugs==0,log(theta+(1-theta)* dpois(data$slugs,lambda=mylambda)),0))
nonzero.term<-sum(ifelse(data$slugs>0,log((1-theta)* dpois(data$slugs,lambda=mylambda)),0))
negloglike<- -(zero.term+nonzero.term)
negloglike
}
table(slugs$slugs,slugs$field)[1,]/40
out9 <- nlm(function(p) zip3(slugs,p),c(3,.6,-.4))
out9
norm.neglike<-function(data,p) {
t.y<-log(data$slugs+1)
mu<-p[1]
my.sd<-p[2]
negloglike<- -sum(log(dnorm(t.y,mean=mu,sd=my.sd)))
negloglike
}
mean(log(slugs$slugs+1))
sd(log(slugs$slugs+1))
out.norm <- nlm(function(p) norm.neglike(slugs,p),c(.73,.74))
out.norm
#calculate negative loglikelihood for AIC
norm.like<-function(data,out) {
t.y<-log(data$slugs+1)
mu<-out$estimate[1]
my.sd<-out$estimate[2]
negloglike<- -sum(log(dnorm(t.y,mean=mu, sd=my.sd)*1/(data$slugs+1)))
out<-list(negloglike,out$estimate)
names(out)<-c("minimum","estimate")
out
}
out20 <- norm.like(slugs,out.norm)
out20
tapply(log(slugs$slugs+1),slugs$field,mean)
norm.neglike2<-function(data,p) {
t.y<-log(data$slugs+1)
field.dummy<-as.numeric(data$field)-1
mu<-p[1]+field.dummy*p[3]
my.sd<-p[2]
negloglike<- -sum(log(dnorm(t.y,mean=mu,sd=my.sd)))
negloglike
}
outnorm2 <- nlm(function(p) norm.neglike2(slugs,p),c(.5,.7,.5))
outnorm2
norm.like2<-function(data,out) {
t.y<-log(data$slugs+1)
field.dummy<-as.numeric(data$field)-1
mu<-out$estimate[1]+field.dummy*out$estimate[3]
my.sd<-out$estimate[2]
negloglike<- -sum(log(dnorm(t.y,mean=mu,
sd=my.sd)*1/(data$slugs+1)))
out<-list(negloglike,out$estimate)
names(out)<-c("minimum","estimate")
out
}
out21 <- norm.like2(slugs,outnorm2)
my.aic(out21)
my.aic(out9)
model.names<-c('Pois.common','Pois.mean','Zip.common', 'Zip.mean','Zip.theta','lognormal','lognormal.mean')
models<-list(out1,out2,out7,out8,out9,out20,out21)
AIC.func<-function(model.list,n,modelnames) {
output<-NULL
for (i in 1:length(model.list)) {
cur.model<-model.list[[i]]
LL<- -cur.model$minimum
K<-length(cur.model$estimate)
AIC<- -2*LL + 2*K
AICc<-AIC + 2*K*(K+1)/(n-K-1)
output<-rbind(output,c(LL,K,AIC,AICc))
}
colnames(output)<-c('LogL','K','AIC','AICc')
minAICc<-min(output[,"AICc"])
deltai<-output[,"AICc"]-minAICc
rel.like<-exp(-deltai/2)
wi<-round(rel.like/sum(rel.like),3)
out<-data.frame(modelnames,output,deltai,wi)[order(deltai,decreasing = F),]
out
}
AIC.func(models,80,model.names)
install.packages(c("tseries", "forecast", "TTR"))
PPT_mth<-read.csv("PRISM_ppt_1895-2015Mo2.csv", head=T, skip=10)
PPT_mth<-read.csv("PRISM_tmean_189511_201610.csv", head=T, skip=10)
str(PPT_mth) # looking at the structure of the data
ts(PPT_mth$ppt..inches,start=c(1895,1), frequency =12, end=c(1968)) ->pptimeseries # reading in the data in a time series format
unlink('LECTURE9_cache', recursive = TRUE)
install.packages(c("vegan", "vegan3d"))
install.packages(c("scatterplot3d", "rgl"))
unlink('LECTURE8_cache', recursive = TRUE)
install.packages("jagsUI")
