# Simple example of MCMC sampling -----------------------
# first, let's build a function that generates random numbers from a bivariate standard normal distribution
rbvn<-function (n, rho)   #function for drawing an arbitrary number of independent samples from the bivariate standard normal distribution.
{
x <- rnorm(n, 0, 1)
y <- rnorm(n, rho * x, sqrt(1 - rho^2))
cbind(x, y)
}
# Now, plot the random draws from this distribution, make sure this makes sense!
bvn<-rbvn(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type="l")
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)
par(mfrow=c(1,1))
# Metropolis-Hastings implementation of bivariate normal sampler...
library(mvtnorm)    # load a package that allows us to compute probability densities for mv normal distribution
metropolisHastings <- function (n, rho=0.98){    # an MCMC sampler implementation of a bivariate random number generator
mat <- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
x <- 0   # initial values for all parameters
y <- 0
prev <- mvtnorm::dmvnorm(c(x,y),mean=c(0,0),sigma = matrix(c(1,rho,rho,1),ncol=2))   # probability density of the distribution at the starting values
mat[1, ] <- c(x, y)        # initialize the markov chain
counter <- 1
while(counter<=n) {
newx <- rnorm(1,x,0.5)     # make a jump. Note the symmetrical proposal distribution
newy <- rnorm(1,y,0.5)
newprob <- dmvnorm(c(newx,newy),sigma = matrix(c(1,rho,rho,1),ncol=2))    # assess whether the new jump is good!
ratio <- newprob/prev   # compute the ratio of probabilities at the old (jump from) and proposed (jump to) locations.
prob.accept <- min(1,ratio)     # decide the probability of accepting the new jump!
rand <- runif(1)
if(rand<=prob.accept){
x=newx;y=newy    # set x and y to the new location
mat[counter,] <- c(x,y)    # store this in the storage array
prev <- newprob    # get ready for the next iteration
}else{
mat[counter,] <- c(x,y)
}
counter=counter+1
}
return(mat)
}
metropolisHastings <- function (n, rho=0.98){    # an MCMC sampler implementation of a bivariate random number generator
mat <- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
x <- 0   # initial values for all parameters
y <- 0
prev <- mvtnorm::dmvnorm(c(x,y),mean=c(0,0),sigma = matrix(c(1,rho,rho,1),ncol=2))   # probability density of the distribution at the starting values
mat[1, ] <- c(x, y)        # initialize the markov chain
counter <- 1
while(counter<=n) {
newx <- rnorm(1,x,0.5)     # make a jump. Note the symmetrical proposal distribution
newy <- rnorm(1,y,0.5)
newprob <- mvtnorm::dmvnorm(c(newx,newy),sigma = matrix(c(1,rho,rho,1),ncol=2))    # assess whether the new jump is good!
ratio <- newprob/prev   # compute the ratio of probabilities at the old (jump from) and proposed (jump to) locations.
prob.accept <- min(1,ratio)     # decide the probability of accepting the new jump!
rand <- runif(1)
if(rand<=prob.accept){
x=newx;y=newy    # set x and y to the new location
mat[counter,] <- c(x,y)    # store this in the storage array
prev <- newprob    # get ready for the next iteration
}else{
mat[counter,] <- c(x,y)
}
counter=counter+1
}
return(mat)
}
# Test the new M-H sampler
bvn<-metropolisHastings(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type="l")
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)
par(mfrow=c(1,1))
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#  NRES 746, Lecture 7
#   Bayesian analysis #2: MCMC  ---------------------------------
# Simple example of MCMC sampling -----------------------
# first, let's build a function that generates random numbers from a bivariate standard normal distribution
rbvn<-function (n, rho)   #function for drawing an arbitrary number of independent samples from the bivariate standard normal distribution.
{
x <- rnorm(n, 0, 1)
y <- rnorm(n, rho * x, sqrt(1 - rho^2))
cbind(x, y)
}
# Now, plot the random draws from this distribution, make sure this makes sense!
bvn<-rbvn(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type="l")
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)
par(mfrow=c(1,1))
# Metropolis-Hastings implementation of bivariate normal sampler...
library(mvtnorm)    # load a package that allows us to compute probability densities for mv normal distribution
metropolisHastings <- function (n, rho=0.98){    # an MCMC sampler implementation of a bivariate random number generator
mat <- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
x <- 0   # initial values for all parameters
y <- 0
prev <- mvtnorm::dmvnorm(c(x,y),mean=c(0,0),sigma = matrix(c(1,rho,rho,1),ncol=2))   # probability density of the distribution at the starting values
mat[1, ] <- c(x, y)        # initialize the markov chain
counter <- 1
while(counter<=n) {
newx <- rnorm(1,x,0.5)     # make a jump. Note the symmetrical proposal distribution
newy <- rnorm(1,y,0.5)
newprob <- mvtnorm::dmvnorm(c(newx,newy),sigma = matrix(c(1,rho,rho,1),ncol=2))    # assess whether the new jump is good!
ratio <- newprob/prev   # compute the ratio of probabilities at the old (jump from) and proposed (jump to) locations.
prob.accept <- min(1,ratio)     # decide the probability of accepting the new jump!
rand <- runif(1)
if(rand<=prob.accept){
x=newx;y=newy    # set x and y to the new location
mat[counter,] <- c(x,y)    # store this in the storage array
prev <- newprob    # get ready for the next iteration
}else{
mat[counter,] <- c(x,y)
}
counter=counter+1
}
return(mat)
}
# Test the new M-H sampler
bvn<-metropolisHastings(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type="l")
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)
par(mfrow=c(1,1))
# MCMC implementation of the Myxomatosis example from the Bolker book --------------
library(emdbook)
MyxDat <- MyxoTiter_sum
Myx <- subset(MyxDat,grade==1)
head(Myx)
# Visualize the Myxomatosis data for the 100th time!
hist(Myx$titer,freq=FALSE)
# ... and overlay a proposed data-generating model (gamma distribution)
hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col="red")
# define 2-D parameter space!
shapevec <- seq(3,100,by=0.1)
scalevec <- seq(0.01,0.5,by=0.001)
# define the likelihood surface  -------------
GammaLogLikelihoodFunction <- function(params){
sum(dgamma(Myx$titer,shape=params['shape'],scale=params['scale'],log=T))
}
surface2D <- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable
newparams <- c(shape=50,scale=0.2)
for(i in 1:length(shapevec)){
newparams['shape'] <- shapevec[i]
for(j in 1:length(scalevec)){
newparams['scale'] <- scalevec[j]
surface2D[i,j] <- GammaLogLikelihoodFunction(newparams)
}
}
# Visualize the likelihood surface
image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
# Write a non-log-transformed likelihood function ------------
GammaLikelihoodFunction <- function(params){
prod(dgamma(Myx$titer,shape=params['shape'],scale=params['scale'],log=F))
}
# and here's the log likelihood function
GammaLogLikelihoodFunction <- function(params){
sum(dgamma(Myx$titer,shape=params['shape'],scale=params['scale'],log=T))
}
params <- c(shape=40,scale=0.15)
params
GammaLikelihoodFunction(params)
GammaLogLikelihoodFunction(params)
# Function for returning the prior probability density for any point in parameter space
GammaPriorFunction <- function(params){
prior <- c(shape=NA,scale=NA)
prior['shape'] <- dgamma(params['shape'],shape=0.001,scale=1000)
prior['scale'] <- dgamma(params['scale'],shape=0.001,scale=1000)
# prior['shape'] <- dunif(params['shape'],3,100)        # alternative: could use uniform prior!
# prior['scale'] <- dunif(params['scale'],0.01,0.5)
return(prod(prior))
}
GammaLogPriorFunction <- function(params){
prior <- c(shape=NA,scale=NA)
prior['shape'] <- dgamma(params['shape'],shape=0.001,scale=1000,log=T)
prior['scale'] <- dgamma(params['scale'],shape=0.001,scale=1000,log=T)
# prior['shape'] <- dunif(params['shape'],3,100)        # alternative: could use uniform prior!
# prior['scale'] <- dunif(params['scale'],0.01,0.5)
return(sum(prior))
}
curve(dgamma(x,shape=0.001,scale=1000),3,100)
params <- c(shape=40,scale=0.15)
params
GammaPriorFunction(params)
prior2D <- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable
newparams <- c(shape=50,scale=0.2)
for(i in 1:length(shapevec)){
newparams['shape'] <- shapevec[i]
for(j in 1:length(scalevec)){
newparams['scale'] <- scalevec[j]
prior2D[i,j] <- GammaPriorFunction(newparams)
}
}
# Visualize the prior likelihood surface
image(x=shapevec,y=scalevec,z=prior2D,zlim=c(0.000000001,0.001),col=topo.colors(12))
#contour(x=shapevec,y=scalevec,z=prior2D,levels=c(-30,-40,-80,-500),add=T)
# Function for computing the ratio of posterior densities -----------------
PosteriorRatio <- function(oldguess,newguess){
oldLik <- max(1e-90,GammaLikelihoodFunction(oldguess))   # compute likelihood and prior density at old guess
oldPrior <- max(1e-90,GammaPriorFunction(oldguess))
newLik <- GammaLikelihoodFunction(newguess)             # compute likelihood and prior density at new guess
newPrior <- GammaPriorFunction(newguess)
return((newLik*newPrior)/(oldLik*oldPrior))          # compute ratio of weighted likelihoods
}
PosteriorRatio2 <- function(oldguess,newguess){
oldLogLik <- GammaLogLikelihoodFunction(oldguess)   # compute likelihood and prior density at old guess
oldLogPrior <- GammaLogPriorFunction(oldguess)
newLogLik <- GammaLogLikelihoodFunction(newguess)             # compute likelihood and prior density at new guess
newLogPrior <- GammaLogPriorFunction(newguess)
return(exp((newLogLik+newLogPrior)-(oldLogLik+oldLogPrior)))          # compute ratio of weighted likelihoods
}
oldguess <- params
newguess <- c(shape=39,scale=0.15)
PosteriorRatio(oldguess,newguess)
PosteriorRatio2(oldguess,newguess)
# Define proposal distribution --------------------------
#for jumps in parameter space (use normal distribution)!
# function for making new guesses
newGuess <- function(oldguess){
sdshapejump <- 4
sdscalejump <- 0.07
jump <- c(shape=rnorm(1,mean=0,sd=sdshapejump),scale=rnorm(1,0,sdscalejump))
newguess <- abs(oldguess + jump)    # note: by taking the abs val to avoid negative numbers, our proposal jump probs are not strictly symmetrical, but this should not present a big issue in practice
return(newguess)
}
# set a new "guess" near to the original guess
newGuess(oldguess=params)
newGuess(oldguess=params)
newGuess(oldguess=params)
# Set a starting point in parameter space -------------------
startingvals <- c(shape=75,scale=0.28)    # starting point for the algorithm
startingvals <- c(shape=75,scale=0.28)    # starting point for the algorithm
newguess <- newGuess(startingvals)    # take a jump in parameter space
newguess
startingvals
newguess
# function for making new guesses
newGuess <- function(oldguess){
sdshapejump <- 1 #4
sdscalejump <- 0.01
jump <- c(shape=rnorm(1,mean=0,sd=sdshapejump),scale=rnorm(1,0,sdscalejump))
newguess <- abs(oldguess + jump)    # note: by taking the abs val to avoid negative numbers, our proposal jump probs are not strictly symmetrical, but this should not present a big issue in practice
return(newguess)
}
newGuess(oldguess=params)
newGuess(oldguess=params)
newGuess(oldguess=params)
startingvals <- c(shape=75,scale=0.28)    # starting point for the algorithm
startingvals
newguess <- newGuess(startingvals)    # take a jump in parameter space
newguess
PosteriorRatio2(startingvals,newguess)   # difference in posterior ratio
startingvals
newguess <- newGuess(startingvals)    # take a jump in parameter space
newguess
# function for making new guesses
newGuess <- function(oldguess){
sdshapejump <- .5 #4
sdscalejump <- 0.01
jump <- c(shape=rnorm(1,mean=0,sd=sdshapejump),scale=rnorm(1,0,sdscalejump))
newguess <- abs(oldguess + jump)    # note: by taking the abs val to avoid negative numbers, our proposal jump probs are not strictly symmetrical, but this should not present a big issue in practice
return(newguess)
}
newGuess(oldguess=params)
newGuess(oldguess=params)
newGuess(oldguess=params)
newGuess(oldguess=params)
newGuess(oldguess=params)
newGuess(oldguess=params)
startingvals <- c(shape=75,scale=0.28)    # starting point for the algorithm
startingvals
newguess <- newGuess(startingvals)    # take a jump in parameter space
newguess
PosteriorRatio2(startingvals,newguess)   # difference in posterior ratio
chain.length <- 11
oldguess <- startingvals
guesses <- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) <- names(startingvals)
guesses[1,] <- startingvals
chain.length <- 11
oldguess <- startingvals
guesses <- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) <- names(startingvals)
guesses[1,] <- startingvals
counter <- 2
while(counter <= chain.length){
newguess <- newGuess(oldguess)
post.rat <- PosteriorRatio2(oldguess,newguess)
prob.accept <- min(1,post.rat)
rand <- runif(1)
if(rand<=prob.accept){
oldguess <- newguess
guesses[counter,] <- newguess
}else{
guesses[counter,] <- oldguess
}
counter=counter+1
}
# Visualize the Metropolis-Hastings routine: ---------------
chain.length <- 11
oldguess <- startingvals
guesses <- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) <- names(startingvals)
guesses[1,] <- startingvals
counter <- 2
while(counter <= chain.length){
newguess <- newGuess(oldguess)
post.rat <- PosteriorRatio2(oldguess,newguess)
prob.accept <- min(1,post.rat)
rand <- runif(1)
if(rand<=prob.accept){
oldguess <- newguess
guesses[counter,] <- newguess
}else{
guesses[counter,] <- oldguess
}
counter=counter+1
}
# visualize!
image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col="red")
# Get more MCMC samples --------------
chain.length <- 100
oldguess <- startingvals
guesses <- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) <- names(startingvals)
guesses[1,] <- startingvals
counter <- 2
while(counter <= chain.length){
newguess <- newGuess(oldguess)
post.rat <- PosteriorRatio2(oldguess,newguess)
prob.accept <- min(1,post.rat)
rand <- runif(1)
if(rand<=prob.accept){
oldguess <- newguess
guesses[counter,] <- newguess
}else{
guesses[counter,] <- oldguess
}
counter=counter+1
}
# visualize!
image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col="red")
# And more... -------------------
chain.length <- 1000
oldguess <- startingvals
guesses <- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) <- names(startingvals)
guesses[1,] <- startingvals
counter <- 2
while(counter <= chain.length){
newguess <- newGuess(oldguess)
post.rat <- PosteriorRatio2(oldguess,newguess)
prob.accept <- min(1,post.rat)
rand <- runif(1)
if(rand<=prob.accept){
oldguess <- newguess
guesses[counter,] <- newguess
}else{
guesses[counter,] <- oldguess
}
counter=counter+1
}
# visualize!
image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col="red")
# And more... -------------------
chain.length <- 10000
oldguess <- startingvals
guesses <- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) <- names(startingvals)
guesses[1,] <- startingvals
counter <- 2
while(counter <= chain.length){
newguess <- newGuess(oldguess)
post.rat <- PosteriorRatio2(oldguess,newguess)
prob.accept <- min(1,post.rat)
rand <- runif(1)
if(rand<=prob.accept){
oldguess <- newguess
guesses[counter,] <- newguess
}else{
guesses[counter,] <- oldguess
}
counter=counter+1
}
# visualize!
image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col="red")
# Evaluate "traceplot" for the MCMC samples... ---------------------
## Shape parameter
plot(1:chain.length,guesses[,'shape'],type="l",main="shape parameter",xlab="iteration",ylab="shape")
guesses
oldguess
install.packages("titanic")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(titanic)
data(titanic::titanic_train)
titanic::titanic_train
titanic <- titanic_train
View(titanic)
?titanic
?titanic_train
library(titanic)
titanic <- titanic::titanic_train
## write jags code
fn <- "titanic_jags.txt"
library(titanic)
titanic <- titanic::titanic_train
## write jags code
fn <- "titanic_jags.txt"
cat("
model{
# likelihood
for(p in 1:npassengers){
logit(psurv[p]) <- psurv0.l[class[p]] + b.fare * fare[p]  + b.age * age[p] + b.female * is.fem[p]
survived[p] ~ dbern(psurv[p])
}
# priors
b.fare ~ dnorm(0,0.1)  # slightly regularized prior
b.age ~ dnorm(0,0.1)
b.female ~ dnorm(0,0.1)
psurv0[1] ~ dunif(0,1)  # flat prior from 0 to 1 on p scale
psurv0[2] ~ dunif(0,1)
psurv0[3] ~ dunif(0,1)
for(i in 1:3){    # convert to logit scale
psurv0.l[i] <- log(psurv[i]/(1-psurv[i]))
}
## interpolate missing data
meanage ~ dnorm(0,.1)
sdage ~ dunif(0,5)
precage <- pow(sdage,-2)
for(p in 1:npassengers){
age[p] ~ dnorm(meanage,precage)   # DATA NODE
}
} ",file=fn)
dat <- list(
npassengers = nrow(titanic),
class = titanic$Pclass,
fare = (titanic$Fare-mean(titanic$Fare))/sd(titanic$Fare),
age = (titanic$Age-mean(titanic$Age,na.rm = T))/sd(titanic$Age,na.rm=T),
is.fem = ifelse(titanic$Sex=="female",1,0),
survived = titanic$Survived
)
dat
params <- c("psurv0","b.fare","b.age","b.female","sdage","meanage")
library(jagsUI)
jags(dat, parameters.to.save=params, model.file=fn,
n.chains=3, n.adapt=1000, n.iter=10000, n.burnin=5000, n.thin=2,
parallel=FALSE)
## write jags code
fn <- "titanic_jags.txt"
cat("
model{
# likelihood
for(p in 1:npassengers){
logit(psurv[p]) <- psurv0.l[class[p]] + b.fare * fare[p]  + b.age * age[p] + b.female * is.fem[p]
survived[p] ~ dbern(psurv[p])
}
# priors
b.fare ~ dnorm(0,0.1)  # slightly regularized prior
b.age ~ dnorm(0,0.1)
b.female ~ dnorm(0,0.1)
psurv0[1] ~ dunif(0,1)  # flat prior from 0 to 1 on p scale
psurv0[2] ~ dunif(0,1)
psurv0[3] ~ dunif(0,1)
for(i in 1:3){    # convert to logit scale
psurv0.l[i] <- log(psurv0[i]/(1-psurv0[i]))
}
## interpolate missing data
meanage ~ dnorm(0,.1)
sdage ~ dunif(0,5)
precage <- pow(sdage,-2)
for(p in 1:npassengers){
age[p] ~ dnorm(meanage,precage)   # DATA NODE
}
} ",file=fn)
dat <- list(
npassengers = nrow(titanic),
class = titanic$Pclass,
fare = (titanic$Fare-mean(titanic$Fare))/sd(titanic$Fare),
age = (titanic$Age-mean(titanic$Age,na.rm = T))/sd(titanic$Age,na.rm=T),
is.fem = ifelse(titanic$Sex=="female",1,0),
survived = titanic$Survived
)
dat
params <- c("psurv0","b.fare","b.age","b.female","sdage","meanage")
library(jagsUI)
jags(dat, parameters.to.save=params, model.file=fn,
n.chains=3, n.adapt=1000, n.iter=10000, n.burnin=5000, n.thin=2,
parallel=FALSE)
