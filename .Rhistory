indexA <- which(dat[,group]=="A")
observed_dif <- mean(dat[indexA,value]) - mean(dat[indexB,value])
h
h
indexA <- which(dat[,group]=="A")
indexB <- which(dat[,group]=="B")
observed_dif <- mean(dat[indexA,value]) - mean(dat[indexB,value])
observed_dif
t.test.algorithm <- function(dat = reshape_df, group = "Treatment", value = "Mass" ){
#############
# Compute the sample statistic
#############
indexA <- which(dat[,group]=="A")
indexB <- which(dat[,group]=="B")
observed_dif <- mean(dat[indexA,value]) - mean(dat[indexB,value])
#############
# Simulate the STATISTICAL POPULATION under the null hypothesis
#############
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(dat[,value])           # assume groups A and B come from a population with common mean
popSD_null <- sd(dat[,value])                      # and common standard deviation...
popData_null <- rnorm(n=lots,mean=popMean_null,sd=popSD_null)    # the statistical "population" of interest (under null model w no treatment effect)
#################
# Repeat sampling process (sampling from population) using a FOR loop
#################
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect (again!)
null_difs[i] <- mean(sampleA)-mean(sampleB)           # compute and store the sampling error produced under the null hypothesis
}
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
to_return <- list()   # initialize object to return
to_return$null_difs <- null_difs
to_return$p_value <- p_value
to_return$observed_dif <- observed_dif
return(to_return)
}
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(dat[,value])           # assume groups A and B come from a population with common mean
popSD_null <- sd(dat[,value])                      # and common standard deviation...
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect (again!)
null_difs[i] <- mean(sampleA)-mean(sampleB)           # compute and store the sampling error produced under the null hypothesis
}
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
t.test.algorithm <- function(dat = reshape_df, group = "Treatment", value = "Mass" ){
#############
# Compute the sample statistic
#############
indexA <- which(dat[,group]=="A")     # rows representing treatment A
indexB <- which(dat[,group]=="B")     # rows representing treatment B
observed_dif <- mean(dat[indexA,value]) - mean(dat[indexB,value])
#############
# Simulate the STATISTICAL POPULATION under the null hypothesis
#############
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(dat[,value])           # assume groups A and B come from a population with common mean
popSD_null <- sd(dat[,value])                      # and common standard deviation...
popData_null <- rnorm(n=lots,mean=popMean_null,sd=popSD_null)    # the statistical "population" of interest (under null model w no treatment effect)
#################
# Repeat sampling process (sampling from population) using a FOR loop
#################
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect (again!)
null_difs[i] <- mean(sampleA)-mean(sampleB)           # compute and store the sampling error produced under the null hypothesis
}
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
to_return <- list()   # initialize object to return
to_return$null_difs <- null_difs
to_return$p_value <- p_value
to_return$observed_dif <- observed_dif
return(to_return)
}
ttest <- t.test.algorithm()   # try to run the new function
ttest <- t.test.algorithm(dat = reshape_df, group = "Treatment", value = "Mass" )   # try to run the new function
ttest$p_value     # get the p_value
hist(ttest$null_difs)       # plot out all the sampling errors under the null hypothesis as a histogram
abline(v=ttest$observed_dif,col="green",lwd=3)     # indicate the observed sample statistic.
hist(ttest$null_difs)       # plot out all the sampling errors under the null hypothesis as a histogram
abline(v=ttest$observed_dif,col="green",lwd=3)
rep(NA,7)
df.ndif <- data.frame(
TreatmentA = c(135, 128, 139, 122, 126, 121, 128, 135, 134, 129, 134, 125, 130, 132, 125),
TreatmentB = c(98, 271, 340, rep(NA,7))
)
df.ndif <- data.frame(
TreatmentA = c(135, 128, 139, 122, 126, 121, 128, 135, 134, 129, 134, 125, 130, 132, 125),
TreatmentB = c(98, 271, 340, rep(NA,12))
)
summary(df.ndif)
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE1.Rmd',rmarkdown::pdf_document())
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
############################################################
####                                                    ####
####  NRES 470, Lecture 2                               ####
####                                                    ####
####  Kevin Shoemaker                                   ####
####  University of Nevada, Reno                        ####
####                                                    ####
############################################################
############################################################
####  Working with Probabilities                        ####
############################################################
#########
# Classic Urn Example
n_red <- 104
n_blue <- 55
n_green <- 30
allSpheres <- c(n_red,n_blue,n_green)           # create vector of urn contents
names(allSpheres) <- c("red","blue","green")    # make it a named vector, for convenience!
P_blue <- allSpheres["blue"]/sum(allSpheres)     # probability of drawing a blue sphere
P_blue
Prob <- allSpheres/sum(allSpheres)    # probability of drawing each type of sphere
Prob
as.numeric( Prob["blue"] + Prob["red"] )     # probability of drawing a blue or red sphere
as.numeric( Prob["blue"] + Prob["red"] + Prob["green"] )      # P(blue OR green)
#### Question: What is the probability of drawing a blue **AND THEN** a red sphere?
#[your command here]    # P(blue AND THEN red)
as.numeric( Prob["blue"] * Prob["red"] )    # P(blue AND THEN red)
as.numeric( (Prob["blue"] * Prob["red"]) + (Prob["red"] * Prob["blue"]) )    # P(blue then red OR red then blue)
##########
# Urn example #2
n_red_sphere <- 39       # contents of new urn
n_blue_sphere <- 76
n_red_cube <- 101
n_blue_cube <- 25
allSpheres <- c(n_red_sphere,n_blue_sphere)         # build up matrix from vectors
allCubes <- c(n_red_cube,n_blue_cube)
allTypes <- c(allSpheres,allCubes)
allTypes <- matrix(allTypes,nrow=2,ncol=2,byrow=T)     # matrix of urn contents
rownames(allTypes) <- c("sphere","cube")               # name rows and columns
colnames(allTypes) <- c("red","blue")
allTypes
Prob_Shape <- apply(allTypes,1,sum)/sum(allTypes)  # marginal probabilities of shape
Prob_Shape
Prob_Color <- apply(allTypes,2,sum)/sum(allTypes)    # marginal probabilities of color
Prob_Color
Prob_Color["red"]      # marginal probability of drawing a red object
as.numeric( Prob_Color["blue"] * Prob_Shape["cube"])      # joint probability of drawing a blue object that is a cube
## NOTE: if the above answer is not correct, please correct it!  And would I be asking this if it were correct? Hmmm...
(allTypes/sum(allTypes))["cube","blue"]        # or... is this the probability of drawing a blue cube?
as.numeric( Prob_Color["blue"] + Prob_Shape["cube"])        # probability of drawing something blue or something cube-shaped...
## NOTE: if the above answer is not correct, please correct it!
as.numeric( Prob_Color["blue"] + Prob_Shape["cube"] - (Prob_Color["blue"] * Prob_Shape["cube"])  )     # not mutually exclusive
allTypes["cube","blue"] / sum(allTypes["cube",])   # probability of drawing a blue object, given it is a cube
as.numeric( allTypes["cube","blue"] / sum(allTypes[,"blue"]) * Prob_Color["blue"])   # probability of drawing a blue cube... using conditional probabilities
uncond_prob_blue <- (allTypes["cube","blue"] / sum(allTypes["cube",])) * Prob_Shape["cube"] +
(allTypes["sphere","blue"] / sum(allTypes["sphere",])) * Prob_Shape["sphere"]       # unconditional probability of drawing a blue item..  Seems too complicated, but this method of computing unconditional probabilities will prove useful when getting into Bayesian statistics!
as.numeric(uncond_prob_blue)
##########
# Bolker medical example
Prob_Disease <- c(1,999999)     # disease prevalence
Prob_Disease <- Prob_Disease/sum(Prob_Disease)      # probability of disease
names(Prob_Disease) <- c("yes","no")                # make it a named vector!
Prob_Disease
### compute the unconditional probability of testing positive
as.numeric( 1*Prob_Disease["yes"] + 0.01*Prob_Disease["no"] )    # Prob(+test|Disease)*Prob(Disease) + Prob(+test|no Disease)*Prob(no Disease)
1/1000000
.5/3
.5/3+0+1
0.166666/1.1666666
1/3
1^0
rmd2rscript <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript("LECTURE1.Rmd")
rmd2rscript("LECTURE2.Rmd")
rmd2rscript("LECTURE3.Rmd")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
###########
# Test data for unequal sample sizes...
df.ndif <- data.frame(
TreatmentA = c(135, 128, 139, 122, 126, 121, 128, 135, 134, 129, 134, 125, 130, 132, 125),
TreatmentB = c(98, 271, 340, rep(NA,times=12))
)
summary(df.ndif)    # summarize!
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
############
# SIMULATE DATA GENERATION: decompose into deterministic and stochastic components
##########
# Deterministic component
deterministic_component <- function(x,a,b){
linear <- a + b*x   # linear functional form
return(linear)
}
xvals = seq(0,100,10)  # simulated x values define parameter space (e.g., tree girth)
expected_vals <- deterministic_component(xvals,175,-1.5)   # use the deterministic component to define the expected response (e.g., tree volume)
expected_vals
plot(xvals,expected_vals)
##########
# Stochastic component: define a function for transforming an expected (deterministic) response and adding a layer of "noise" on top!
# Arguments:
# x: vector of expected responses
# variance: variance of the "noise" component of your data simulation model
stochastic_component <- function(x,variance){
sd <- sqrt(variance)       # convert variance to standard deviation
stochvals <- rnorm(length(x),x,sd)       # add a layer of "noise" on top of the expected response values
return(stochvals)
}
sim_vals <- stochastic_component(expected_vals,variance=500)   # try it- run the function to add noise to your expected values.
plot(xvals,sim_vals)     # plot it- it should look much more noisy now!
# ALTERNATIVELY:
sim_vals <- stochastic_component(deterministic_component(xvals,175,-1.5),500)    # stochastic "shell" surrounds a deterministic "core"
############
# Goodness-of-fit test!
############
# Imagine you have the following "real" data (e.g., tree volumes).
realdata <- c(125,50,90,110,80,75,100,400,350,290,350)
plot(xvals,realdata)
############
# Goodness-of-fit test!
############
# Imagine you have the following "real" data (e.g., tree volumes).
realdata <- data.frame(Volume=c(125,50,90,110,80,75,100,400,350,290,350),Girth=xvals)
plot(realdata$Girth,realdata$Volume)
#############
# Let's simulate many datasets from our hypothesized data generating model:
reps <- 1000    # specify number of replicate datasets to generate
samplesize <- length(xvals)    # define the number of data points we should generate for each simulation "experiment"
simresults <- array(0,dim=c(samplesize,reps))   # initialize a storage array for results
for(i in 1:reps){       # for each independent simulation "experiment":
exp_vals <- deterministic_component(realdata$Girth,a=10,b=4)          # simulate the expected tree volumes for each measured girth value
sim_vals <- stochastic_component(exp_vals,1000)  # add stochastic noise
simresults[,i] <- sim_vals   # store the simulated data for later
}
# now make a boxplot of the results
boxplot(lapply(1:nrow(results), function(i) simresults[i,]))
#############
# Let's simulate many datasets from our hypothesized data generating model:
reps <- 1000    # specify number of replicate datasets to generate
samplesize <- length(xvals)    # define the number of data points we should generate for each simulation "experiment"
simresults <- array(0,dim=c(samplesize,reps))   # initialize a storage array for results
for(i in 1:reps){       # for each independent simulation "experiment":
exp_vals <- deterministic_component(realdata$Girth,a=10,b=4)          # simulate the expected tree volumes for each measured girth value
sim_vals <- stochastic_component(exp_vals,1000)  # add stochastic noise
simresults[,i] <- sim_vals   # store the simulated data for later
}
# now make a boxplot of the results
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]))
boxplot(lapply(1:nrow(results), function(i) results[i,]),xaxt="n")    # (repeat) make a boxplot of the results
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:length(xvals)),labels=xvals)                          # add x axis labels
points(c(1:length(xvals)),realdata,pch=20,cex=3,col="red",xaxt="n")
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:length(xvals)),labels=xvals)                          # add x axis labels
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:length(xvals)),labels=xvals)
#############
# Let's simulate many datasets from our hypothesized data generating model:
reps <- 1000    # specify number of replicate datasets to generate
samplesize <- length(xvals)    # define the number of data points we should generate for each simulation "experiment"
simresults <- array(0,dim=c(samplesize,reps))   # initialize a storage array for results
for(i in 1:reps){       # for each independent simulation "experiment":
exp_vals <- deterministic_component(realdata$Girth,a=10,b=4)          # simulate the expected tree volumes for each measured girth value
sim_vals <- stochastic_component(exp_vals,1000)  # add stochastic noise
simresults[,i] <- sim_vals   # store the simulated data for later
}
# now make a boxplot of the results
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:length(xvals)),labels=xvals)                          # add x axis labels
#########
# Now overlay the "real" data
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:length(xvals)),labels=xvals)
points(c(1:length(realdata$Girth)),realdata$Volume,pch=20,cex=3,col="red",xaxt="n")     # this time, overlay the "real" data
###########
# Using data simulation to flesh out sampling distributions for frequentist inference
# e.g., the "brute force t test" example:
reps <- 1000                    # number of replicate samples to generate
null_difs <- numeric(reps)              # storage vector for the test statistic for each sample
for(i in 1:reps){
sampleA <- rnorm(10,10,4)        # sample representing "groups" A and B under the null hypothesis
sampleB <- rnorm(10,10,4)
null_difs[i] <- mean(sampleA)-mean(sampleB)         # test statistic (model result)
}
hist(null_difs)           # plot out the sampling distribution
abline(v=13,col="green",lwd=3)
###########
# Using data simulation to flesh out sampling distributions for frequentist inference
# e.g., the "brute force t test" example:
reps <- 1000                    # number of replicate samples to generate
null_difs <- numeric(reps)              # storage vector for the test statistic for each sample
for(i in 1:reps){
sampleA <- rnorm(10,10,4)        # sample representing "groups" A and B under the null hypothesis
sampleB <- rnorm(10,10,4)
null_difs[i] <- mean(sampleA)-mean(sampleB)         # test statistic (model result)
}
hist(null_difs)           # plot out the sampling distribution
abline(v=3.5,col="green",lwd=3)
###############
# Power analysis example: designing a monitoring program for a rare species
### first, let's develop some helper functions:
# Arguments:
# TrueN: true population abundance
# surveyors: number of survey participants each day
# days: survey duration, in days
NumObserved <- function(TrueN=1000,surveyors=1,days=3){
probPerPersonDay <- 0.02      # define the probability of detection per person-days
probPerDay <- 1-(1-probPerPersonDay)^surveyors      # define the probability of detection per day (multiple surveyors)
probPerSurvey <- 1-(1-probPerDay)^days       # define the probability of detection for the entire survey
nobs <- rbinom(1,size=TrueN,prob=probPerSurvey)     # simulate the number of animals detected!
return(nobs)
}
NumObserved(TrueN=500,surveyors=2,days=7)   # test the new function
# Arguments:
# LastYearAbund: true population abundance in the previous year
# trend: proportional change in population size from last year
ThisYearAbund <- function(LastYearAbund=1000,trend=-0.03){
CurAbund <- LastYearAbund + trend*LastYearAbund    # compute abundance this year
CurAbund <- floor(CurAbund)  # can't have fractional individuals!
return(CurAbund)
}
ThisYearAbund(LastYearAbund=500,trend=-0.03)    # test the new function
# Arguments:
# initabund: true initial population abundance
# trend: proportional change in population size from last year
# years: duration of simulation
# observers: number of survey participants each day
# days: survey duration, in days
# survint: survey interval, in years (e.g., 2 means surveys are conducted every other year)
SimulateMonitoringData <- function(initabund=1000,trend=-0.03,years=25,observers=1,days=3,survint=2){
prevabund <- initabund        # initialize "previous-year abundance" at initial abundance
detected <- numeric(years)     # set up storage variable
for(y in 1:years){            # for each year of the simulation:
thisAbund <- ThisYearAbund(prevabund,trend)             # compute the current abundance on the basis of the trend
detected[y] <- NumObserved(thisAbund,observers,days)     # sample the current population using this monitoring scheme
prevabund <- thisAbund   # set this years abundance as the previous years abundance (to set up the simulation for next year)
}
surveyed <- c(1:years)%%survint==0    # which years were surveys actually performed?
detected[!surveyed] <- NA            # if the survey is not performed that year, return a missing value
return(detected)       # return the number of individuals detected
}
SimulateMonitoringData(initabund=1000,trend=-0.03,years=25,observers=1,days=3,survint=2)    # test the new function
#########
# finally, develop a function for assessing whether or not a decline was detected:
# Arguments:
# monitoringData: simulated results from a long-term monitoring study
# alpha: define acceptable type-I error rate (false positive rate)
IsDecline <- function(monitoringData,alpha=0.05){
time <- 1:length(monitoringData)      # vector of survey years
model <- lm(monitoringData~time)    # for now, let's use ordinary linear regression (perform linear regression on simulated monitoring data)
p_value <- summary(model)$coefficients["time","Pr(>|t|)"]      # extract the p-value
isdecline <- ifelse(summary(model)$coefficients["time","Estimate"]<0,TRUE,FALSE)     # determine if the simulated monitoring data determined a "significant" decline
sig_decline <- ifelse((p_value<=alpha)&(isdecline),TRUE,FALSE)    # if declining and significant trend, then the monitoring protocol successfully diagnosed a decline
return(sig_decline)
}
IsDecline(monitoringData=c(10,20,NA,15,1),alpha=0.05)    # test the function
nreps <- 1000
initabund <- 100
GetPower <- function(observers=1,days=3,alpha=0.05,years=25,survint=2,trend=-0.03){
decline <- logical(nreps)
for(i in 1:nreps){
detected <- SimulateMonitoringData(initabund,trend,years,observers,days,survint)
decline[i] <- IsDecline(detected,alpha)
}
Power <- length(which(decline==TRUE))/nreps
return(Power)
}
cat((sprintf("The statistical power to detect a decline for the default parameters is: %s",GetPower())))    # for default
initabund = 1000
survints <- c(1:5)
powers <- numeric(length(survints))
for(i in 1:length(survints)){
powers[i] <- GetPower(survint=survints[i])
}
plot(powers~survints,xlab="Survey interval(years)",ylab="Statistical Power",main="Power to detect trand, by sampling interval")
rmd2rscript <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript("LECTURE2.Rmd")
rmd2rscript("LECTURE3.Rmd")
source('~/.active-rstudio-document', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
############################################################
####                                                    ####
####  NRES 746, Lecture 3                               ####
####                                                    ####
####  Kevin Shoemaker                                   ####
####  University of Nevada, Reno                        ####
####                                                    ####
############################################################
############################################################
####  The virtual ecologist                             ####
####     Building data simulation models                ####
############################################################
############
# SIMULATE DATA GENERATION: decompose into deterministic and stochastic components
##########
# Deterministic component: define function for transforming a predictor variable into an expected response (linear regression)
# Arguments:
# x: vector of covariate values
# a: the intercept of a linear relationship mapping the covariate to an expected response
# b: the slope of a linear relationship mapping the covariate to an expected response
deterministic_component <- function(x,a,b){
linear <- a + b*x   # specify a deterministic, linear functional form
return(linear)
}
xvals = seq(0,100,10)  # define the values of a hypothetical predictor variable (e.g., tree girth)
expected_vals <- deterministic_component(xvals,175,-1.5)   # use the deterministic component to determine the expected response (e.g., tree volume)
expected_vals
plot(xvals,expected_vals)   # plot out the relationship
##########
# Stochastic component: define a function for transforming an expected (deterministic) response and adding a layer of "noise" on top!
# Arguments:
# x: vector of expected responses
# variance: variance of the "noise" component of your data simulation model
stochastic_component <- function(x,variance){
sd <- sqrt(variance)       # convert variance to standard deviation
stochvals <- rnorm(length(x),x,sd)       # add a layer of "noise" on top of the expected response values
return(stochvals)
}
### Simulate stochastic data!!
sim_vals <- stochastic_component(expected_vals,variance=500)   # try it- run the function to add noise to your expected values.
plot(xvals,sim_vals)     # plot it- it should look much more "noisy" now!
# ALTERNATIVELY:
sim_vals <- stochastic_component(deterministic_component(xvals,175,-1.5),500)    # stochastic "shell" surrounds a deterministic "core"
############
# Goodness-of-fit test!
############
# Imagine you have the following "real" data (e.g., tree volumes).
realdata <- data.frame(Volume=c(125,50,90,110,80,75,100,400,350,290,350),Girth=xvals)
plot(realdata$Girth,realdata$Volume)
#############
# Let's simulate many datasets from our hypothesized data generating model:
reps <- 1000    # specify number of replicate datasets to generate
samplesize <- nrow(realdata)    # define the number of data points we should generate for each simulation "experiment"
simresults <- array(0,dim=c(samplesize,reps))   # initialize a storage array for results
for(i in 1:reps){       # for each independent simulation "experiment":
exp_vals <- deterministic_component(realdata$Girth,a=10,b=4)          # simulate the expected tree volumes for each measured girth value
sim_vals <- stochastic_component(exp_vals,1000)  # add stochastic noise
simresults[,i] <- sim_vals   # store the simulated data for later
}
# now make a boxplot of the results
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:samplesize),labels=realdata$Girth)                          # add x axis labels
#########
# Now overlay the "real" data
boxplot(lapply(1:nrow(simresults), function(i) simresults[i,]),xaxt="n")    # (repeat) make a boxplot of the simulation results
axis(1,at=c(1:samplesize),labels=realdata$Girth)                          # add x axis labels
points(c(1:samplesize),realdata$Volume,pch=20,cex=3,col="red",xaxt="n")     # this time, overlay the "real" data
###########
# Using data simulation to flesh out sampling distributions for frequentist inference
# e.g., the "brute force t test" example:
reps <- 1000                    # number of replicate samples to generate
null_difs <- numeric(reps)              # storage vector for the test statistic for each sample
for(i in 1:reps){
sampleA <- rnorm(10,10,4)        # sample representing "groups" A and B under the null hypothesis
sampleB <- rnorm(10,10,4)
null_difs[i] <- mean(sampleA)-mean(sampleB)         # test statistic (model result)
}
hist(null_difs)           # plot out the sampling distribution
abline(v=3.5,col="green",lwd=3)
rmd2rscript("LECTURE3.Rmd")
