################
t.test(df$TreatmentA,df$TreatmentB, var.equal=TRUE, paired=FALSE)
######################
# ALTERNATIVE ALGORITHMIC APPROACH!
######################
#############
# Simulate the STATISTICAL POPULATION under the null hypothesis
#############
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(reshape_df$Mass)        # assume groups A and B come from a population with common mean
popSD_null <- sd(reshape_df$Mass)                      # and common standard deviation...
popData_null <- rnorm(n=lots,mean=popMean_null,sd=popSD_null)    # the statistical "population" of interest (under null model w no treatment effect)
#############
# Draw a SAMPLE from that population
#############
sampleA <- sample(popData_null,size=sample.size)    # use R's native "sample()" function
sampleB <- sample(popData_null,size=sample.size)
round(sampleA)
difference <- mean(sampleA)-mean(sampleB)   # sample statistic = difference between sample means
difference
#################
# Repeat this process using a FOR loop
#################
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample of body masses assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample of body masses assuming no treatment effect (again!)
null_difs[i] <- mean(sampleA)-mean(sampleB)           # compute and store the sampling error produced under the null hypothesis
}
hist(null_difs)       # plot out all the sampling errors under the null hypothesis as a histogram
abline(v=observed_dif,col="green",lwd=3)     # indicate the observed sample statistic.
############
# Generate a p-value algorithmically!!
############
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps       # compute a p-value!
p_value
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
reshape_df
#############
# Develop a function that wraps up all the above steps into one!
#############
t.test.algorithm <- function(dat = reshape_df, group = "Treatment", value = "Mass" ){
#############
# Compute the sample statistic
#############
observed_dif <- mean(df$TreatmentA) - mean(df$TreatmentB)
#############
# Simulate the STATISTICAL POPULATION under the null hypothesis
#############
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(dat[,value])           # assume groups A and B come from a population with common mean
popSD_null <- sd(dat[,value])                      # and common standard deviation...
popData_null <- rnorm(n=lots,mean=popMean_null,sd=popSD_null)    # the statistical "population" of interest (under null model w no treatment effect)
#################
# Repeat sampling process (sampling from population) using a FOR loop
#################
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect (again!)
null_difs[i] <- mean(sampleA)-mean(sampleB)           # compute and store the sampling error produced under the null hypothesis
}
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
to_return <- list()   # initialize object to return
to_return$null_difs <- null_difs
to_return$p_value <- p_value
to_return$observed_dif <- observed_dif
return(to_return)
}
t.test.algorithm()   # try to run your function!
ttest <- t.test.algorithm()   # try to run your function!
ttest$p_value
hist(ttest$null_difs)       # plot out all the sampling errors under the null hypothesis as a histogram
abline(v=ttest$observed_dif,col="green",lwd=3)     # indicate the observed sample statistic.
hist(ttest$null_difs)       # plot out all the sampling errors under the null hypothesis as a histogram
abline(v=ttest$observed_dif,col="green",lwd=3)     # indicate the observed sample statistic.
mean(reshape_df$Mass)
df.vardif <- data.frame(
TreatmentA = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179),
TreatmentB = c(195, 109, 173, 163, 208, 196, 165, 104, 200, 240)
)
summary(df.vardif)    # summarize!
df.vardif <- data.frame(
TreatmentA = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179),
TreatmentB = c(195, 69, 143, 163, 208, 196, 165, 104, 230, 340)
)
summary(df.vardif)    # summarize!
df.vardif <- data.frame(
TreatmentA = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179),
TreatmentB = c(215, 69, 143, 153, 218, 186, 125, 98, 230, 340)
)
summary(df.vardif)    # summarize!
df.vardif <- data.frame(
TreatmentA = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179),
TreatmentB = c(215, 69, 143, 153, 218, 186, 125, 98, 271, 340)
)
summary(df.vardif)    # summarize!
df.vardif <- data.frame(
TreatmentA = c(135, 128, 139, 122, 126, 121, 128, 135, 134, 129),
TreatmentB = c(215, 69, 143, 153, 218, 186, 125, 98, 271, 340)
)
summary(df.vardif)    # summarize!
df.ndif <- data.frame(
TreatmentA = c(135, 128, 139, 122, 126, 121, 128, 135, 134, 129, 134, 125, 130, 132, 125),
TreatmentB = c(98, 271, 340)
)
summary(df.ndif)    # summarize!
##################
# NON-PARAMETRIC T-TEST -- PERMUTATION TEST
##################
reps <- 5000            # Define the number of permutations to run (number of replicates)
null_difs <- numeric(reps)   # initialize storage variable
for (i in 1:reps){			# For each replicate:
newGroup <- reshape_df$Group[sample(c(1:nrow(reshape_df)))]			   # randomly shuffle the observed data with respect to treatment group
dif <- mean(reshape_df$Mass[newGroup=="A"])	- mean(reshape_df$Mass[newGroup=="B"])	   #  compute the difference between the group means after reshuffling the data
null_difs[i] <- dif	    # store this value in a vector
}
hist(null_difs)    # Plot a histogram of null differences between group A and group B under the null hypothesis (sampling errors)
##################
# NON-PARAMETRIC T-TEST -- PERMUTATION TEST
##################
reps <- 5000            # Define the number of permutations to run (number of replicates)
null_difs <- numeric(reps)   # initialize storage variable
for (i in 1:reps){			# For each replicate:
newGroup <- reshape_df$Treatment[sample(c(1:nrow(reshape_df)))]			   # randomly shuffle the observed data with respect to treatment group
dif <- mean(reshape_df$Mass[newGroup=="A"])	- mean(reshape_df$Mass[newGroup=="B"])	   #  compute the difference between the group means after reshuffling the data
null_difs[i] <- dif	    # store this value in a vector
}
hist(null_difs)    # Plot a histogram of null differences between group A and group B under the null hypothesis (sampling errors)
abline(v=observed_dif,col="green",lwd=3)   # Add a vertical line to the plot to indicate the observed difference
#############
# Develop a function that performs a permutation-t-test!
#############
t.test.permutation <- function(dat = reshape_df, group = "Treatment", value = "Mass" ){
#############
# Compute the sample statistic
#############
observed_dif <- mean(df$TreatmentA) - mean(df$TreatmentB)
reps <- 5000            # Define the number of permutations to run (number of replicates)
null_difs <- numeric(reps)   # initialize storage variable
for (i in 1:reps){			# For each replicate:
newGroup <- reshape_df$Treatment[sample(c(1:nrow(reshape_df)))]			   # randomly shuffle the observed data with respect to treatment group
dif <- mean(reshape_df$Mass[newGroup=="A"])	- mean(reshape_df$Mass[newGroup=="B"])	   #  compute the difference between the group means after reshuffling the data
null_difs[i] <- dif	    # store this value in a vector
}
higher_anomaly <- length(which(abs(null_difs)>=abs(observed_dif)))
p_value <- higher_anomaly/reps
to_return <- list()   # initialize object to return
to_return$null_difs <- null_difs
to_return$p_value <- p_value
to_return$observed_dif <- observed_dif
return(to_return)
}
ttest2 <- t.test.permutation()
ttest2$p_value
hist(ttest2$null_difs)    # Plot a histogram of null differences between group A and group B under the null hypothesis (sampling errors)
abline(v=ttest2$observed_dif,col="green",lwd=3)   # Add a vertical line to the plot to indicate the observed difference
############
# new function to generate "bootstrap" samples from a data frame
boot_sample <- function(df,statfunc,n_samples,n_stats){
indices <- c(1:nrow(df))
output <- matrix(NA,nrow=n_samples,ncol=n_stats)
for(i in 1:n_samples){
boot_rows <- sample(indices,size=nrow(df),replace=T)
newdf <- df[boot_rows,]
output[i,] <- statfunc(newdf)
}
return(output)
}
##########
# Generate a bunch of bootstrapped samples!
boot <- boot_sample(trees,Rsquared,10,length(stat))
##############
# Demonstration: bootstrapping a confidence interval!
## use the "trees" dataset in R:
head(trees)   # use help(trees) for more information
#########
# Basic data exploration
plot(trees$Volume~trees$Height, main = 'Black Cherry Tree Height/Volume Relationship', xlab = 'Height', ylab = 'Volume', pch = 16, col ='blue')
plot(trees$Volume~trees$Girth, main = 'Black Cherry Tree Girth/Volume Relationship', xlab = 'Girth', ylab = 'Volume', pch = 16, col ='red')
#########
# Function for computing R-squared statistics for models regressing a response variable on multiple possible predictor variables
Rsquared <- function(df,responsevar="Volume"){    # interactions not yet implemented
response <- df[,responsevar]
names <- names(df)
rsq <- numeric(length(names))
names(rsq) <- names(df)
rsq <- rsq[names(rsq)!=responsevar]
for(i in names(rsq)){         # loop through predictors
predictor <- df[,i]
model <- lm(response~predictor)
rsq[i] <- summary(model)$r.square
}
return(rsq)
}
#########
# test the function to see if it works!
stat <- Rsquared(trees,"Volume")
stat
############
# new function to generate "bootstrap" samples from a data frame
boot_sample <- function(df,statfunc,n_samples,n_stats){
indices <- c(1:nrow(df))
output <- matrix(NA,nrow=n_samples,ncol=n_stats)
for(i in 1:n_samples){
boot_rows <- sample(indices,size=nrow(df),replace=T)
newdf <- df[boot_rows,]
output[i,] <- statfunc(newdf)
}
return(output)
}
##########
# Generate a bunch of bootstrapped samples!
boot <- boot_sample(trees,Rsquared,10,length(stat))
colnames(boot) <- names(stat)
boot
stat
#############
# use bootstrapping to generate confidence intervals for R-squared statistic!
boot <- boot_sample(trees,Rsquared,1000,length(stat))   # 1000 bootstrap samples
confint <- apply(boot,2,function(t)  quantile(t,c(0.025,0.5,0.975)))
colnames(confint) <- names(stat)
t(confint)
quantile(boot,c(0.025,0.975))
boot
#############
# use bootstrapping to generate confidence intervals for R-squared statistic!
boot <- boot_sample(trees,Rsquared,1000,length(stat))   # generate test statistics (Rsquared vals) for 1000 bootstrap samples
confint <- apply(boot,2,function(t)  quantile(t,c(0.025,0.5,0.975)))       # summarize the quantiles to generate confidence intervals for each predictor variable
colnames(confint) <- names(stat)
t(confint)
stat
#############
# use bootstrapping to generate confidence intervals for R-squared statistic!
boot <- boot_sample(df=trees,statfunc=Rsquared,n_samples=1000,n_stats=2)   # generate test statistics (Rsquared vals) for 1000 bootstrap samples
confint <- apply(boot,2,function(t)  quantile(t,c(0.025,0.5,0.975)))       # summarize the quantiles to generate confidence intervals for each predictor variable
colnames(confint) <- names(stat)
t(confint)
confint
is.matrix(confint)
help.start()
TRUEMIN <- 10
TRUEMAX <- 20
N_IND_SAMPLES <- 1000
SAMPLESIZE <- 10
lots <- 100000
datafountain <- runif(lots,TRUEMIN,TRUEMAX)
samplemean <- numeric(N_IND_SAMPLES)
for(i in 1:N_IND_SAMPLES){
sample <- sample(datafountain,SAMPLESIZE)
samplemean[i] <- mean(sample)
}
hist(datafountain,freq=F,ylim=c(0,1))
hist(samplemean,freq=F,add=T,col="red")
?coef
coef(lm(trees$Volume~trees$Girth))
?faithful
u
faithful$eruptions
faithful$waiting
?lowess
?loess.smooth
?loess
loess(trees$Volume~trees$Height)
a<-loess(trees$Volume~trees$Height)
coef(a)
a$fitted
plot(trees$Volume~trees$Height)
abline(a)
lines(a)
a
?scatter.smooth
newsum <- function(x=c(1,2,4)){
sm <- sum(x)
return(sm)
}
newsum(x=c(5:10))   # specify x manually
newsum()    # use default value!
rlocodist <- function(n){
vals <- c(1,7,10,35)         # possible data values
probs <- c(1,0.01,5,0.5)     # relative probability of each data values
probs <- probs/sum(probs)
vals[apply(rmultinom(n,1,probs),2,function(t) which(t==1))]     # sample from this made-up distribution
}
lots=10000
datafountain <- rlocodist(lots)
hist(datafountain)
rlocodist <- function(n){
vals <- c(1,7,10,35)         # possible data values
probs <- c(1,2,5,0.5)     # relative probability of each data values
probs <- probs/sum(probs)
vals[apply(rmultinom(n,1,probs),2,function(t) which(t==1))]     # sample from this made-up distribution
}
lots=10000
datafountain <- rlocodist(lots)
hist(datafountain, head="non standard made-up distribution")
rlocodist <- function(n){
vals <- c(1,7,10,35)         # possible data values
probs <- c(1,2,5,0.5)     # relative probability of each data values
probs <- probs/sum(probs)
vals[apply(rmultinom(n,1,probs),2,function(t) which(t==1))]     # sample from this made-up distribution
}
lots=10000
datafountain <- rlocodist(lots)
hist(datafountain, head="non standard made-up distribution")
rlocodist <- function(n){
vals <- c(1,7,10,35)         # possible data values
probs <- c(1,2,5,0.5)     # relative probability of each data values
probs <- probs/sum(probs)
vals[apply(rmultinom(n,1,probs),2,function(t) which(t==1))]     # sample from this made-up distribution
}
lots=10000
datafountain <- rlocodist(lots)
hist(datafountain, main="non standard made-up distribution")
?airquality
model1 <- lm(Ozone~Solar.R+Wind+Temp,data=air.cleaned)
air.cleaned <- na.omit(airquality) # remove rows with missing data
model1 <- lm(Ozone~Solar.R+Wind+Temp,data=air.cleaned)
stats:::plot.lm(model1, which=c(1:4))   # in some cases, need to make sure the package is explicitly referenced!
plot(model1, which=c(1:4))   # in some cases, need to make sure the package is explicitly referenced!
par(mfrow=c(2,2))
plot(model1, which=c(1:4))   # diagnostic plots
hist(residuals(model1), breaks=10)
plot(predict(model1) ~ air.cleaned$Ozone)
abline(0,1)
par(mfrow=c(3,2))
plot(model1, which=c(1:4))   # diagnostic plots
hist(residuals(model1), breaks=10)
plot(predict(model1) ~ air.cleaned$Ozone)
abline(0,1)
symbols(air.cleaned$Temp, air.cleaned$Solar.R, circles=air.cleaned$Ozone/100, ylab="Solar Radiation", xlab="Temperature", main="Interaction Plot", inches=FALSE)
# alternatively...
coplot(air.cleaned$Ozone~air.cleaned$Temp|air.cleaned$Solar.R,rows=1)
symbols(air.cleaned$Temp, air.cleaned$Solar.R, circles=air.cleaned$Ozone/100, ylab="Solar Radiation", xlab="Temperature", main="Interaction Plot", inches=FALSE)
# alternatively...
coplot(air.cleaned$Ozone~air.cleaned$Temp|air.cleaned$Solar.R,rows=1)
symbols(air.cleaned$Temp, air.cleaned$Solar.R, circles=air.cleaned$Ozone/100, ylab="Solar Radiation", xlab="Temperature", main="Interaction Plot", inches=FALSE)
formula2 <- "Ozone ~ Wind + Solar.R * Temp"   # you can name formulas...
model2 <- lm(formula2,data=air.cleaned)
model2 <- lm(formula2,data=air.cleaned)
anova(model1, model2, test="F")   # how would you run an LRT test instead?
NobleFir.df <- read.csv("TreeData.csv")
cor(NobleFir.df[,c(2,4,7:9)])
boxplot(NobleFir.df$Northeastness~NobleFir.df$ABPR, xlab="Presence of Noble Fir", ylab="Northeastness")   # for example
boxplot(NobleFir.df$Northeastness~NobleFir.df$ABPR, xlab="Presence of Noble Fir", ylab="Northeastness")   # for example
Biomass_std.lm <- with(NobleFir.df,     # using the "with()" statement, we don't need to keep referencing the name of the data frame.
lm(scale(Biomass) ~ scale(elev) + scale(Northeastness) + scale(Slope) + SlopePos + scale(StandAge))
)
with(NobleFir.df,
symbols(x,y,circles=abs(residuals(Biomass_std.lm)), inches=0.3, ylab="Northing", xlab="Easting", main="Errors from Biomass Regression Model")
)
install.packages("emdbook")
install.packages("vegan3d")
rmarkdown::render('index.Rmd',rmarkdown::pdf_document())
rmarkdown::render('INTRO.Rmd',rmarkdown::pdf_document())
rmarkdown::render('index.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('INTRO.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE1.Rmd',rmarkdown::pdf_document())
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
############################################################
####                                                    ####
####  NRES 470, Lecture 1                               ####
####                                                    ####
####  Kevin Shoemaker                                   ####
####  University of Nevada, Reno                        ####
####                                                    ####
############################################################
############################################################
####  Computational algorithms vs standard statistics   ####
############################################################
#############
# Start with a made-up data frame!
#############
df <- data.frame(
TreatmentA = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179),
TreatmentB = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)
)
summary(df)    # summarize!
sample.size <- length(df$TreatmentA)     # determine sample size
reshape_df <- data.frame(                # "reshape" the data frame so each observation gets its own row ("tidy" format)
Treatment = rep(c("A","B"),each=sample.size),
Mass = c(df$TreatmentA,df$TreatmentB)
)
plot(Mass~Treatment, data=reshape_df)    # explore/visualize the data
# boxplot(df$TreatmentA,df$TreatmentB,names=c("TreatmentA","TreatmentB"))  # (alternative method!)
observed_dif <- mean(df$TreatmentA) - mean(df$TreatmentB)     # compute sample statistic
observed_dif
################
# Perform standard t-test
################
t.test(df$TreatmentA,df$TreatmentB, var.equal=TRUE, paired=FALSE)
######################
# ALTERNATIVE ALGORITHMIC APPROACH!
######################
#############
# Simulate the STATISTICAL POPULATION under the null hypothesis
#############
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(reshape_df$Mass)        # assume groups A and B come from a population with common mean
popSD_null <- sd(reshape_df$Mass)                      # and common standard deviation...
popData_null <- rnorm(n=lots,mean=popMean_null,sd=popSD_null)    # the statistical "population" of interest (under null model w no treatment effect)
#############
# Draw a SAMPLE from that population
#############
sampleA <- sample(popData_null,size=sample.size)    # use R's native "sample()" function
sampleB <- sample(popData_null,size=sample.size)
round(sampleA)
difference <- mean(sampleA)-mean(sampleB)   # sample statistic = difference between sample means
difference
#################
# Repeat this process using a FOR loop
#################
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample of body masses assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample of body masses assuming no treatment effect (again!)
null_difs[i] <- mean(sampleA)-mean(sampleB)           # compute and store the sampling error produced under the null hypothesis
}
hist(null_difs)       # plot out all the sampling errors under the null hypothesis as a histogram
abline(v=observed_dif,col="green",lwd=3)     # indicate the observed sample statistic.
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
dat = reshape_df
group = "Treatment"
group = "Treatment"
value = "Mass"
observed_dif <- mean(df$TreatmentA) - mean(df$TreatmentB)
observed_dif
observed_dif <- mean(df$TreatmentB) - mean(df$TreatmentA)
observed_dif
lots <- 1000000  # large number approximating infinity
popMean_null <- mean(dat[,value])           # assume groups A and B come from a population with common mean
popSD_null <- sd(dat[,value])                      # and common standard deviation...
popData_null <- rnorm(n=lots,mean=popMean_null,sd=popSD_null)    # the statistical "population" of interest (under null model w no treatment effect)
reps <- 1000                 # set the number of replicates
null_difs <- numeric(reps)       # initialize a storage structure to hold one anomaly (sampling error) per replicate
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect (again!)
null_difs[i] <- mean(sampleB)-mean(sampleA)           # compute and store the sampling error produced under the null hypothesis
}
for(i in 1:reps){            # for each replicate...
sampleA <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect
sampleB <- sample(popData_null,size=sample.size)      # draw a sample assuming no treatment effect (again!)
null_difs[i] <- mean(sampleB)-mean(sampleA)           # compute and store the sampling error produced under the null hypothesis
}
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
ordered_difs <- sort(abs(null_difs))       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
ordered_difs
higher_anomaly <- length(which(ordered_difs>=observed_dif))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
p_value
higher_anomaly <- length(which(ordered_difs>=abs(observed_dif)))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
p_value
ordered_difs <- sort(null_difs)       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=observed_dif))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
p_value
observed_dif <- mean(df$TreatmentA) - mean(df$TreatmentB)
ordered_difs <- sort(null_difs)       # sort the vector of sampling errors
higher_anomaly <- length(which(ordered_difs>=observed_dif))       # how many of these sampling errors equal or exceed the sample statistic?
p_value <- higher_anomaly/reps
p_value
rmarkdown::render('LECTURE1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LECTURE1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('INTRO.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
class(TRUE)
class(c(T,F))
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
rmarkdown::render('LAB1.Rmd',rmarkdown::pdf_document())
########
# Compute a p-value based on the permutation test, just like we did before!
########
higher_anomaly <- length(which(abs(null_difs)>=abs(observed_dif)))
p_value <- higher_anomaly/reps
p_value
rmd2rscript <- function(infile="module1_1.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
isrblock <- FALSE
count=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
if(isrblock){
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if(count>1) write(newline,file=con2,append=TRUE)
count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript("LECTURE1.Rmd")
