MyxDat <- MyxoTiter_sum
Myx <- subset(MyxDat,grade==1)  #Data set from grade 1 of myxo data
#head(Myx)
myx.data.for.bugs <- list(
titer = Myx$titer,
days = Myx$day,
n.observations = nrow(Myx)
)
#myx.data.for.bugs
init.vals.for.bugs <- function(){
list(
shape=runif(1,20,30),
a=runif(1,0.05,0.3),
b=runif(1,1,1.3)
)
}
init.vals.for.bugs2 <- function(){
list(
shape=runif(1,20,30),
a=runif(1,8,9),
b=runif(1,1,1.5)
)
}
jags.fit_ricker <- jags(data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,n.iter=50000,model.file="BUGSmodel_ricker.txt",n.chains = 3,n.burnin = 5000,n.thin = 20 )
jags.fit_mm <- jags(data=myx.data.for.bugs,inits=init.vals.for.bugs2,parameters.to.save=params.to.store,n.iter=50000,model.file="BUGSmodel_mm.txt",n.chains = 3,n.burnin = 5000,n.thin = 20 )
Myx_PostPredCheck <- function(MCMC1=jags.fit_ricker,MCMC2=jags.fit_mm){
lots <- 1000
nMCMC <- min(jags.fit_ricker$BUGSoutput$n.keep,jags.fit_mm$BUGSoutput$n.keep)
# hard-code the data
MyxDat <- MyxoTiter_sum
Myx <- subset(MyxDat,grade==1)  #Data set from grade 1 of myxo data
nobs <- nrow(Myx)
SSEobs_rick <- numeric(lots)
SSEobs_mm <- numeric(lots)
SSEsim_rick <- numeric(lots)
SSEsim_mm <- numeric(lots)
params_rick <- matrix(NA, nrow=lots,ncol=3)
params_mm <- matrix(NA, nrow=lots,ncol=3)
alldays <- sort(unique(Myx$day))
allsims_rick <- matrix(nrow=lots,ncol=length(alldays))
allsims_mm <- matrix(nrow=lots,ncol=length(alldays))
colnames(allsims_rick) <- alldays
colnames(allsims_mm) <- alldays
i=2
for(i in 1:lots){
random_index <- sample(1:nMCMC,1)
params_rick[i,] <- c(a=MCMC1$BUGSoutput$sims.list$a[random_index],
b=MCMC1$BUGSoutput$sims.list$b[random_index],
shape=MCMC1$BUGSoutput$sims.list$shape[random_index])
params_mm[i,] <- c(a=MCMC2$BUGSoutput$sims.list$a[random_index],
b=MCMC2$BUGSoutput$sims.list$b[random_index],
shape=MCMC2$BUGSoutput$sims.list$shape[random_index])
### loop through titer observations
exptiter_rick <- Ricker(Myx$day,params_rick[i,1],params_rick[i,2])
exptiter_mm <- MMFunc(Myx$day,params_mm[i,1],params_mm[i,2])
simobs_rick <- rgamma(nobs,shape=params_rick[i,3],rate=(params_rick[i,3]/exptiter_rick))
simobs_mm <- rgamma(nobs,shape=params_mm[i,3],rate=(params_mm[i,3]/exptiter_mm))
SSEobs_rick[i] <- sum((Myx$titer-exptiter_rick)^2)
SSEobs_mm[i] <- sum((Myx$titer-exptiter_mm)^2)
SSEsim_rick[i] <- sum((simobs_rick-exptiter_rick)^2)
SSEsim_mm[i] <- sum((simobs_mm-exptiter_mm)^2)
allsims_rick[i,] <- tapply(simobs_rick,Myx$day,function(t) t[sample.int(length(t))][1] )
allsims_mm[i,] <- tapply(simobs_mm,Myx$day,function(t) t[sample.int(length(t))][1] )
}
boxplot(allsims_rick,xlab="Days",ylab="Titer",xlim=c(0,10),main="Ricker",at=as.numeric(colnames(allsims_rick)))
points(Myx$day,Myx$titer,col="green",pch=19,cex=2)
boxplot(allsims_mm,xlab="Days",ylab="Titer",xlim=c(0,10),main="M-M",at=as.numeric(colnames(allsims_mm)))
points(Myx$day,Myx$titer,col="green",pch=19,cex=2)
plot(SSEsim_rick~SSEobs_rick,main="Posterior Pred Check, Ricker")
abline(0,1,col="red")
plot(SSEsim_mm~SSEobs_mm,main="Posterior Pred Check, M-M")
abline(0,1,col="red")
pval_rick <- length(which(SSEsim_rick>SSEobs_rick))/lots
pval_mm <- length(which(SSEsim_mm>SSEobs_mm))/lots
output <- list()
output[["p-vals"]] <- c(pval_rick,pval_mm)
output[["PPC_rick"]] <- as.data.frame(cbind(params_rick,SSEsim_rick,SSEobs_rick))
output[["PPC_mm"]] <- as.data.frame(cbind(params_mm,SSEsim_mm,SSEobs_mm))
colnames <- c("a","b","shape","SSEsim","SSEobs")
names(output[["PPC_rick"]]) <- colnames
names(output[["PPC_mm"]]) <- colnames
return(output)
}
####
# Test the function
test <- Myx_PostPredCheck(MCMC1=jags.fit_ricker,MCMC2=jags.fit_mm)
test[[1]]   # p-vals
head(test[[2]])    # ppc for ricker
head(test[[3]])    # ppc for M-M
rmd2rscript <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript_labanswers <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- c("answer","test")
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- grepl(input, pattern = stringToFind2[1], perl = TRUE) | grepl(input, pattern = stringToFind2[2])
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript("SpatialAutocorrelation.Rmd")
rmd2rscript("SpatialAutocorrelation.Rmd")
install.packages("gstat")
install.packages("ape")
install.packages("geosphere")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
############################################################
####                                                    ####
####  NRES 746, Student-led topic #3                    ####
####                                                    ####
############################################################
############################################################
####  Spatial autocorrelation                           ####
############################################################
#first things first: here are the packages you'll need to follow along:
list = c('gstat', 'raster', 'geosphere', 'ape', 'foreach', 'doParallel', 'rgdal')
#install.packages(list)
library(gstat)
library(raster)
xy <- expand.grid(1:100, 1:100) # create a coordinate grid to represent a real landscape.
# the larger the grid, the longer it will take to generate data
names(xy) <- c('x','y') # name the axes of the grid
# Specify a model to create a spatially-autocorrelated z variable.
# range parameter controls degree of spatial autocorrelation.
saLandscapeModel <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(100),
model=vgm(psill=100, range=100, model='Exp'), nmax=20)
saLandscape <- predict(saLandscapeModel, newdata = xy, nsim = 1)
image(saLandscape, axes = FALSE, col = terrain.colors(10))
sample <- xy[sample(nrow(xy), 30),] # Randomly sample pixels from the grid
samplePoints <- SpatialPoints(sample) # Create a SpatialPoints object
colors <- topo.colors(20)
# Extract explanatory values.
names(saLandscape) <- c('x','y','z') # Step 1: rename columns in envImage df for clarity
gridded(saLandscape)=~x+y # Step 2: Data frame must be converted to gridded object to create raster layer
saRaster <- raster(saLandscape) # Step 3: convert gridded object to raster
elev <- raster::extract(x=saRaster, y=samplePoints) # Step 4: extract values.
sample <- cbind(sample, elev)
row.names(sample) <- (1:30)
print(head(sample))
image(saLandscape, axes = FALSE, col = terrain.colors(10))
points(sample)
semivariogram <- function(value,x,y){
# building empty and null vectors
dist <- vector(mode="numeric", length=length(value))
semivar <- vector(mode="numeric", length=length(value))
distance <- c()
semivariance <- c()
# calculating all possible
for (i in 1:length(value)) {                    # these loops compare all the values with each other
for( j in 1:length(value)) {
dist[j] <-  sqrt((x[i]-x[j])^2 + (y[i]-y[j])^2)           # measuring the distance
semivar[j] <- (value[i]-value[j])^2                       # calcualting the semivariance
}
distance <- c(distance,dist)
semivariance <- c(semivariance,semivar)
}
plot(distance, semivariance, xlab="Distance Between Points", ylab="Squared Difference")
}
semivariogram(sample$elev,sample$x,sample$y)
semivariogram.ll <- function(value,lat,lon){
library(geosphere)
# building empty and null vectors
dist <- vector(mode="numeric", length=length(value))
semivar <- vector(mode="numeric", length=length(value))
distance <- c()
semivariance <- c()
# calculating all possible
for (i in 1:length(value)) {
for( j in 1:length(value)) {
dist[j] <- distm(c(lon[i], lat[i]), c(lon[j], lat[j]), fun = distHaversine) # measuring the euclidean distance [meters]
semivar[j] <- (value[i]-value[j])^2          # calcualting the semivariance
}
distance <- c(distance,dist)
semivariance <- c(semivariance,semivar)
}
plot(distance, semivariance, xlab="Distance [meters]", ylab="Squaired Difference")
#data <- cbind(distance,semivariance)
#return(data)
}
week1_transects <- read.csv("SnowEx17_GPR_Week1_transects.txt")
tail(week1_transects)
########### seperating the data by transect ###########
transect46 = week1_transects[which(week1_transects$TRANSECT == 46),]
transect1 = week1_transects[which(week1_transects$TRANSECT == 1),]
# building a small data set for the semivarance
df.new <- transect46[seq(1, nrow(transect1), 200),]
#df.new <- week1_transects[seq(1, nrow(week1_transects), 1000),] #this will take all the transects
# seprating data for the function
lat <- df.new$LAT
lon <- df.new$LONG
x <- df.new$SNOW.DEPTH..m..assuming.velocity.of.0.234.m.ns.
# run the function
#semivariogram.ll(x,lat,lon)
semivariogram.ll(x,lat,lon)
# building a small data set for the semivarance
df.new <- transect46[seq(1, nrow(transect1), 500),]
#df.new <- week1_transects[seq(1, nrow(week1_transects), 1000),] #this will take all the transects
# seprating data for the function
lat <- df.new$LAT
lon <- df.new$LONG
x <- df.new$SNOW.DEPTH..m..assuming.velocity.of.0.234.m.ns.
semivariogram.ll(x,lat,lon)
#install.packages("ape")
library("ape")
#look at package "ape"
#??ape
#function we will use is Moran.I()
#?Moran.I
#look at the help and see the Moran's forumla
#requires "x" - a numeric vector - which is our variable.
#requires "weight" - a matrix of weights - calculated using dist().
#generating IDW matrix
#use dist() to compute and return the distance matrix between rows of data
#?dist
sample.distances<-as.matrix(dist(cbind(sample$x,sample$y))) #distance weight matrix
sample.distances.inverse<-1/sample.distances #inverse distance weight matrix
sample.distances.inverse[1:5,1:5] #however "infin" problem"
install.packages("sp")
install.packages("sp")
install.packages("sp")
install.packages("sp")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
############################################################
####                                                    ####
####  NRES 746, Student-led topic #3                    ####
####                                                    ####
############################################################
############################################################
####  Spatial autocorrelation                           ####
############################################################
#first things first: here are the packages you'll need to follow along:
list = c('gstat', 'raster', 'geosphere', 'ape', 'foreach', 'doParallel', 'rgdal')
#install.packages(list)
library(gstat)
library(raster)
xy <- expand.grid(1:100, 1:100) # create a coordinate grid to represent a real landscape.
# the larger the grid, the longer it will take to generate data
names(xy) <- c('x','y') # name the axes of the grid
# Specify a model to create a spatially-autocorrelated z variable.
# range parameter controls degree of spatial autocorrelation.
saLandscapeModel <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(100),
model=vgm(psill=100, range=100, model='Exp'), nmax=20)
saLandscape <- predict(saLandscapeModel, newdata = xy, nsim = 1)
image(saLandscape, axes = FALSE, col = terrain.colors(10))
sample <- xy[sample(nrow(xy), 30),] # Randomly sample pixels from the grid
samplePoints <- SpatialPoints(sample) # Create a SpatialPoints object
colors <- topo.colors(20)
# Extract explanatory values.
names(saLandscape) <- c('x','y','z') # Step 1: rename columns in envImage df for clarity
gridded(saLandscape)=~x+y # Step 2: Data frame must be converted to gridded object to create raster layer
saRaster <- raster(saLandscape) # Step 3: convert gridded object to raster
elev <- raster::extract(x=saRaster, y=samplePoints) # Step 4: extract values.
sample <- cbind(sample, elev)
row.names(sample) <- (1:30)
print(head(sample))
image(saLandscape, axes = FALSE, col = terrain.colors(10))
points(sample)
semivariogram <- function(value,x,y){
# building empty and null vectors
dist <- vector(mode="numeric", length=length(value))
semivar <- vector(mode="numeric", length=length(value))
distance <- c()
semivariance <- c()
# calculating all possible
for (i in 1:length(value)) {                    # these loops compare all the values with each other
for( j in 1:length(value)) {
dist[j] <-  sqrt((x[i]-x[j])^2 + (y[i]-y[j])^2)           # measuring the distance
semivar[j] <- (value[i]-value[j])^2                       # calcualting the semivariance
}
distance <- c(distance,dist)
semivariance <- c(semivariance,semivar)
}
plot(distance, semivariance, xlab="Distance Between Points", ylab="Squared Difference")
}
semivariogram(sample$elev,sample$x,sample$y)
semivariogram.ll <- function(value,lat,lon){
library(geosphere)
# building empty and null vectors
dist <- vector(mode="numeric", length=length(value))
semivar <- vector(mode="numeric", length=length(value))
distance <- c()
semivariance <- c()
# calculating all possible
for (i in 1:length(value)) {
for( j in 1:length(value)) {
dist[j] <- distm(c(lon[i], lat[i]), c(lon[j], lat[j]), fun = distHaversine) # measuring the euclidean distance [meters]
semivar[j] <- (value[i]-value[j])^2          # calcualting the semivariance
}
distance <- c(distance,dist)
semivariance <- c(semivariance,semivar)
}
plot(distance, semivariance, xlab="Distance [meters]", ylab="Squaired Difference")
#data <- cbind(distance,semivariance)
#return(data)
}
week1_transects <- read.csv("SnowEx17_GPR_Week1_transects.txt")
tail(week1_transects)
########### seperating the data by transect ###########
transect46 = week1_transects[which(week1_transects$TRANSECT == 46),]
transect1 = week1_transects[which(week1_transects$TRANSECT == 1),]
# building a small data set for the semivarance
df.new <- transect46[seq(1, nrow(transect1), 200),]
#df.new <- week1_transects[seq(1, nrow(week1_transects), 1000),] #this will take all the transects
# seprating data for the function
lat <- df.new$LAT
lon <- df.new$LONG
x <- df.new$SNOW.DEPTH..m..assuming.velocity.of.0.234.m.ns.
# run the function
#semivariogram.ll(x,lat,lon)
semivariogram.ll(x,lat,lon)
# building a small data set for the semivarance
df.new <- transect46[seq(1, nrow(transect1), 500),]
#df.new <- week1_transects[seq(1, nrow(week1_transects), 1000),] #this will take all the transects
# seprating data for the function
lat <- df.new$LAT
lon <- df.new$LONG
x <- df.new$SNOW.DEPTH..m..assuming.velocity.of.0.234.m.ns.
semivariogram.ll(x,lat,lon)
#install.packages("ape")
library("ape")
#look at package "ape"
#??ape
#function we will use is Moran.I()
#?Moran.I
#look at the help and see the Moran's forumla
#requires "x" - a numeric vector - which is our variable.
#requires "weight" - a matrix of weights - calculated using dist().
#generating IDW matrix
#use dist() to compute and return the distance matrix between rows of data
#?dist
sample.distances<-as.matrix(dist(cbind(sample$x,sample$y))) #distance weight matrix
sample.distances.inverse<-1/sample.distances #inverse distance weight matrix
sample.distances.inverse[1:5,1:5] #however "infin" problem"
#Remove infinity and replace with 0's - occurs because 1 divide by 0 is infinity
diag(sample.distances.inverse)<-0
sample.distances.inverse[1:5,1:5]
#Morans I
#?Moran.I
#Weights are obtained, now place desired variable to test as "x"
Moran.I(x=sample$elev,weight=sample.distances.inverse)
#Results!
## Parallel Spatial Autocorrelation Simulation
library(parallel)
library(foreach)
library(doParallel)
numCores <- detectCores() - 1
registerDoParallel(numCores)
resultsList<-foreach(i=1:100, .combine = c) %dopar% {
library(gstat)
library(raster)
xy <- expand.grid(1:100, 1:100) # create a coordinate grid to represent a real landscape.
# the larger the grid, the longer it will take to generate data
names(xy) <- c('x','y') # name the axes
# If desired, model deterministic component of explanatory variable on landscape surface
# by altering the beta parameters in the gstat() call.
# Currently set to 0 (no deterministic relationship between space and explanatory variable)
envDeterm <- gstat(formula=z~1+x+y, locations=~x+y, dummy=T, beta=c(1,0,0),
model=vgm(psill=0, range=1, model='Exp'), nmax=1)
# Model spatially-autocorrelated component of explanatory variable on landscape surface.
# range parameter controls degree of spatial autocorrelation. Set to 1 to eliminate spatial autocorrelation.
envSA <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0),
model=vgm(psill=.025, range=20, model='Gau'), nmax=2)
# Model stochastic (error) component of explanatory variable on landscape surface.
envErr <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0),
model=vgm(psill=.025, range=1, model='Gau'), nmax=1)
# Simulate data from each model across the landscape surface.
# Can visualize each surface using image() if desired
eDe <- predict(envDeterm, newdata=xy, nsim=1)
eSa <- predict(envSA, newdata=xy, nsim=1)
eEr <- predict(envErr, newdata=xy, nsim=1)
# image(eDe)
# image(eSa)
# image(eEr)
# Add each component into a single landscape surface.
envImage <- eDe+eSa+eEr
# image(envImage)
# Model spatially-autocorrelated component of response variable on landscape surface.
# range parameter controls degree of spatial autocorrelation. Set to 1 to eliminate spatial autocorrelation.
resSA <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0),
model=vgm(psill=.025, range=20, model='Gau'), nmax=2)
# Model stochastic (error) component of response variable on landscape surface.
resErr <- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0),
model=vgm(psill=.025, range=1, model='Gau'), nmax=1)
# Simulate data from response variable SA and error models across the landscape surface.
# Again, can visualize each surface using image() if desired
rSa <- predict(resSA, newdata=xy, nsim=1)
rEr <- predict(resErr, newdata=xy, nsim=1)
# image(rSA)
# image(rEr)
# Combine the explanatory variable surface with the response SA and error
# surfaces to generate predicted response values. If there is no true
# relationship between the explanatory and response variables, the beta1
# parameter is set to zero.
beta1 <- 0
resImage <- (beta1*envImage$sim1) + rSa + rEr
# Create random sample points
sample <- xy[sample(nrow(xy), 100),] # Randomly sample pixels from the grid
samplePoints <- SpatialPoints(sample) # Create a SpatialPoints object
# Extract explanatory and response values using the sampling locations. There
# is probably a simpler way to do this, but in this case I am creating rasters
# from the envImage and resImage data frames, then using raster::extract to
# extract the explanatory and response values and combine them in a new data
# frame of simulated data.
# Extract explanatory values.
names(envImage) <- c('x','y','explanatory') # Step 1: rename columns in envImage df for clarity
gridded(envImage)=~x+y # Step 2: Data frame must be converted to gridded object to create raster layer
envRaster <- raster(envImage) # Step 3: convert gridded object to raster
explanatory <- raster::extract(x=envRaster, y=samplePoints) # Step 4: extract values.
# Extract response values. Repeat steps 1-4.
names(resImage) <- c('x','y','response')
gridded(resImage)=~x+y
resRaster <- raster(resImage)
response <- raster::extract(x=resRaster, y=samplePoints)
# Bind locations, explanatory values, and response values into a singe data
# frame of simulated data
sample <- cbind(sample,explanatory,response)
# Create simple linear model and extract the p-value.
model <- lm(response ~ explanatory)
pvals <- summary(model)$coefficients[,4]
# pvals
i <- as.numeric(pvals[2]) # Store the p-value for the beta1 parameter in the resultsList
}
rmarkdown::render('index.Rmd', 'word_document')
rmarkdown::render('schedule.Rmd', 'word_document')
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
rmarkdown::render('index.Rmd',rmarkdown::pdf_document())
rmd2rscript <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- "echo"
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- !grepl(input, pattern = stringToFind2, perl = TRUE)   # is it hidden (echo=FALSE)
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript_labanswers <- function(infile="LECTURE2.Rmd"){    # function for converting markdown to scripts
outfile <- gsub(".Rmd",".R",infile)
close( file( outfile, open="w" ) )   # clear output file
con1 <- file(infile,open="r")
con2 <- file(outfile,"w")
stringToFind <- "```{r*"
stringToFind2 <- c("answer","test")
isrblock <- FALSE
#count=0
blocknum=0
while(length(input <- readLines(con1, n=1)) > 0){   # while there are still lines to be read
isrblock <- grepl(input, pattern = stringToFind, perl = TRUE)   # is it the start of an R block?
showit <- grepl(input, pattern = stringToFind2[1], perl = TRUE) | grepl(input, pattern = stringToFind2[2])
if(isrblock){
blocknum=blocknum+1
while(!grepl(newline<-readLines(con1, n=1),pattern="```",perl=TRUE)){
if((blocknum>1)&((showit)|(blocknum==2))) write(newline,file=con2,append=TRUE)
#count=count+1
}
isrblock=FALSE
}
}
closeAllConnections()
}
rmd2rscript("SpatialAutocorrelation.Rmd")
