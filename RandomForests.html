<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="William Richardson, Wade Lieurance" />

<meta name="date" content="2019-11-27" />

<title>Random Forests</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 54px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h2 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h3 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h4 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h5 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h6 {
  padding-top: 59px;
  margin-top: -59px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
    <li>
      <a href="LECTURE10.html">Machine Learning</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="SDM_v6.html">Species distribution models</a>
    </li>
    <li>
      <a href="BayesianNetworks-1.html">Bayesian Networks</a>
    </li>
    <li>
      <a href="TimeSeries_Draft.html">Time Series</a>
    </li>
    <li>
      <a href="RandomForests.html">Random Forest</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Random Forests</h1>
<h4 class="author">William Richardson, Wade Lieurance</h4>
<h4 class="date">11/27/2019</h4>

</div>


<p><img src="forest.gif" width="800px" style="display: block; margin: auto;" /></p>
<p>NOTE: you can download the R code <a href="RandomForests.R">here</a></p>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p><strong><em>What are </em>Random Forests?</strong>*</p>
<p><em>An ensemble learning method utilizing multiple decision trees at training time and outputting the class that is the mode or mean of the individual trees.</em></p>
<p>Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem.</p>
<p>In short, a random forest is a predictive, non-linear, computational algorithm based on decision trees.</p>
<p><strong><em>OK… so what’s a </em>Decision Tree* then?</strong>*</p>
<p>Woah there pardner! Don’t put the <strong><em>CART</em></strong> before the horse!</p>
<p><img src="cart.gif" width="400px" /></p>
<p>In this case, <strong><em>CART</em></strong> is an acronym for <em>Classification and Regression Trees</em>.</p>
<p>Originally described in <span class="citation">(Breiman 1984)</span>, a <em>classification tree</em> is a partitioning of categorical data and a <em>regression tree</em> is a partitioning of continuous numerical data.</p>
<p>These split partitions or regions aim to maximize the homogeneity of the target variable.</p>
</div>
<div id="thinking-about-partitioning" class="section level1">
<h1><span class="header-section-number">2</span> Thinking about Partitioning</h1>
<p>If we wanted to maximize homogeneity of a continuous response variable by dividing it into regions, how might we go about that?</p>
<div id="simple-example-quantiles" class="section level2">
<h2><span class="header-section-number">2.1</span> Simple Example: Quantiles</h2>
<p>Lets look at one of the example data sets in R: <strong><em>mtcars</em></strong>.</p>
<table>
<caption>mtcars</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">mpg</th>
<th align="right">cyl</th>
<th align="right">disp</th>
<th align="right">hp</th>
<th align="right">drat</th>
<th align="right">wt</th>
<th align="right">qsec</th>
<th align="right">vs</th>
<th align="right">am</th>
<th align="right">gear</th>
<th align="right">carb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mazda RX4</td>
<td align="right">21.0</td>
<td align="right">6</td>
<td align="right">160</td>
<td align="right">110</td>
<td align="right">3.90</td>
<td align="right">2.620</td>
<td align="right">16.46</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td>Mazda RX4 Wag</td>
<td align="right">21.0</td>
<td align="right">6</td>
<td align="right">160</td>
<td align="right">110</td>
<td align="right">3.90</td>
<td align="right">2.875</td>
<td align="right">17.02</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td>Datsun 710</td>
<td align="right">22.8</td>
<td align="right">4</td>
<td align="right">108</td>
<td align="right">93</td>
<td align="right">3.85</td>
<td align="right">2.320</td>
<td align="right">18.61</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>Hornet 4 Drive</td>
<td align="right">21.4</td>
<td align="right">6</td>
<td align="right">258</td>
<td align="right">110</td>
<td align="right">3.08</td>
<td align="right">3.215</td>
<td align="right">19.44</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td>Hornet Sportabout</td>
<td align="right">18.7</td>
<td align="right">8</td>
<td align="right">360</td>
<td align="right">175</td>
<td align="right">3.15</td>
<td align="right">3.440</td>
<td align="right">17.02</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p><strong>mpg</strong> = Miles/(US) gallon, <strong>cyl</strong> = Number of cylinders, <strong>disp</strong> = Displacement (cu.in.), <strong>hp</strong> = Gross horsepower, <strong>drat</strong> = Rear axle ratio, <strong>wt</strong> = Weight (1000 lbs), <strong>qsec</strong> = 1/4 mile time, <strong>vs</strong> = Engine (0 = V-shaped, 1 = straight), <strong>am</strong> = Transmission (0 = automatic, 1 = manual), <strong>gear</strong> = Number of forward gears, <strong>carb</strong> = Number of carburetors.</p>
<p>Let’s take a look at <em>mpg</em> as a response variable to <em>disp</em> and fit a linear model to it. <img src="RandomForests_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p><strong>SSE:</strong> 317.2</p>
<p>What sort of gain do we get if we partition the data set into R = 4 regions and calculate a mean?</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><strong>SSE:</strong> 220.5</p>
<p>Can we improve our SSE gain if we partition the data set into R = 10 regions?</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p><strong>SSE:</strong> 139.6</p>
<p>What about R = n regions (where n = unique(x))?</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><strong>SSE:</strong> 13</p>
<p>We could go even further by fitting linear models to each region instead of just taking the mean of region R.</p>
<p><strong><em>Simple partitioning - Some drawbacks</em></strong></p>
<ul>
<li>Clearly easy to over fit the data based on the number of partitions chosen (low bias, large variance).</li>
<li>How do we decide how many partitions to use to maximize the utility of the partition?</li>
<li>What if we wanted some way to partition <em>disp</em> that isn’t based on quantiles? Something more flexible?</li>
<li>What if we had many possible predictor variables (as in this case), some continuous and some categorical? Where would we partition?</li>
</ul>
</div>
<div id="decision-trees---a-partial-solution" class="section level2">
<h2><span class="header-section-number">2.2</span> Decision Trees - A partial solution…</h2>
<div id="binary-splitting-with-continuous-response-regression-trees" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Binary Splitting with Continuous Response (Regression Trees)</h3>
<p>We can use binary splitting to brute force selection of regions R.</p>
<p>If we break x = displacement between every possible break point and calculate the squared estimate of errors for regions left of the break and right of the break and add them, we can determine which breaking point results in the minimal SSE.</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>If we then plot this regional break on out mpg ~ disp plot we get:</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>If we continue to partition regions R<sub>1</sub> and R<sub>2</sub> into sub-partitions, and those into even further partitions, we can see the result:</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We can visualize this process through a decision tree diagram and with the help of the rpart library!</p>
<pre class="r"><code>library(rpart)
library(rpart.plot)
tree &lt;- rpart(formula = mpg~disp, data = mtcars, 
              control = rpart.control(maxdepth = 3, minbucket = 1, cp = 0))
prp(tree, faclen = 0, cex = 0.8, extra = 1)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Decision trees have 4 different parts:</p>
<ol style="list-style-type: decimal">
<li>Root Node</li>
<li>Internal Nodes</li>
<li>Branches</li>
<li>Leaf Nodes</li>
</ol>
<p>In the above regression tree, we have passed some control variables to rpart() in order to control how our tree is grown. In this case:</p>
<ul>
<li><strong>maxdepth = 3</strong> tells rpart to grow our tree to a depth of <strong>3</strong> (or to split 3 times).</li>
<li><strong>minbucket = 1</strong> tells rpart that the minimum <em>n</em> for a leaf node is <strong>1</strong></li>
<li><strong>cp = 0</strong> is a complexity parameter.</li>
</ul>
<p>The complexity parameter cp (or <span class="math inline">\(\alpha\)</span>) specifies how the cost of a tree <span class="math inline">\(C(T)\)</span> is penalized by the number of terminal nodes <span class="math inline">\(|T|\)</span>, resulting in a regularized cost <span class="math inline">\(C_{\alpha}(T)\)</span></p>
<p>Thus:</p>
<p><span class="math inline">\(C_{\alpha}(T) = C(T) + \alpha|T|\)</span></p>
<p>Small <span class="math inline">\(\alpha\)</span> results in lartger trees with potential overfitting. Large <span class="math inline">\(\alpha\)</span> in small trees an potential underfitting. In practice, setting cp = 0 sets a cost of 0 to new branches of our tree, meaning there is no penalized cost for adding nodes.</p>
<p>Higher values ‘pre-prune’ our tree, ignoring splits that don’t meet a minimum target for increased homogeneity at a certain node depth.</p>
<p>There are many control parameters we can pass to rpart to determine how the tree is grown!</p>
</div>
<div id="binary-splitting-with-multiple-variables" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Binary Splitting with Multiple Variables</h3>
<p>What if we wanted to partition our data according to multiple input variables?</p>
<p>Luckily this algorithm is not restricted to a single partitioning variable!</p>
<p>Conceptually the process can be thought of thus:</p>
<ol style="list-style-type: decimal">
<li>For each variable <strong>k</strong>, determine the minimum SSE for the first split.</li>
<li>Compare these SSEs from all <strong>k</strong></li>
<li>Create the root node from the <strong>k</strong> split that gives min(SSE).</li>
<li>Repeat steps 1-3 on the newly branched nodes for all <strong>k</strong>.</li>
</ol>
<p>While we could expand our binary splitting algorithms from above to produce these results, lets take advantage of rpart!</p>
<p>Lets use a fancier plotting library this time.</p>
<pre class="r"><code>library(rattle)
library(RColorBrewer)
tree &lt;- rpart(formula = mpg~disp+wt, data = mtcars, 
              control = rpart.control(maxdepth = 3, minbucket = 1, cp = 0))
fancyRpartPlot(tree, caption = NULL)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>What would this look like if we plotted mpg by both displacement and weight and added the nodes from our tree?</p>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>And if we used many more predictor variables?</p>
<pre class="r"><code>tree.m &lt;- rpart(formula = mpg~disp+wt+cyl+drat+vs+am+gear+carb, data = mtcars, 
              control = rpart.control(maxdepth = 3, minbucket = 1, cp = 0))
fancyRpartPlot(tree.m, caption = NULL)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="categorical-response-classification-trees" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Categorical Response (Classification Trees)</h3>
<p>What if we had a categorical response variable? CART algorithms handle these as well!</p>
<p>Lets modify our mtcars example and change mpg to a categorical variable.</p>
<pre class="r"><code>mtcars.class &lt;- mutate(mtcars,
                mpg = case_when(mpg &lt; 16.7 ~ &quot;low&quot;,
                                mpg &gt;= 16.7 &amp; mpg &lt; 21.4 ~ &quot;medium&quot;,
                                mpg &gt;= 21.4 ~ &quot;high&quot;))
kable(mtcars.class[1:7,], caption = &quot;mtcars (classified)&quot;)</code></pre>
<table>
<caption>mtcars (classified)</caption>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">mpg</th>
<th align="right">cyl</th>
<th align="right">disp</th>
<th align="right">hp</th>
<th align="right">drat</th>
<th align="right">wt</th>
<th align="right">qsec</th>
<th align="right">vs</th>
<th align="right">am</th>
<th align="right">gear</th>
<th align="right">carb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Mazda RX4</td>
<td align="left">medium</td>
<td align="right">6</td>
<td align="right">160</td>
<td align="right">110</td>
<td align="right">3.90</td>
<td align="right">2.620</td>
<td align="right">16.46</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">Mazda RX4 Wag</td>
<td align="left">medium</td>
<td align="right">6</td>
<td align="right">160</td>
<td align="right">110</td>
<td align="right">3.90</td>
<td align="right">2.875</td>
<td align="right">17.02</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left">Datsun 710</td>
<td align="left">high</td>
<td align="right">4</td>
<td align="right">108</td>
<td align="right">93</td>
<td align="right">3.85</td>
<td align="right">2.320</td>
<td align="right">18.61</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">Hornet 4 Drive</td>
<td align="left">high</td>
<td align="right">6</td>
<td align="right">258</td>
<td align="right">110</td>
<td align="right">3.08</td>
<td align="right">3.215</td>
<td align="right">19.44</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">Hornet Sportabout</td>
<td align="left">medium</td>
<td align="right">8</td>
<td align="right">360</td>
<td align="right">175</td>
<td align="right">3.15</td>
<td align="right">3.440</td>
<td align="right">17.02</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">Valiant</td>
<td align="left">medium</td>
<td align="right">6</td>
<td align="right">225</td>
<td align="right">105</td>
<td align="right">2.76</td>
<td align="right">3.460</td>
<td align="right">20.22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">Duster 360</td>
<td align="left">low</td>
<td align="right">8</td>
<td align="right">360</td>
<td align="right">245</td>
<td align="right">3.21</td>
<td align="right">3.570</td>
<td align="right">15.84</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">4</td>
</tr>
</tbody>
</table>
<p>While we can attempt to minimize SSE for a continuous response variable, we need a different method for categorical response variables.</p>
<p>The two methods used in rpart are:</p>
<ol style="list-style-type: decimal">
<li>Gini impurity (default)</li>
<li>Information gain</li>
</ol>
<p>Lets have a look a Gini impurity as its description is more straightforward.</p>
<p>With a set of items with <span class="math inline">\(J\)</span> classes, <span class="math inline">\(i \in \{1,2,...,J\}\)</span> and <span class="math inline">\(p_{i}\)</span> being the fraction labeled with class <span class="math inline">\(i\)</span> in the set. Then the Gini impurity can be calculated as the following:</p>
<p><span class="math display">\[I_{G}(p) =\sum_{i = 1}^Jp_{i}\sum_{k \neq i}p_{k} =\sum_{i = 1}^J p_{i}(1 - p_{i})\]</span> As a simple example lets say we have the following data set:</p>
<pre class="r"><code>set &lt;- c(&quot;horse&quot;, &quot;horse&quot;, &quot;cart&quot;, &quot;cart&quot;, &quot;cart&quot;)
set</code></pre>
<pre><code>## [1] &quot;horse&quot; &quot;horse&quot; &quot;cart&quot;  &quot;cart&quot;  &quot;cart&quot;</code></pre>
<p>then the Gini impurity would be:</p>
<pre class="r"><code>p.horse &lt;- length(set[set == &quot;horse&quot;])/length(set) # 2/5
p.cart &lt;-  length(set[set == &quot;cart&quot;])/length(set)  # 3/5
I &lt;- (p.horse * (1-p.horse))+ (p.cart * (1-p.cart))
I  # 0.48</code></pre>
<pre><code>## [1] 0.48</code></pre>
<p>or <span class="math inline">\(0.4 * (1 - 0.4) + 0.6 * (1 - 0.6) = 0.4*0.6 + 0.6 * 0.4 = 0.24 + 0.24 = 0.48\)</span></p>
<p>Gini impurity is a measure of impurity, so what would happen if our set is composed of just one class? Then:</p>
<p><span class="math inline">\(1*(1-1) + 0*(1-0) = 0 + 0 = 0\)</span></p>
<p>So we then can use a decrease in Gini impurity to classify whether a split is a good choice. We do this by calculating the <strong>Gini Gain</strong>. Lets say we split the above set into two shabby groups:</p>
<pre class="r"><code>set.1 &lt;- c(&quot;horse&quot;, &quot;cart&quot;)
set.2 &lt;- c(&quot;horse&quot;, &quot;cart&quot;, &quot;cart&quot;)
I.1 &lt;- (0.5*(1-0.5)) + (0.5*(1-0.5))  # 0.5
I.2 &lt;- (1/3 * (1 - 1/3)) + (2/3 * (1 - 2/3))  # 0.44
I.1</code></pre>
<pre><code>## [1] 0.5</code></pre>
<pre class="r"><code>I.2</code></pre>
<pre><code>## [1] 0.4444444</code></pre>
<p>We can then take the sum of the Gini Impurity for each group, weighted by the number of members of each group.</p>
<pre class="r"><code>I.new &lt;- (I.1 * length(set.1)/length(set)) + (I.2 * length(set.2)/length(set))
I.new</code></pre>
<pre><code>## [1] 0.4666667</code></pre>
<pre class="r"><code>I.gain &lt;- I - I.new
I.gain</code></pre>
<pre><code>## [1] 0.01333333</code></pre>
<p>I this case our split is only marginally better than the unsplit data. If we were to change our node groupings to pure horses and carts, what sort of gain would we get? Well, we know that a homogeneous group has a Gini impurity of zero, so: <span class="math inline">\(0.4 * 0 + 0.6 * 0 = 0\)</span> and thus <span class="math inline">\(0.48 - 0 = 0.48\)</span>, a significant gain!</p>
<p>Knowing this, lets take a look at a classification tree for our modified mtcars data set:</p>
<pre class="r"><code>tree.class &lt;- rpart(formula = mpg~disp+wt+hp+vs+gear, data = mtcars.class, 
              control = rpart.control(maxdepth = 3, minbucket = 1, cp = 0))
fancyRpartPlot(tree.class, caption = NULL)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>What other types of methods are available in rpart?</p>
<ul>
<li>Categorical - method==“class”</li>
<li>Continuous - method==“anova”</li>
<li>PoissonProcess/Count - method==“poisson”</li>
<li>Survival - method==“exp”</li>
</ul>
</div>
<div id="tree-complexity-and-pruning" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Tree Complexity and Pruning</h3>
<p>So far we have built a regression tree and a classification tree and given both a depth of n = 3 (three splits). But we have yet to address if our tree is a good fit for our data. How do we address this? How do we prevent over-fitting?</p>
<p>We can use k-fold cross validation and select the split that gives the minimum CV error. rpart does this cross validation automatically (n=10) for us, which we can see the results of via printcp()</p>
<pre class="r"><code>printcp(tree)</code></pre>
<pre><code>## 
## Regression tree:
## rpart(formula = mpg ~ disp + wt, data = mtcars, control = rpart.control(maxdepth = 3, 
##     minbucket = 1, cp = 0))
## 
## Variables actually used in tree construction:
## [1] disp wt  
## 
## Root node error: 1126/32 = 35.189
## 
## n= 32 
## 
##          CP nsplit rel error  xerror    xstd
## 1 0.6526612      0  1.000000 1.06204 0.25329
## 2 0.1947024      1  0.347339 0.62690 0.19928
## 3 0.0457737      2  0.152636 0.35042 0.12332
## 4 0.0250138      3  0.106863 0.34204 0.12013
## 5 0.0232497      4  0.081849 0.31209 0.11951
## 6 0.0083256      5  0.058599 0.26375 0.10245
## 7 0.0044773      6  0.050274 0.26762 0.10460
## 8 0.0000000      7  0.045796 0.25698 0.10471</code></pre>
<p>The rule of thumb is that we sum the minimum cross-validation error (xerror) and the xstd field and then choose the lowest nsplit where xerror that is &gt;= the sum of these values. In this case, min(xerror) + xstd =</p>
<pre class="r"><code>0.22539  + 0.05324</code></pre>
<pre><code>## [1] 0.27863</code></pre>
<p>In this case the min nsplit that has a cross validation error less than 0.27863 is nsplit = 4 and CP = 0.0121987. We can choose a cp for pruning which is slightly greater (0.013)</p>
<pre class="r"><code>tree.p &lt;- prune(tree, cp = 0.013)
fancyRpartPlot(tree.p, caption = NULL)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
</div>
<div id="example-regression-trees" class="section level2">
<h2><span class="header-section-number">2.3</span> Example: Regression Trees</h2>
<div id="creating-the-model-ames-housing-data-set" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Creating the Model: Ames Housing Data Set</h3>
<p><img src="AmesPhoto.jpg" width="800px" style="display: block; margin: auto;" /></p>
<p>We wanted a data set that had a high number of variables and data points.</p>
<p>The Ames Housing data set has 82 fields and 2,930 properties all from Ames, IA.</p>
<p>Some other good data sets are:</p>
<ol style="list-style-type: decimal">
<li>Package: SwissAir</li>
</ol>
<ul>
<li>Data: AirQual</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Package: lme4</li>
</ol>
<ul>
<li>Data: Arabidopsis</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Package: PASwr</li>
</ol>
<ul>
<li>Data: titanic3</li>
</ul>
<p>Some of the variables include:</p>
<ol style="list-style-type: decimal">
<li>Lot Area</li>
<li>Lot Shape</li>
<li>Utilities</li>
<li>House Style</li>
<li>Roof Style</li>
<li>Number of Car Garage</li>
<li>Pool area</li>
<li>House Condition</li>
</ol>
<p><strong>How do these variables affect sales price? Let’s find out!!!</strong></p>
<p>First let’s create a model.</p>
<p>These are the base packages you will need.</p>
<pre class="r"><code>library(rsample) 
library(dplyr)
library(rpart)
library(rpart.plot)
library(ModelMetrics)</code></pre>
<p>You should separate the data into training and test data using the ‘initial_split()’ function.</p>
<p>Additionally we will be setting a seed so that the data is split the same way each time.</p>
<pre class="r"><code>set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</code></pre>
<p>Now, let’s set up a model!!!!</p>
<pre class="r"><code>m1 &lt;- rpart(
formula = Sale_Price ~ .,
data    = ames_train,
method  = &quot;anova&quot;
)</code></pre>
<p>Let’s look at the output.</p>
<pre class="r"><code>m1</code></pre>
<pre><code>## n= 2051 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 2051 1.273987e+13 180775.50  
##    2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1703 4.032269e+12 156431.40  
##      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste 1015 1.360332e+12 131803.50  
##        8) First_Flr_SF&lt; 1048.5 611 4.924281e+11 118301.50  
##         16) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 152 1.053743e+11  91652.57 *
##         17) Overall_Qual=Average,Above_Average,Good 459 2.433622e+11 127126.40 *
##        9) First_Flr_SF&gt;=1048.5 404 5.880574e+11 152223.50  
##         18) Gr_Liv_Area&lt; 2007.5 359 2.957141e+11 145749.50 *
##         19) Gr_Liv_Area&gt;=2007.5 45 1.572566e+11 203871.90 *
##      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 688 1.148069e+12 192764.70  
##       10) Gr_Liv_Area&lt; 1725.5 482 5.162415e+11 178531.00  
##         20) Total_Bsmt_SF&lt; 1331 352 2.315412e+11 167759.00 *
##         21) Total_Bsmt_SF&gt;=1331 130 1.332603e+11 207698.30 *
##       11) Gr_Liv_Area&gt;=1725.5 206 3.056877e+11 226068.80 *
##    3) Overall_Qual=Very_Good,Excellent,Very_Excellent 348 2.759339e+12 299907.90  
##      6) Overall_Qual=Very_Good 249 9.159879e+11 268089.10  
##       12) Gr_Liv_Area&lt; 1592.5 78 1.339905e+11 220448.90 *
##       13) Gr_Liv_Area&gt;=1592.5 171 5.242201e+11 289819.70 *
##      7) Overall_Qual=Excellent,Very_Excellent 99 9.571896e+11 379937.20  
##       14) Gr_Liv_Area&lt; 1947 42 7.265064e+10 325865.10 *
##       15) Gr_Liv_Area&gt;=1947 57 6.712559e+11 419779.80  
##         30) Neighborhood=Old_Town,Edwards,Timberland 7 8.073100e+10 295300.00 *
##         31) Neighborhood=College_Creek,Somerset,Northridge_Heights,Northridge,Stone_Brook 50 4.668730e+11 437207.00  
##           62) Total_Bsmt_SF&lt; 2168.5 40 1.923959e+11 408996.90 *
##           63) Total_Bsmt_SF&gt;=2168.5 10 1.153154e+11 550047.30 *</code></pre>
<p>The tree looks like this.</p>
<pre class="r"><code>rpart.plot(m1)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>The ‘rpart()’ function automatically applies a range of cost complexity.</p>
<p>Remember that the cost complexity parameter penalizes our model for every additional terminal node of the tree.</p>
<p><span class="math inline">\(minimize(SSE + ccp*T)\)</span></p>
<pre class="r"><code>plotcp(m1)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p><span class="citation">(Breiman 1984)</span> suggests that it’s common practice to use the smallest tree within 1 standard deviation of the minimum cross validation error.</p>
<p>We can see what would happen if we generate a full tree. We do this by using ‘cp = 0’.</p>
<pre class="r"><code>m2 &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = &quot;anova&quot;, 
  control = list(cp = 0, xval = 10)
)

plotcp(m2)
abline(v = 12, lty = &quot;dashed&quot;)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>So we see that rpart does some initial pruning on its own. However, we can go deeper!!!!!!</p>
</div>
<div id="tuning" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Tuning</h3>
<p>Two common tuning tools used besides the cost complexity parameter are:</p>
<ol style="list-style-type: decimal">
<li><code>minsplit</code> : The minimum number of data points required to attempt a split before it is forced to create a terminal node. Default is 20.</li>
<li><code>maxdepth</code> : The maximum number of internal nodes between the root node and the terminal nodes.</li>
</ol>
<p>Let’s mess with these variables!!!!</p>
<pre class="r"><code>m3 &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = &quot;anova&quot;, 
  control = list(minsplit = 10, maxdepth = 12, xval = 10)
)

m3$cptable</code></pre>
<pre><code>##            CP nsplit rel error    xerror       xstd
## 1  0.46690132      0 1.0000000 1.0004448 0.05850012
## 2  0.11961409      1 0.5330987 0.5343156 0.03093134
## 3  0.06955813      2 0.4134846 0.4148699 0.03035832
## 4  0.02559992      3 0.3439265 0.3455539 0.02190359
## 5  0.02196620      4 0.3183265 0.3259151 0.02168056
## 6  0.02023390      5 0.2963603 0.3062045 0.02114604
## 7  0.01674138      6 0.2761264 0.3061135 0.02176061
## 8  0.01188709      7 0.2593850 0.2917534 0.02058535
## 9  0.01127889      8 0.2474980 0.2872380 0.02441006
## 10 0.01109955      9 0.2362191 0.2850234 0.02440721
## 11 0.01060346     11 0.2140200 0.2829790 0.02334151
## 12 0.01000000     12 0.2034165 0.2735069 0.02260957</code></pre>
<p>We can perform a hyperparameter grid to determine the best values for these parameters.</p>
<pre class="r"><code>hyper_grid &lt;- expand.grid(
minsplit = seq(5, 20, 1),
maxdepth = seq(8, 15, 1)
)
head(hyper_grid)</code></pre>
<pre><code>##   minsplit maxdepth
## 1        5        8
## 2        6        8
## 3        7        8
## 4        8        8
## 5        9        8
## 6       10        8</code></pre>
<pre class="r"><code>nrow(hyper_grid)</code></pre>
<pre><code>## [1] 128</code></pre>
<p>Now let’s run a for loop to determine what are the best values!!!</p>
<pre class="r"><code>models &lt;- list()

for (i in 1:nrow(hyper_grid)) {

minsplit &lt;- hyper_grid$minsplit[i]
maxdepth &lt;- hyper_grid$maxdepth[i]

models[[i]] &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = &quot;anova&quot;,
  control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}</code></pre>
<p>Let’s use some data wrangling and handy dandy R to figure out the top 5 sequences of values which would produce the lowest error values.</p>
<pre class="r"><code>get_cp &lt;- function(x) {
  min    &lt;- which.min(x$cptable[, &quot;xerror&quot;])
  cp &lt;- x$cptable[min, &quot;CP&quot;] 
}

get_min_error &lt;- function(x) {
  min    &lt;- which.min(x$cptable[, &quot;xerror&quot;])
  xerror &lt;- x$cptable[min, &quot;xerror&quot;] 
}

hyper_grid %&gt;%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
    ) %&gt;%
  arrange(error) %&gt;%
  top_n(-5, wt = error)</code></pre>
<pre><code>##   minsplit maxdepth         cp     error
## 1       16       12 0.01060346 0.2628987
## 2        6       11 0.01000000 0.2645615
## 3       11       11 0.01000000 0.2650862
## 4       10       10 0.01000000 0.2655860
## 5        7       15 0.01000000 0.2656602</code></pre>
<p>If we are satisfied, we can use these values and create a new model. With this model, we can predict the sales prices of the test data.</p>
<pre class="r"><code>optimal_tree &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = &quot;anova&quot;,
  control = list(minsplit = 11, maxdepth = 8, cp = 0.01)
  )

pred &lt;- predict(optimal_tree, newdata = ames_test)
rmse(pred = pred, actual = ames_test$Sale_Price)</code></pre>
<pre><code>## [1] 39852.01</code></pre>
</div>
</div>
</div>
<div id="ensemble-methods" class="section level1">
<h1><span class="header-section-number">3</span> Ensemble Methods</h1>
<div id="bootstrap-aggregating-or-bagging" class="section level2">
<h2><span class="header-section-number">3.1</span> Bootstrap Aggregating (or Bagging)</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Overview</h3>
<p>In our previous interaction with the mtcars data set, we fitted a tree and used k-fold cross validation to prune the tree in order to prevent over-fitting.</p>
<p>One similar but slightly different technique is an ensemble method called <em>bootstrap aggregating</em> or <strong>bagging</strong>.</p>
<p>In k-fold cross validation, our sample is divided into k samples where the <span class="math inline">\(k_{}i\)</span> sample is set aside for prediction and the <span class="math inline">\(k_{j \neq i}\)</span> samples are use for training data, averaging the results. <strong>Each fold is run through the same model</strong></p>
<p>In <strong>bagging</strong> we similarly create a N subsets of our training data (with replacement) and then we fit <strong>a full unpruned model</strong> to each subset <span class="math inline">\(N_{i}\)</span> and average the models. Generally speaking, we are aiming for 50+ bootstraps.</p>
<p><img src="bagging.png" width="800px" style="display: block; margin: auto auto auto 0;" /> Source: <span class="citation">(Support 2018)</span></p>
<p>Similarly to cross validation, this method helps to reduce over-fitting of our model. Bagging is particularly useful in situations where we have unstable predictors with high variance.</p>
<p>One problem with bagging, trees produced use the same splitting variables, which can lead to highly correlated trees, <strong>especially if there are only a few highly dominant predictors!</strong></p>
</div>
<div id="example-bagging-the-ames-housing-data-set" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Example: Bagging the Ames Housing Data Set</h3>
<p><img src="baggingSterf.png" width="800px" style="display: block; margin: auto auto auto 0;" /></p>
<pre class="r"><code>library(rsample) 
library(dplyr)
library(ipred)       
library(caret)
library(ModelMetrics)
library(AmesHousing)</code></pre>
<pre class="r"><code>set.seed(123)

ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</code></pre>
<p>Fitting a bagged tree model is not that much more difficult than single regression trees. Within the model function we will use <code>coob = TRUE</code> to use the OOB sample to estimate the test error.</p>
<pre class="r"><code>set.seed(123)
bagged_m1 &lt;- bagging(
formula = Sale_Price ~ .,
data    = ames_train,
coob    = TRUE
)
bagged_m1</code></pre>
<pre><code>## 
## Bagging regression trees with 25 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     coob = TRUE)
## 
## Out-of-bag estimate of root mean squared error:  36991.67</code></pre>
<p>The default for <code>bagging</code> is 25 bootstrap samples. Let’s asses the error versus number of trees!!!</p>
<pre class="r"><code>ntree &lt;- 10:70
rmse &lt;- vector(mode = &quot;numeric&quot;, length = length(ntree))
for (i in seq_along(ntree)) {
  set.seed(123)
  model &lt;- bagging(
  formula = Sale_Price ~ .,
  data    = ames_train,
  coob    = TRUE,
  nbagg   = ntree[i]
  )
rmse[i] &lt;- model$err
}
plot(ntree, rmse, type = &#39;l&#39;, lwd = 2)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>We can also use <code>caret</code> to do some bagging. <code>caret</code> is good because it:</p>
<ol style="list-style-type: decimal">
<li>Is easier to perform cross-validation</li>
<li>We can assess variable importance</li>
</ol>
<p>Let’s perform a 10-fold cross-validated model.</p>
<pre class="r"><code>ctrl &lt;- trainControl(method = &quot;cv&quot;,  number = 10) 

bagged_cv &lt;- train(
Sale_Price ~ .,
data = ames_train,
method = &quot;treebag&quot;,
trControl = ctrl,
importance = TRUE
)

bagged_cv</code></pre>
<pre><code>## Bagged CART 
## 
## 2051 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1845, 1845, 1847, 1847, 1845, ... 
## Resampling results:
## 
##   RMSE     Rsquared   MAE     
##   36625.8  0.7909732  24276.46</code></pre>
<pre class="r"><code>plot(varImp(bagged_cv), 20)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>The predictors with the largest average impact to SSE are considered most important. The importance value is simply the relative mean decrease in SSE compared to the most important variable (provides a 0-100 scale).</p>
<pre class="r"><code>pred &lt;- predict(bagged_cv, ames_test)
rmse(pred, ames_test$Sale_Price)</code></pre>
<pre><code>## [1] 35177.35</code></pre>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">3.2</span> Random Forests</h2>
<div id="overview-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Overview</h3>
<p>And here we are, the title of our presentation. <em>Random Forests</em> are very popular machine learning algorithm. One can think of Random Forests as a variation on bagging. The primary difference?</p>
<ol style="list-style-type: decimal">
<li>Bagging uses the average prediction of many bootstrapped models.</li>
<li>Random Forest also does this, <strong>but also selects a random subset of predictor features at each candidate split</strong>.</li>
</ol>
<p>This helps deal with the correlation issue present in bagging and helps further reduce variance and stabilize our predictive model.</p>
<p><img src="rf.png" width="800px" style="display: block; margin: auto auto auto 0;" /> Source: <span class="citation">(Support 2018)</span></p>
</div>
<div id="example-random-forest-on-the-ames-housing-data" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Example: Random Forest on the Ames Housing Data</h3>
<div id="model-development" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Model Development</h4>
<p>There are over twenty different packages we can use for random forest analysis, we will be going over a few.</p>
<pre class="r"><code>library(rsample)      
library(randomForest)
library(ranger)      
library(caret)        
library(h2o)
library(AmesHousing)</code></pre>
<p>Let’s set our seed and split our data like the previous examples!</p>
<pre class="r"><code>set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</code></pre>
<p>Let’s go over the basic steps for random forest analysis one more time. Given our training data set we:</p>
<ol style="list-style-type: decimal">
<li>Select a number of trees to build (ntrees)</li>
<li>For i = 1 to ntrees we:
<ol style="list-style-type: lower-alpha">
<li>Generate a bootstrap sample of the original data.</li>
<li>Grow a regression tree to the bootstrapped data</li>
<li>For each split we:
<ol style="list-style-type: lower-roman">
<li>Select m variables at random from all p variables</li>
<li>Pick the best variable/split-point among the m</li>
<li>Split the node into two child nodes</li>
</ol></li>
<li>Use typical tree model stopping criteria to determine when a tree is complete (but do not prune)</li>
</ol></li>
<li>Repeat the process for each tree</li>
</ol>
<p>Let’s run a basic random forest model! The default number of trees used is 500 and the default m value is <code>features/3</code>.</p>
<pre class="r"><code>set.seed(123)

m1 &lt;- randomForest(
formula = Sale_Price ~ .,
data    = ames_train
)

m1</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 639516350
##                     % Var explained: 89.7</code></pre>
<p>If we plot m1 we can see the error rate as we average across more trees.</p>
<pre class="r"><code>plot(m1)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>The plotted error rate is based on the OOB sample error and can be accessed as follows:</p>
<pre class="r"><code>which.min(m1$mse)</code></pre>
<pre><code>## [1] 280</code></pre>
<pre class="r"><code>sqrt(m1$mse[which.min(m1$mse)])</code></pre>
<pre><code>## [1] 25135.88</code></pre>
</div>
<div id="tuning-1" class="section level4">
<h4><span class="header-section-number">3.2.2.2</span> Tuning</h4>
<p>Tuning random forest models are fairly easy since there are only a few tuning parameters. Most packages will have the following tuning parameters:</p>
<ol style="list-style-type: decimal">
<li><code>ntree</code>: Number of trees</li>
<li><code>mtry</code>: The number of variables to randomly sample at each split</li>
<li><code>sampsize</code>: The number of samples to train on. The default is 63.25% of the training set.</li>
<li><code>nodesize</code>: The minimum number of samples within the terminal nodes.</li>
<li><code>maxnodes</code>: Maximum number of terminal nodes.</li>
</ol>
<p>If we want to tune just <code>mtry</code>, we can use <code>randomForest::tuneRF</code>. <code>tuneRF</code> will start at a value of <code>mtry</code> that you input and increase the amount until the OOB error stops improving by an amount that you specify.</p>
<pre class="r"><code>features &lt;- setdiff(names(ames_train), &quot;Sale_Price&quot;)

set.seed(123)

m2 &lt;- tuneRF(
x          = ames_train[features],
y          = ames_train$Sale_Price,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = FALSE      # to not show real-time progress 
)</code></pre>
<pre><code>## -0.04236505 0.01 
## 0.0614441 0.01 
## 0.02425961 0.01 
## 0.06634214 0.01 
## 0.02149491 0.01 
## -0.02257957 0.01</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>In order to preform a larger search of optimal parameters, we will have to use the <code>ranger</code> function. This package is a c++ implementation of Brieman’s random forest algorithm. Here are the changes in speed between the two methods.</p>
<pre class="r"><code>system.time(
ames_randomForest &lt;- randomForest(
  formula = Sale_Price ~ ., 
  data    = ames_train, 
  ntree   = 500,
  mtry    = floor(length(features) / 3)
)
)</code></pre>
<pre><code>##    user  system elapsed 
##   50.73    0.01   50.75</code></pre>
<pre class="r"><code>system.time(
ames_ranger &lt;- ranger(
  formula   = Sale_Price ~ ., 
  data      = ames_train, 
  num.trees = 500,
  mtry      = floor(length(features) / 3)
)
)</code></pre>
<pre><code>##    user  system elapsed 
##    8.14    0.11    0.63</code></pre>
<p>Let’s create a grid of parameters with <code>ranger</code>!!!</p>
<pre class="r"><code>hyper_grid &lt;- expand.grid(
mtry       = seq(20, 30, by = 2),
node_size  = seq(3, 9, by = 2),
sampe_size = c(.55, .632, .70, .80),
OOB_RMSE   = 0
)

nrow(hyper_grid)</code></pre>
<pre><code>## [1] 96</code></pre>
<p>Now, we can loop through the grid. Make sure to set your seed so we consistently sample the same observations for each sample size and make it more clear the impact that each change makes.</p>
<pre class="r"><code>for(i in 1:nrow(hyper_grid)) {

model &lt;- ranger(
  formula         = Sale_Price ~ ., 
  data            = ames_train, 
  num.trees       = 500,
  mtry            = hyper_grid$mtry[i],
  min.node.size   = hyper_grid$node_size[i],
  sample.fraction = hyper_grid$sampe_size[i],
  seed            = 123
)

hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}

hyper_grid %&gt;% 
dplyr::arrange(OOB_RMSE) %&gt;%
head(10)</code></pre>
<pre><code>##    mtry node_size sampe_size OOB_RMSE
## 1    28         3        0.8 25477.32
## 2    28         5        0.8 25543.14
## 3    28         7        0.8 25689.05
## 4    28         9        0.8 25780.86
## 5    30         3        0.8 25818.27
## 6    24         3        0.8 25838.55
## 7    26         3        0.8 25839.71
## 8    20         3        0.8 25862.25
## 9    30         5        0.8 25884.35
## 10   24         5        0.8 25895.22</code></pre>
<p>Let’s run the best model we found multiple times to get a better understanding of the error rate.</p>
<pre class="r"><code>OOB_RMSE &lt;- vector(mode = &quot;numeric&quot;, length = 100)

for(i in seq_along(OOB_RMSE)) {

optimal_ranger &lt;- ranger(
  formula         = Sale_Price ~ ., 
  data            = ames_train, 
  num.trees       = 500,
  mtry            = 28,
  min.node.size   = 3,
  sample.fraction = .8,
  importance      = &#39;impurity&#39;
)

OOB_RMSE[i] &lt;- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>We set importance to <code>impurity</code> in this example, this means we can assess variable importance.</p>
<p>Variable importance is measured by recording the decrease in MSE each time a variable is used as a node split in a tree. The remaining error left in predictive accuracy after a node split is known as node impurity and a variable that reduces this impurity is considered more important than those variables that do not. Consequently, we accumulate the reduction in MSE for each variable across all the trees and the variable with the greatest accumulated impact is considered the more important, or impactful.</p>
<pre class="r"><code>plot(optimal_ranger$variable.importance)</code></pre>
<p><img src="RandomForests_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<pre class="r"><code>which.max(optimal_ranger$variable.importance)</code></pre>
<pre><code>## Overall_Qual 
##           17</code></pre>
<pre class="r"><code>which.min(optimal_ranger$variable.importance)</code></pre>
<pre><code>## Utilities 
##         9</code></pre>
<p>There are other ways we can run random forest models that take even less time than <code>ranger</code>. One such way is through the <code>h2o</code> package. This package is a java-based interface that provides parallel distributed algorithms. Let’s start up <code>h2o</code>!!</p>
<pre class="r"><code>h2o.no_progress()
h2o.init(max_mem_size = &quot;5g&quot;)</code></pre>
<pre><code>## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     C:\Users\KSHOEM~1\AppData\Local\Temp\RtmpYHCOda/h2o_kshoemaker_started_from_r.out
##     C:\Users\KSHOEM~1\AppData\Local\Temp\RtmpYHCOda/h2o_kshoemaker_started_from_r.err
## 
## 
## Starting H2O JVM and connecting:  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 917 milliseconds 
##     H2O cluster timezone:       America/Los_Angeles 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.26.0.2 
##     H2O cluster version age:    4 months !!! 
##     H2O cluster name:           H2O_started_from_R_kshoemaker_oor778 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   4.44 GB 
##     H2O cluster total cores:    16 
##     H2O cluster allowed cores:  16 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.6.1 (2019-07-05)</code></pre>
<pre><code>## Warning in h2o.clusterInfo(): 
## Your H2O cluster version is too old (4 months)!
## Please download and install the latest version from http://h2o.ai/download/</code></pre>
<p>Let’s try a comprehensive (full cartesian) grid search using <code>h2o</code>.</p>
<pre class="r"><code>y &lt;- &quot;Sale_Price&quot;
x &lt;- setdiff(names(ames_train), y)

train.h2o &lt;- as.h2o(ames_train)

hyper_grid.h2o &lt;- list(
ntrees      = seq(200, 500, by = 100),
mtries      = seq(20, 30, by = 2),
sample_rate = c(.55, .632, .70, .80)
)

grid &lt;- h2o.grid(
algorithm = &quot;randomForest&quot;,
grid_id = &quot;rf_grid&quot;,
x = x, 
y = y, 
training_frame = train.h2o,
hyper_params = hyper_grid.h2o,
search_criteria = list(strategy = &quot;Cartesian&quot;)
)

grid_perf &lt;- h2o.getGrid(
grid_id = &quot;rf_grid&quot;, 
sort_by = &quot;mse&quot;, 
decreasing = FALSE
)
print(grid_perf)</code></pre>
<p>This method obtains a model with a lower OOB RMSE, this is because some of the default settings regarding minimum node size, tree depth, etc. are more “generous” than ranger and randomForest (i.e. h2o has a default minimum node size of 1 whereas ranger and randomForest default settings are 5).</p>
<p>If we want to do this even faster, we can use the <code>RandomDiscrete</code> function. This will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran. While not able to find the best model, it finds a pretty good model.</p>
<pre class="r"><code>y &lt;- &quot;Sale_Price&quot;
x &lt;- setdiff(names(ames_train), y)

train.h2o &lt;- as.h2o(ames_train)

hyper_grid.h2o &lt;- list(
ntrees      = seq(200, 500, by = 150),
mtries      = seq(15, 35, by = 10),
max_depth   = seq(20, 40, by = 5),
min_rows    = seq(1, 5, by = 2),
nbins       = seq(10, 30, by = 5),
sample_rate = c(.55, .632, .75)
)

search_criteria &lt;- list(
strategy = &quot;RandomDiscrete&quot;,
stopping_metric = &quot;mse&quot;,
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 10*60
)

random_grid &lt;- h2o.grid(
algorithm = &quot;randomForest&quot;,
grid_id = &quot;rf_grid2&quot;,
x = x, 
y = y, 
training_frame = train.h2o,
hyper_params = hyper_grid.h2o,
search_criteria = search_criteria
)

grid_perf2 &lt;- h2o.getGrid(
grid_id = &quot;rf_grid2&quot;, 
sort_by = &quot;mse&quot;, 
decreasing = FALSE
)
print(grid_perf2)</code></pre>
<pre><code>## H2O Grid Details
## ================
## 
## Grid ID: rf_grid2 
## Used hyper parameters: 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  nbins 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 40 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   max_depth min_rows mtries nbins ntrees sample_rate         model_ids
## 1        40      1.0     35    25    350        0.75 rf_grid2_model_35
## 2        20      1.0     25    15    350        0.75 rf_grid2_model_10
## 3        35      1.0     25    30    350        0.75 rf_grid2_model_37
## 4        40      1.0     15    15    350        0.75 rf_grid2_model_30
## 5        20      1.0     35    10    500        0.75  rf_grid2_model_8
##                   mse
## 1 6.127801029473622E8
## 2 6.132440095542786E8
## 3 6.172964879155214E8
## 4 6.229006088981339E8
## 5 6.248965258676519E8
## 
## ---
##    max_depth min_rows mtries nbins ntrees sample_rate         model_ids
## 35        25      5.0     25    30    350       0.632 rf_grid2_model_12
## 36        30      5.0     25    10    200       0.632 rf_grid2_model_28
## 37        35      5.0     35    30    350        0.55 rf_grid2_model_22
## 38        40      5.0     15    15    350       0.632 rf_grid2_model_26
## 39        20      5.0     15    30    500        0.55 rf_grid2_model_29
## 40        30      1.0     35    30    350       0.632 rf_grid2_model_40
##                    mse
## 35 7.119339034273456E8
## 36 7.133293817672838E8
## 37 7.189580157453196E8
## 38 7.229833710160284E8
## 39 7.341871320679325E8
## 40 7.882606522955607E8</code></pre>
<p>Once we find our best model, we can run it with our test data and calculate RMSE!!</p>
<pre class="r"><code>best_model_id &lt;- grid_perf2@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)


ames_test.h2o &lt;- as.h2o(ames_test)
best_model_perf &lt;- h2o.performance(model = best_model, newdata = ames_test.h2o)


h2o.mse(best_model_perf) %&gt;% sqrt()</code></pre>
<pre><code>## [1] 24386.26</code></pre>
<p>Now we can predict values using each of the three techniques that we used!!!!</p>
<pre class="r"><code>pred_randomForest &lt;- predict(ames_randomForest, ames_test)
head(pred_randomForest)</code></pre>
<pre><code>##        1        2        3        4        5        6 
## 129459.5 185526.7 263271.6 196375.7 176455.2 392007.5</code></pre>
<pre class="r"><code>pred_ranger &lt;- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)</code></pre>
<pre><code>## [1] 129130.5 186123.7 269912.0 198751.7 176939.0 395345.4</code></pre>
<pre class="r"><code>pred_h2o &lt;- predict(best_model, ames_test.h2o)</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Utilities&#39; has levels not trained on: [NoSeWa]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Neighborhood&#39; has levels not trained on:
## [Landmark]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Condition_2&#39; has levels not trained on: [RRAe,
## RRAn]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Exterior_1st&#39; has levels not trained on:
## [ImStucc]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Bsmt_Qual&#39; has levels not trained on: [Poor]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Kitchen_Qual&#39; has levels not trained on: [Poor]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Functional&#39; has levels not trained on: [Sev]</code></pre>
<pre><code>## Warning in doTryCatch(return(expr), name, parentenv, handler): Test/
## Validation dataset column &#39;Garage_Qual&#39; has levels not trained on:
## [Excellent]</code></pre>
<pre class="r"><code>head(pred_h2o)</code></pre>
<pre><code>##    predict
## 1 128126.5
## 2 182538.8
## 3 263480.6
## 4 195616.4
## 5 176408.3
## 6 386656.8</code></pre>
</div>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-cart">
<p>Breiman, J. H.; Olshen, Leo; Friedman. 1984. <em>Classification and Regression Trees.</em> Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software.</p>
</div>
<div id="ref-global">
<p>Support, Global Software. 2018. “Random Forest Classifier – Machine Learning.” 2018. <a href="https://www.globalsoftwaresupport.com/random-forest-classifier/">https://www.globalsoftwaresupport.com/random-forest-classifier/</a>.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
