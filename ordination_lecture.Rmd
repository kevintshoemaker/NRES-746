---
title: "Ordination Student-Led Topic Lecture"
author: "Tessa Putz, Ryan McKim, & Alexander Selvey"
date: "20191209"
output:
    html_document:
      theme: cerulean
      toc: yes
      toc_float: yes
---
***

# Ordination Analyses

***

## Background

[Michael W. Palmer: Ordination Overview Webpage](http://ordination.okstate.edu/overview.htm)

### Definition
A statistical technique in which data from a large number of sites or populations are represented as points in a two- or three-dimentional coordiante frame

### History
- Ramensky 1930: Began using informal ordination techniques for vegetation
- Curtis and McIntosh 1951: Developed the 'continuum index' leading to conceptual links between species responses to gradients and mltivariate methods
- Goodall 1954: Introduced the term 'ordination' in an ecological context for Principal Coordinates Analysis (PCA)
- Bray and Curtis 1957: Developed polar ordination
- Austin 1968: Used canonical correlation to assess plant-environment relationships
- Hill 1973: Introduced correspondence analysis (a technique originating in the 1930's) to ecologists
- Fasham and Prentice 1977: Independently discovered and demonstrated the utility of Kruskal's (1964) nonmetric multidimensional scaling for community ecology
- Hill 1979: Corrected some of the flaws of Correspondence Analysis and thereby created Detrended Correspondence Analysis (the most widely used indirect gradient analysis technique)
- Gauch 1982: Wrote the first book, "Multivariate Analysis in Community Ecology", that descried ordination in non-technical terms, allowing ordination techniques to enter the mainstream
- Roberts 1986: Introduced Fuzzy set theory to ecologists
- Ter Braak 1986: Coupled Correspondence Analysis with regression methodoligies, also providing hypothesis testing, with Canonical Correspondence Analysis.
- Ter Braak and Prentice 1988: Developed a theoretical unification of ordination techniques

### Data Properties

Regardless of scale, most ordination datasets contain these general properties:

- They tend to be sparce, consisting of a large portion of zeros
- Most data points are infrequent and the majority of data is present in a minority of locations
- The number of factors that determine the overall data composition is potentially very large
- The number of important factors is typically few and can explain the majority of variation
- The datasets may contain a lot of noise, making replicate samples highly variable
- The datasets contain highly redundant information

### Coenospace

#### Cenoclines

A *cenocline* is a pictoral representation of all response functions combined along a single gradient. Cenoclines are further divided into cenoplanes (2 gradients) and cenospaces (>2 gradients).

#### Alpha, Beta, and Gamma Diversity

Ordination techinques can be used for almost any type of data, but are most commonly used in community ecology, where species abundance, composition, and diversity are of high interest.

- Alpha diversity is the diversity of a community
- Beta diversity is the rate of change in species composition from one community to another along gradients
- Gamma diversity is the diversiy of a region or a landscape

#### Ecological Similarity and Distance

Ecologically similar samples tend to share similar speies composition, whereas ecologically distant samples share few species. Some ordination techniques, such as NMDS, reqiure a measure of ecological distance. A few considerations when calculating a distance matrix:

- The distance matrix must be square and symmetric
- The diagnols are zero (i.e. no difference between the sample and itself)
- Some ecological insights can be derived from the matrix
- All information about a particular species is lost, meaning any analysis relying on the distance matrix alone will have limits to its interpretability

### Why use ordination?

From Gauch (1982): "Ordination primarily endeavors to represent sample and species relationships as faithfully as possible in a low-dimensional space." To highlight why this objective is desirable, below is a list of reasonable answers:

- It is impossible to visualize multiple dimenstions simultaneously
- A single multivariate analysis saves time as compared to several univariate analyses
- Ideally and typically, dimensions of this 'low dimensional space' will represent important and interpretable gradients
- If statistical tests are desired, problems of multiple comparisons are diminshed when compositions are studied in their entirety
- Statistical power is enhanced when individual groups are considered in aggregate because of redundancy
- By focusing on 'important dimensions' we avoid interpreting noise
- We can determine the relative importance of different gradients
- Community patterns may differ from population patterns
- Some techniques provide a measure of beta diversity
- The graphical results from most techniques often lead to ready and intuitive interpretations of relationships relevant to the dataset used

### List of ordination techniques

- Polar Ordination (PO)
- Principle Coordinates Analysis (PCoA)
- Nonmetric Multidimensional Scaling (NMDS)
- Principle Components Analysis (PCA)
- Correspondence Analysis (CA)
- Detrended Correspondence Analysis (DCA)
- Direct Gradient Analysis (DGA)
- Canonical Correspondence Analysis (CCA)
- Exploratory Factor Analysis (EFA)
- There may be more that I haven't heard of or found on the interweb!

***

## Ordination Techniques Covered in this Demonstration

- PCA (with an aside on PCoA)
- EFA
- NMDS

***

### Principle Components Analysis (PCA)

[DataCamp PCA Tutorial](http://datacamp.com/community/tutorials/pca-analysis-r)

#### Introduction

PCA or Principle Components Analysis is an eigenvector/eigenvalue based analysis that represents multiple dimensions of data as the linear combination of all of the variables. This technique takes a large number of variable and represents them in the smallest number of what are know as *principle components*. Essentially, PCA puts the most significant variance from the linear transformation on the first PC, with each subsequent component orthogonal to the last and having a lesser variance. PCA's work best with numerical data and all categorical data should be avoided if possible.

##### Eigenvalues and Eigenvectors

An eigenvector is a non-zero vector of a linear transformation of the data across all the variables. Eigenvectors in PCA give us a sense of the direction of the variance with respect to the linear combination of our varibles. The eigenvalue is the factor by which an eigenvector is scaled and in terms of PCA, quantifies how large the variance is.

Let's look at a simple dataset like this:
 <center>
![**Figure 1.** Data](https://i.stack.imgur.com/jPw90.png)

An eigenvector would be determined by the slope that accounts for the maximum variance with minimal error (those points when the distance between the data and the eigenvector are smallest), which just so happens to occur at the same time:

 <center>
![**Figure 2.** Eigenvector](https://i.stack.imgur.com/Q7HIP.gif)
![**Figure 3.** Three dimensional data](https://steemitimages.com/DQmao3qfNkqSsNp2mN1RVtipVMPVPb3DwfWXpFJPXQT99R3/image.png)

This works through multiple dimensions (as many as you have variables), though it is very difficult to convey this information in anything over three dimensions.  In ecology, we often collect data in far greater than three dimensions-- in these cases, dimensional reduction isn't just about making the data more intuitive, but also works in identifying which variables could be removed, which is important when you can be operating with dozens or even hundreds.

##### So, what ARE principal components

Principal components are a combination of all of the variables resulting in the eigenvectors that account for some proportional amount of the variance in the data, with the first component accounting for the largest proportion of variance, the second accounting for the second most, etc.  The principal components themselves are each affected to some degree by all of the variables being examined.  The correlation between the original variable and the principal component is called its *loading.*

#### Example Using `mtcars`

Running a PCA utilizing base R is quite simple. First we will take a look at the `mtcars` dataset and then run our PCA using `prcomp`.

```{r}
head(mtcars)
```

Here, we can see that we have a good lot of numerical data to work with, but a few categorical variables; most notable `vs` and `am`. In order to smooth out our PCA, we will avoid these varaibles, but pass all others to our `prcomp` command.

```{r}
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)],center=TRUE,scale.=TRUE)
summary(mtcars.pca)
str(mtcars.pca)
```

WOW! PCA completed! Super quick and easy. Let's first interpret the results using a scree plot!

```{r}
x.var <- mtcars.pca$sdev ^ 2
x.pvar <- x.var/sum(x.var)
plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
```

This is just another way of visualizing which principal component accounts for how much variance in the data (same information in the summary).  In this case, PC1 accounts for 63% of the variance, followed by PC2 that accounts for 23%.

But what is the loading on PC1?  We can determine that by just looking at mtcars.pca directly.
```{r}
mtcars.pca
```
Unfortunately there is a lot of multicolinearity going on here.  For interpretive sake, we will only identify the strongest individual factor correlating to PC1, cyl at 0.4025537, and the lowest is gear at -0.2094749.

While these results can be looked at numerically, they can also be very easily and rapidly interpretted in a figure.

Let's plot out our results using `ggbiplot`.

```{r eval=FALSE}
library(devtools)
install_github('vqv/ggbiplot')
```

After installing, plot!

```{r}
library(ggbiplot)
ggbiplot(mtcars.pca)
```

OOOOOOHHHHHHH! AAAAAAAHHHHHHHH! Looking very nice! So, as we knew earlier from our `summary`, PC1 explains almost 63% of the variance, while PC2 explains a little over 23%. We can now see the contributions of the variables to each PC axis as well by observing the red arrows radiating from the center point. However, what cars are contributing to the variance across PC1 and PC2? Let's add some labels to better understand what cars each black point represents, furthering our understanding of the contributions of each individaul vehicle to the overall variance across our PC axes.

```{r}
ggbiplot(mtcars.pca,labels=rownames(mtcars))
```

Ahhh, much better! Now we can visualize the contributions of each individual vehicle to the variance across the PC axes. What if we want to take this further? The next logical step is to create groupings to better understand which cars are most similar. Lets start by grouping cars by country of origin using the `ellipse` option of `ggbiplot`.

```{r}
mtcars.country <- c(rep("Japan",3),rep("US",4),rep("Europe",7),rep("US",3),"Europe",rep("Japan",3),rep("US",4),rep("Europe",3),"US",rep("Europe",3))
ggbiplot(mtcars.pca,ellipse=TRUE,labels=rownames(mtcars),groups=mtcars.country)
```

Voila! Now, what can we deduce from these new groupings? 

Question #1: What does this tell us about the association of the variables with a particular country of origin? Can we start to deduce some patterns and even start making predictions? If you wanted to build a model for classification of vehicles based on this data, which variables would be the most useful?

Lets not forget that we have several different PC axes (9 to be exact), all of which may provide a different look at each of the variables and have differing amounts of explained variance. Lets use the `choices` option in `ggbiplot` to map our results across PC3 and PC4.

```{r}
ggbiplot(mtcars.pca,choices=c(3,4),ellipse=TRUE,labels=rownames(mtcars),groups=mtcars.country)
```

Hmmmmmm... Not too informative, but considering that these PC axes explain about 6% and 3%, respectively, this isn't too surprising. What *would* be unusual is if these axes allowed us to discern indivdual groupings or apparent patterns. Now that we have a handle on this data, lets add another data point into the mix. (NOTE: The actual tutorial goes more into ggbiplot settings, but for brevity, we will skip this part and allow you to view it on your own time if desired.)

Let's say Elon develops a new **Tesla MarsRover** for intergalatic travel! Sweet baby Einstein! We definitely want to include that to our classification system, so lets add it to our dataset, run our PCA again, and plot the results!

```{r}
TeslaMarsCar <- c(1000,60,50,500,0,0.5,2.5,0,1,0,0)

mtcarsplus <- rbind(mtcars, TeslaMarsCar)
mtcars.countryplus <- c(mtcars.country, "Mars")

mtcarsplus.pca <- prcomp(mtcarsplus[,c(1:7,10,11)], center = TRUE,scale. = TRUE)
summary(mtcarsplus.pca)

ggbiplot(mtcarsplus.pca, obs.scale = 1, var.scale = 1, ellipse = TRUE, circle = FALSE, var.axes=TRUE, labels=c(rownames(mtcars), "TeslaMarsCar"), groups=mtcars.countryplus)+
  scale_colour_manual(name="Origin", values= c("forest green", "red", "violet", "dark blue"))+
  ggtitle("PCA of mtcars dataset, with extra sample added")+
  theme_minimal()+
  theme(legend.position = "bottom")
```

So, our explained variance for PC1 has now dropped, our expalined varaince for PC2 has increased, and the shape is entirely different! Adding this new sample has really thrown a wrench in our analysis and we cannot make any conclusions about the similarity of this new sample to our original data. Let's project this new sample onto our old analysis and see if we can come to something more logical!

```{r}
tmc.sc <- scale(t(TeslaMarsCar[c(1:7,10,11)]), center= mtcars.pca$center)
tmc.pred <- tmc.sc %*% mtcars.pca$rotation

mtcars.plusproj.pca <- mtcars.pca
mtcars.plusproj.pca$x <- rbind(mtcars.plusproj.pca$x, tmc.pred)

ggbiplot(mtcars.plusproj.pca, obs.scale = 1, var.scale = 1, ellipse = TRUE, circle = FALSE, var.axes=TRUE, labels=c(rownames(mtcars), "TeslaMarsCar"), groups=mtcars.countryplus)+
  scale_colour_manual(name="Origin", values= c("forest green", "red3", "violet", "dark blue"))+
  ggtitle("PCA of mtcars dataset, with extra sample projected")+
  theme_minimal()+
  theme(legend.position = "bottom")
```

So, what the heck did we do here? Essentially, instead of including the sample in our calculation of the sample means, thereby skewing the results, we defined the PC axes without relation to `TeslaMarsCar` then applied the transformations that our PCA made to our `TeslaMarsCar` sample. All we really did was scale the values for `TeslaMarsCar` in relation to the PCA's center (`mtcars.pca$center`), applied the PCA rotation to `TeslaMarsCar`, and bound the predicted values for `TeslaMarsCar` to the rest of the `pca$x` matrix.

Question #2: Does this really tell us much about or more about `TeslaMarsCar` than did including the values in the PCA calculation? What do the two methods tell us about `TeslaMarsCar` individually and does this help us classify it in any meaningful way?

Quick shoutout to the lesser used, but never forgotten Principle Coordinates Analysis (PCoA). PCoA *is not* an eigen-based ordination method, rather it is a distance based method more akin to NMDS. (Although these two vary greatly in the foundation of their analyses and the difference will be discussed in the NMDS section later.) However, if the distance metric is Euclidean, PCA and PCoA methods become essentially identical. What both of these techniques suffer from is known as the *arch* or *horseshoe effect* (if beta diversity is high) because they are linear methods and often times the PCA/PCoA solution is distorted into a horseshoe shape, resulting in difficulties interpretating the results. This makes these analyses unsuitable for most ecological datasets (Gauch 1982).

For more on PCA, visit [Victor Powell's](http://setosa.io/ev/principal-component-analysis), PCA: Explained Visually, webpage!

This wraps up the PCA portion of the Ordination lecture. Next up, Factor Analysis!

***

### Exploratory Factor Analysis (EFA)

- [UCLA EFA Webpage](https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/)
- [EFA Tutorial Webpage](https://www.promptcloud.com/blog/exploratory-factor-analysis-in-r/)

#### Introduction

Exploratory Factor Analysis, EFA, is a statistical method (much like PCA) that aims to deduce the relational structure among a set of variables and narrow down it down to a smaller number of variables. There are two ways to extract factors: the first is PCA, which we have covered, and the second is *common factor analysis*, which will be the focus of this portion of the lecture. Utilizing both methods, we can reduce the dimensionality of our dataset to fewer unobserved variables. 

The difference is that PCA assumes the common variance takes up all of the total variance, whereas factor analysis divides the variance into common and unique. The latent variable that makes up the common variance is called a **factor**. The two also differ in the overall goal of the analysis: PCA simply reduces the dimensionality down to a linear combination of smaller components, whereas factor analysis is used more to deduce a latent construct that defines the interrelationship among variables.

 <center>
![**Figure 1.** EFA Overview Flowchart](https://22570l2e793j2oo9c81ug2nh-wpengine.netdna-ssl.com/wp-content/uploads/2017/02/Exploratory-Factor-Analysis.png)
</center>

#### Example using EFA.csv dataset

The dataset has been provided and is called, EFA.csv. Here, we have 90 responses for 14 different variables that customers consider when buying a car. The survey used a point scale, 1 being the lowest and 5 being the highest, to rank these variables by importance: Price, Safety, Exterior looks, Space and comfort, Technology, After sales service, Resale value, Fuel type, Fuel efficiency, Color, Maintenance, Test drive, Product reviews, and Testimonials.

First, lets read-in our data, view it, and install some packages (or load them if you already have them installed) that we will need for the analysis!

```{r}
fa.data <- read.csv('EFA.csv',header=TRUE)
head(fa.data)

library(psych)
#install.packages('psych')
library(GPArotation)
#install.packages('GPArotation')
```

Now, let's find the number of factors we will use for our factor analysis. We will do this by using the `psych` package, called `fa.parallel`, to execute a parallel analysis. We will specify the data frame and factor method (`minres`), then generate a scree plot to see how many factors we should use.

```{r}
parallel <- fa.parallel(fa.data, fm='minres',fa='fa')
```

SHA-BAM!! Factor recommendation and scree plot a la mode! Alright, looking at this scree plot, we can see that we should use anywhere between 2 and 5 factors, for our purposes we will start with 4. Here, we are looking at the drops in the actual data and finding the point at which it levels off to the right. Then, we locate the point of inflection, or the point at which the gap between simulated and actual data is at a minimum.

Next, we will choose our number of factors and run our factor analysis using the `fa` function in the `psych` package. Factor loadings are the relationship of each variable to the factor and are interpreted similar to standard regression coefficients. What we are looking for is loadings more than 0.35 on all variables, without loading onto more than one factor. Here, we also select oblique rotation `rotate='oblimin'` because we have reason to believe there is correlation in the factors and 'Ordinary Least Squared/Minres' factoring (`fm='minres'`), which will provide us with results that are similar to a 'Maximum Likelihood'. Then we will check out the factor mapping using the `fa.diagram` command.

```{r}
fourfactor <- fa(fa.data,nfactors=4,rotate='oblimin',fm='minres')
print(fourfactor$loadings,cutoff=0.35)
fa.diagram(fourfactor)
```

There we go, looking better all the time! There is some debate on the cutoff value to use, some say .2, others say .3 or .4, and still others say .33 is an acceptable cutoff. Here, we used 0.35 as it gave us 1) our single loadings and 2) didn't drop the significance of any of our variables. Now that we have achieved what is known as **simple structure**, we need to validate our model. 

```{r}
print(fourfactor)
```

So, how'd we do?

Question #3: What metrics would you consider most important? Based on the above analysis, does our model provide a good fit for this factor analysis? Why?

Well folks, that wraps up EFA for now, but for more fun visit the [EFA Psych package bfi analysis](http://r-bloggers.com/exploratory-factor-analysis-in-r/) to hone your skills in EFA utilizing the bfi dataset in the `psych` package!

***

### Nonmetric Multidimensional Scaling (NMDS)

[Jon Lefcheck NMDS Tutorial](https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/)

#### Introduction

First off, what is NMDS? Well, NMDS is a distance based ordination technique that relies on *rank orders*, rather than euclidean distances, to collapse multidimensional datasets into fewer dimenstions in order to be visualized and interpreted. A brief outline of the algorithm is as follows:

1) The user selects the number of dimensions (N) for the solution and choses an appropriate distance metric
2) The distance matrix is calculated
3) An initial configuration of samples in N dimensions is selected, which can be random or derived from another ordination method
4) A measure of 'stress' (mismatch between the rank order of distances in the data and the rank order distances in the ordination) is calculated
5) The samples are moved slightly in a direction that decreases the stress
6) 4 and 5 are repeated until 'stress' appears to reach a minimum; the final configuration of points may be rotated if desired

#### NMDS Example in `vegan`

First, lets begin to conceptualize the reasons for using NMDS. Let's begin with a single axis that represents the abundance of a single species, along which communities can be plotted.

```{r}
plot(0:10,0:10,type="n",axes=F,xlab="Abundance of Species 1",ylab="") 
axis(1)
points(5,0); text(5.5,0.5,labels="community A")
points(3,0); text(3.2,0.5,labels="community B")
points(0,0); text(0.8,0.5,labels="community C")
```

Let's add another species and consider its abundance on a separate axis.

```{r}
plot(0:10,0:10,type="n",xlab="Abundance of Species 1",
     ylab="Abundance of Species 2")
points(5,5); text(5,4.5,labels="community A")
points(3,3); text(3,3.5,labels="community B")
points(0,5); text(0.8,5.5,labels="community C")
```

Hey, what do you know, a third species has suddenly entered into the relm of possibility! Let's add it to our plot on a third axis!

```{r, message=FALSE, warning=FALSE}
# install.packages("scatterplot3d")
library(scatterplot3d)
d=scatterplot3d(0:10,0:10,0:10,type="n",xlab="Abundance of Species 1", ylab="Abundance of Species 2",zlab="Abundance of Species 3"); 
d$points3d(5,5,0); text(d$xyz.convert(5,5,0.5),labels="community A")
d$points3d(3,3,3); text(d$xyz.convert(3,3,3.5),labels="community B")
d$points3d(0,5,5); text(d$xyz.convert(0,5,5.5),labels="community C")
```

Hopefully we can all see where this is heading. If we would like to come to a reasonable solution, we cannot continue to add more and more dimensionality to our problem. Physicist's run into the problem of multidimensionality around 4. Compared to ecologists, they have it pretty easy when it comes to handling multidimensionality if you ask me!

Well, enough of doing it the hard way, let's utilize NMDS to make our analysis (and life) a little bit easier! In order to do this, we will need the `vegan` (shoutout to my plant based homies out there! whoop whoop!) and the `metaMDS` function.

```{r}
#install.packages("vegan")
library(vegan)
set.seed(2)
community_matrix <- matrix(
   sample(1:100,300,replace=T),nrow=10,
   dimnames=list(paste("community",1:10,sep=""),paste("sp",1:30,sep="")))

example_NMDS <- metaMDS(community_matrix,k=2)
```

If you are having problems with convergence, utilize the `trymax` option and if you have high stress (try taking a break or drinking some chamomile tea) try setting `k=3`.

```{r}
example_NMDS <- metaMDS(community_matrix,k=3,trymax=100)
```

Whoo! We need a Staples Button because THAT WAS EASY! Alright, lets check out the Shepard plot showing the scatter around the regression between the interpoint distances in the final configuration (or simply the distance between each pair of communities) agains their original dissimilarities.

```{r}
stressplot(example_NMDS)
```

Looking at this, we can see that we don't have a ton of scatter around our line, which is good news for us! A large amount of scatter indicates that the original dissimilarities were not well preserved in the reduced number of dimensions. Now, let's plot it out!

```{r}
plot(example_NMDS)
```

Well, this is useless. Let's turn this into something more intuitive using the functions `ordiplot` and `orditorp`.

```{r}
ordiplot(example_NMDS,type="n")
orditorp(example_NMDS,display="species",col="red",air=0.01)
orditorp(example_NMDS,display="sites",cex=1.25,air=0.01)
```

NICE! Now we have a picture of where each community and species lie in ordination space! Let's take this even further and add some groupings to our plot by simulating two treatments and see how our communities and species cluster based on treatment.

```{r}
treat <- c(rep("Treatment1",5),rep("Treatment2",5))
ordiplot(example_NMDS,type="n")
ordiellipse(example_NMDS,groups=treat,draw="polygon",col="grey90",label=F)
orditorp(example_NMDS,display="species",col="red",air=0.01)
orditorp(example_NMDS,display="sites",col=c(rep("green",5),rep("blue",5)),
   air=0.01,cex=1.25)
```

We can color the ellipses by treatment as well.

```{r}
colors=c(rep("red",5),rep("blue",5))
ordiplot(example_NMDS,type="n")

for(i in unique(treat)) {
  ordiellipse(example_NMDS$point[grep(i,treat),],draw="polygon",
   groups=treat[treat==i],col=colors[grep(i,treat)],label=F) } 
orditorp(example_NMDS,display="species",col="red",air=0.01)
orditorp(example_NMDS,display="sites",col=c(rep("green",5),
  rep("blue",5)),air=0.01,cex=1.25)
```

We can even make a minimum spanning tree (MST) using `ordicluster`, connecting similar communities, which is useful to see if treatments are effectively controlling community structure.

```{r}
ordiplot(example_NMDS,type="n")
orditorp(example_NMDS,display="species",col="red",air=0.01)
orditorp(example_NMDS,display="sites",col=c(rep("green",5),rep("blue",5)),
         air=0.01,cex=1.25)
ordicluster(example_NMDS,hclust(vegdist(community_matrix,"bray"))) 
```

Finally, if you have a continuous treatment, such as an environmental gradient or temperature, it my be of interest to you to plot contour lines instead of convex hulls, ellipses, or dendograms. We can do this using `ordisurf`.

```{r}
# Define random elevations for previous example
elevation=runif(10,0.5,1.5)
# Use the function ordisurf to plot contour lines
ordisurf(example_NMDS,elevation,main="",col="forestgreen")
# Finally, display species on plot
orditorp(example_NMDS,display="species",col="grey30",air=0.1,
   cex=1)
```

Question #4: Utilizing NMDS, generate your own set of data with 1000 data points with 20 communities composed of 50 different species and produce a contour graph like the one above. Also, generate your own set of communities and species with treatments and produce a MST graph. What information can be deuced from these types of plots?

That's all for NMDS folks!

***

## Outro

To wrap up, using ordination techniques we can collapse highly multidimensional datasets to generate informative figures that generate intuitive conclusions from extremely messy data. These tools are very powerful for generating hypotheses about interactions, the influence particular variables have on the data, and can even give you statistical measurements. Utilize these tools to choose variables for statistical analysis, divide variables into factors, or generate plots that give information about the organization of the data. There are even more ordination techniques out there, these are just a few of the most common! (Well, the only ones we really knew anything about, at least!)

## Questions???

![](data:image/jpeg;base64,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)

## References

Hayden, L. (2018, August 09). *Principal Component Analysis in R*. Retrieved from <https://www.datacamp.com/community/tutorials/pca-analysis-r>

Lefcheck, J. (2012, October 24). *NMDS TUTORIAL IN R*. Retreived from <https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/>

Palmer, M. W. *Ordination Methods - an overview*. Retrieved from <http://ordination.okstate.edu/overview.htm>

Panda, P. (2017, February 15). *Exploratory Factor Analysis in R*. Retreived from <https://www.promptcloud.com/blog/exploratory-factor-analysis-in-r/>

UCLA: Statistical Consulting Group. (2019). *A PRACTICAL INTRODUCTION TO FACTOR ANALYSIS: EXPLORATORY FACTOR ANALYSIS*. Retreived from <https://stats.idre.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/>
