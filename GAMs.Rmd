---
title: "Generalized Additive Models"
author: "Israel Borokini"
date: "November 20, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Introduction
Regression analysis is used to investigate the relationships between and among response and predictor variables. Once a reliable relationship has been established among the variables, the analysis can be used to estimate how changes in an independent variable affect the response variable.

Regression models have three components:
1.	Response, Y [also called regressand, criterion, or endogenous dependent variables]
2.	Predictors, X [also called regressors, independent, exogenous or explanatory variables]
3.	Unknown parameters, ??
The typical relationship among the three is:
Y ~ f(X, ??) or E(Y|X) = f(X, ??)
While function f, must be specified.

Regression models are used for describing the correlation and directionality of the variables (parametric, semi-parametric or non-parametric), making inference on the relationship between predictors and response variables, evaluating the relative strength (or influence) of each predictor on the response, and predicting the future effect of changes in response variables in relation to changes in predictor variables. Hence, a good model should perform three functions: description, inference and prediction.

## Linear regression
There are many extensions of linear regression:
Linear regression is used to describe the relationship between dependent variables and one or many independent variables. 
If only one dependent variable (interval or ratio) is used with one independent variable, it is called simple linear regression, given as:
Y = ?? + ??*X + ??

If more than one independent variable is used, but for only one dependent variable (interval or ratio), then it is called multiple linear regression, given as:
Y = ?? + ??_1*X_1 + ??_2*X_2 + . + ??_n*X_n + ??
Where ?? ~ N (0,??^2)
?? and ?? are intercept and slope respectively, to be determined at CI = 95% using ordinary least squares (OLS) method;
?? - residual error, to cover unexplained information, assumed to be normally distributed with mean of 0 and ??^2; 
N is the sample size
However, multiple linear regression [also called multivariable linear models] is different from multivariate linear regression [also called general linear models], which involves prediction of two or more correlated dependent variables using the same set of independent variables.

Logistic regression is used when one binary (or categorical) dependent variable is used with two or more independent variables; while ordinal regression is used when the dependent variable is ordinal data. 
Multinominal regression or discriminant analysis are used for only one dependent nominal variable. Probit model is a special case of logistic regression where dependent variable has only two values, for example, present or absent.

There are several other extensions of the linear models including generalized linear models (GLMs), heteroscedastic models or hierarchical linear models, etc. some of which are discussed later.

## Assumptions of linear models (parametric models)
1.	Linearity: predictors and response variables must have linear relationship; hence linear regressions are sensitive to outliers and data inaccuracy. This can be tested with scatterplots
2.	Normality: all variables to be used must be normally distributed. This can be checked using histogram, Q-Q plot, partial residual plots, Shapiro-Wilk test (for parametric regression), or Anderson-Darling test.
3.	Little or no multicollinearity: multicollinearity means there is strong correlations among predictors, indicated as higher r values closer to 1.0. Singularity is when r = 1.0. It increased dimensionality of models while adding no new information, increases noise and leads to bias predictions. Multicollinearity can be detected using:
a.	Correlation matrix: (correlation values >1 indicates multicollinearity)
b.	Tolerance measures: T = 1 - R^2 (T < 0.1 indicates multicollinearity)
c.	Variance inflation factor: VIF = 1/T (VIF >100 indicates multicollinearity)
d.	Condition index (values ???10 indicates multicollinearity)
Depending on the studied system, some of the correlated variables should be removed from the model.
4.	Autocorrelation: This is statistical dependence among residuals, given as: y(x+1) = y(x). This can be detected using scatter plots, or running Durbin-Watson's d test, in which d values > 2.5 indicates autocorrelation.
5.	Homoscedasticity: if the residual plots have the same width for all values of the response variable, then the data is homoscedastic. In other words, variance should be constant for the errors of all response variable data points, regardless of the predictors. See Figure 1. This can be detected using scatter plots or Goldfeld-Quandt test.
6.	The error is assumed to be normally distributed with mean of 0 and ??^2 for predictors.
7.	The general rule of thumb is that there should be 20 dependent values to 1 independent value.
Model transformations (a little digression here.)
Transformations are used if the data is non-normal. See Figure 2 for types of transformations. Square root transformation is used when the moderately and positively skewed. Substantial non-normal positively skewed data is corrected by log transformation. Inverse transformation is used to correct severe non-normal positively skewed data. Data reflection should be done to negatively skewed data before transformation.
In case of heteroscedastic data, unknown parameters should be estimated using general least squares (GLS) or iteratively weighted least squares (IRLS). 

## Types of regression models
1.	Parametric model: These models adhere strictly to the linear model assumptions described earlier, such that average change in response variables is proportional to change in predictors. All linear regression models and their extensions are examples of parametric models.
2.	Non-parametric model: this type of models assumes no relationship among predictors and response variables. In order words, plots show a curved relationship between variables. Non-parametric models include kernel regression and non-parametric multiplicative regression. This becomes the last resort when transformation does not ensure that data meets assumptions of linear regression. Unknown parameters are estimated using nonlinear least squares. 
3.	Semi-parametric model: This is a case where the shape of relationship among variables are determined by the data, hence the relationship is not restricted to any shape. Examples of this include additive models, generalized additive models.

## Generalized linear models
GLM is a generalization of linear regression to allow for modelling of response variables that have non-normal error distribution. This is operated by relating the predictor variables to the mean of response variable, via a link function g. In this way, different forms of linear regression are unified into one model. The developers, Drs. John Nelder and Robert Wedderburn, recommended using maximum likelihood estimation for the unknown parameters. Kindly note that there are many types of link function, depending on the distribution family, but the predictor variables must be linear.

Do not confused generalized linear model with general linear model. While both models use least square method to estimate unknown parameters, residual distribution in general linear model is assumed to be Gaussian, but in GLM, residual distribution can belong to any exponential distribution family, e.g. poisson, logistic, linear, binomial, negative binomial or gamma. In general linear model, Y must be continuous data, while in GLM, it can be categorical, continuous, interval, etc. In general linear model, residual distribution must be linear, but GLM accommodates non-linear residual distribution, as in binomial response. Also, in general linear model, OLS is not relaxed as errors are fit to linear distribution, but in GLM, OLS is relaxed and errors can be modeled to fit the model.

# Additive models
This was developed by Dr. Stone in 1985, to estimate additive approximation to multivariate regression function. Therefore, it helps reduce the effect of dimensionality in the model by using univariate smoother. Relationships among variables are explained by individual terms estimates.

# Generalized additive models
GAM is a semi-parametric extension of GLM, which makes assumptions that link functions are additive and components are smooth. Statistically, the main difference between GLM and GAM is that in GAM, linear predictor is not forced to be linear, but is the sum of smoothing functions. GAM can handle non-linear, linear and non-monotonic relationships between response and predictor variables. GAM was developed by Drs. Hastie and Tibshirani, who were students of the professors who developed GLM.
Essentially, shape of predictor functions is determined by the data, therefore no assumption is made on the specific link function for error distribution.
GAM is given as: Y = ?? + f(X) + ??
Where ??*X is replaced by a smoothing curve f(X) which is derived from the model.
In the classical GAM, the response variable is scalar, but Yee and Wild (1996) developed vector smoothing for model classes including multiple logistic regression for nominal response variables, bivariate probit, bivariate logistic, and continuation ratio model, etc. This extension of GAM is called vector generalized additive model. (VGAM).

## How GAM works
The predictors are first separated into knot, k (sections), low order polynomials or spline functions are used to fit the data into each section independently. Then, functions of all k are added to predict the link function (smoothing), which is why it is called additive model. Smoothing of knots is done using "loess" and "splines", depending on R package used.  Model fitting is based on likelihood methods (e.g. AIC scores).

## Uniqueness of GAM
1.	The non-parametric (unspecified) function f of the predictor variables x
2.	GAM is flexible and provide excellent fit for both linear and nonlinear relationships (multiple link functions)
3.	Just like GLM, GAM can be applied to any exponential distribution family
4.	Regularization of predictor functions helps to avoid over-fitting.
Application and advantages of GAM
1.	Very powerful for prediction and interpolation
2.	Highly used in SDMs and ENMs (Elith et al. 2006)
3.	Analogous to hinge feature of maxent algorithm (Phillips et al. 2006)
4.	Building optimization models
5.	Comparatively GAMs shows lower AIC scores and explained higher deviance than GLMs
6.	Applied in Genetics, epidemiology, molecular biology, air quality and medicine (Dominici et al. 2002).

## R packages that implement GAM
1. gdxrrw (can read or write GDX files)
2. mgcv - requires package "nlme"
3. gam (original GAM package developed by Hastie and Tibshirani) - requires "splines" package
4. mda - "bruto" function
5. gamstools

## Example
Example and exercise are given in the R codes provided below

```{r}
library(mgcv)
library(nlme)

data("airquality")

#setwd("C:\\Users\\tbisrael\\Documents\\Academic Classes\\Fall 2016\\NRES 746\\GAMs")
ozone.data<-  airquality  #read.csv("ozone.csv", header = T)
head(ozone.data) # ozone is the response variable while the others are predictors

ozone.data$rad <- ozone.data$Solar.R
ozone.data$temp <- ozone.data$Temp
ozone.data$wind <- ozone.data$Wind
ozone.data$ozone <- ozone.data$Ozone

ozone.data <- ozone.data[,c("ozone","rad","wind","temp")]

pairs(ozone.data)
# we can see that the relationships among the variables are not linear

plot(ozone.data$ozone)
# and the response variable does not obey linear regression assumption
```

## Let's first play around linear regression

```{r}

model1<-lm(ozone~rad+temp+wind, data = ozone.data)
summary(model1)
summary.aov(model1)
opar <- par(mfrow = c(2, 2))
plot(model1)  
# the results suggests non-normality of residuals.

model2<-glm(ozone~rad+temp+wind, data = ozone.data)
summary(model2)
opar <- par(mfrow = c(2, 2))
plot(model2)  
# the results suggests non-normality of residuals.

## Both LM and GLM analyses show that the predictors are significantly different, with AIC = 998.63, in spite of the fact that the predictors do not obey linear regression assumptions. This can lead to serious misinterpretation and prediction of relationships.
```

## Let's go to GAMs

```{r}
model3<-gam(ozone ~ s(rad)+s(temp)+s(wind), data = ozone.data)
summary(model3)
anova.gam(model3)  # kindly note: this gives the same results as in the summary() function
AIC(model3)
## gave lower AIC of 962.298 compared to that of GLM
opar <- par(mfrow = c(2, 2))
plot(model3,residuals = TRUE)  # plots model output, showing partial residuals
plot(model3,seWithMean = TRUE) # plots model output, with intercept confidence intervals
plot(model3,scheme = 1,unconditional = TRUE)  # another plotting method. Note that "scheme=2" is the default shown in previous plots

gam.check(model3)  # checking the smoothing basis dimensions by running some diagnostic checks
# K' was observed to be 9.0 and K' = K - 1. So, even though we didn't set K, the default k used by the algorithm was 10

# the default method for estimating smoothness is GCV, which is a cross-validation method. Let's change it to a likelihood method using REML

model3a<-gam(ozone ~ s(rad)+s(temp)+s(wind), data = ozone.data, method = "REML")
summary(model3a)
```

## let's add interaction terms, as we would expect in natural systems

```{r}
wt<-ozone.data$wind*ozone.data$temp
model4<-gam(ozone ~ s(rad)+s(temp)+s(wind)+s(wt), data = ozone.data)
summary(model4)
anova.gam(model4)
AIC(model4) # gave AIC value of 927.247

plot(model4,residuals = TRUE)  # plots model output, showing partial residuals
# the results showed "rad" predictor to be less significantly different under non-linear assumptions, and the plot also confirms it.
plot(model4,seWithMean = TRUE) # plots model output, with intercept confidence intervals

gam.check(model4)
# the results from this diagnostic test shows that the default k is too low.

# compare between the two models
anova(model3,model4, test = "F")
# results showed significant difference between the two models, which could be due to the interaction term added to "model 4" 
AIC(model3,model4) # could produce AIC for all the models at the same time. Clearly, the interaction produced better results for model selection

```
## Now, we set smoothing parameter k for both additive and interaction models (models 3 and 4 respectively).
## Recall, that the default k was 10 and the gam.check() indicates that k was too low, so we specify k and check the results

```{r}
model3b<-gam(ozone ~ s(rad, k=20)+s(temp, k=20)+s(wind, k=20), data = ozone.data)
summary(model3b)
gam.check(model3b)
AIC(model3b)
## Based on AIC score, k = 20 didn't improve the results

model3c<-gam(ozone ~ s(rad, k=25)+s(temp, k=25)+s(wind, k=25), data = ozone.data)
summary(model3c)
gam.check(model3c)
AIC(model3c)  ## a significant reduction in AIC value
plot(model3c) ## there is terrible overfitting of "wind" predictor, also, the edf value is too close to k', so we try again by reducing k for wind

model3d<-gam(ozone ~ s(rad, k=25)+s(temp, k=25)+s(wind, k=10), data = ozone.data)
summary(model3d)
gam.check(model3d)
AIC(model3d) ## AIC values shot up again

AIC(model3, model3a, model3b, model3c, model3d) # compare AIC values for all fitted models
```
# Exercise
Can you fit a GAM with best AIC value, highest deviance explained and less overfitting of the predictors? Hint: use different k values and try different additive and interaction terms (backward model selection). After trying different models, recommend to the researcher what is your recommendation for the researcher on what predictors to measure?

```{r}

```
# Suggested readings
1. Granadeiro JP, Andrade J and Palmeirim JM (2004). Modelling the distribution of shorebirds in estuarine areas using generalized additive models. Journal of Sea Research 52: 227-240
2. Hale SS, Hughes MH, Strobel CJ, Buffum HW, Copeland JL and Paul JF (2002). Coastal ecological data from the Virginian Biogeographic Province, 1990-1993. Ecology 83 (10): 2942, and Ecological Archives E083-057
3. Leathwick JR, Elith J and Hastie T (2006). Comparative performance of generalized additive models and multivariate adaptive regression splines for statistical modelling of species distributions. Ecological Modelling 199: 188-196
4. Bellido JM, Pierce GJ and Wang J (2001). Modelling intra-annual variation in abundance of squid, Loligo forbesi in Scottish waters using generalized additive models. Fisheries Research 52: 23-39
5. Suarez-Seoane S, Osborne PE and Alonso JC (2002). Large-scale habitat selection by agricultural steppe birds in Spain: identifying species-habitat responses using generalized additive models. Journal of Applied Ecology 39: 755-771
6. Yee T and Mitchell ND (1991). Generalized additive models in plant ecology. Journal of Vegetation Science 2:587-602
7. Yee TW and Wild CJ (1996). Vector Generalized Additive models. Journal of Royal Statistical Society B 58 (3): 481-493
8. Hastie T and Tibshirani R (1986). Generalized additive models (with discussion). Statistical Science 1: 297-318.
9. Wood S (2002). Package 'mgcv'. [Online]. Available: http://cran.rproject.org/web/packages/mgcv/mgcv.pdf


# References (Weblinks): with R codes
1. https://stat.ethz.ch/R-manual/Rdevel/library/mgcv/html/summary.gam.html
2. http://multithreaded.stitchfix.com/blog/2015/07/30/gam/
3. https://support.sas.com/rnd/app/stat/topics/gam/gam.pdf
4. http://plantecology.syr.edu/fridley/bio793/gam.html
5. http://geog.uoregon.edu/GeogR/topics/gamex1.html
6. https://rpubs.com/ryankelly/GAMs
7. http://www.statisticssolutions.com/assumptions-of-linear-regression/
