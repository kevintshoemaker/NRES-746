---
title: "Lab Exercise 3"
author: "NRES 746"
date: "October 4, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Maximum likelihood and optimization

These next couple weeks are focused on fitting models, specifically estimating model parameters and confidence intervals, using likelihood techniques.  Estimating model parameters means finding the values of a set of parameters that best “fit” the data.  *Likelihood* is a metric that specifies the probability of drawing your particular data given a specified model (or parameters). 

This lab is designed to take 2 lab sessions to complete. **LAB 3 is due by midnight on Monday 10/17**

First, take a little time to review the likelihood lecture!

## Example: reed frog predation data

Read in the tadpole predation data for *Hyperolius spinigularis* (Vonesh and Bolker 2005) given to you as ReedfrogPred.csv.  Copy the ReedfrogPred.csv file from the course website to your computer. Read in the data.

```{r}
rfp <- read.csv("ReedfrogPred.csv")
head(rfp)

```


Parameter estimation is simplest when the data represent a collection of independent observations, with each observation having the same set of parameters.  Because predation on tadpoles is size and density-dependent, we will subset these data to a single size class (“small”) and density (10) for all treatments including a predator. Subset your data now.

```{r}
rfp_sub <- subset(rfp, (rfp$pred=='pred')&(rfp$size=="small")&(rfp$density==10))
rfp_sub

```


For each individual, the per-trial probability of being eaten by a predator is a binomial process (i.e., they can survive or die during the interval). Recall that the likelihood that k out of N individuals are eaten as a function of the per capita predation probability p is:

$Prob(k|p,N) = \binom{N}{k}p^{k}(1-p)^{N-k}$

Since the observations are independent, the joint likelihood of the whole data set is the product of the likelihood of each individual observation.  So, if we have n observations, each with the same total number of tadpoles N, and the number of tadpoles killed in the ith observation is ki, then the likelihood is:
 
$L = \prod_{i=1}^{n}\binom{N}{k_{i}}p^{k_{i}}(1-p)^{N-k_{i}}$
 
We conventionally work in terms of the log-likelihood (LL), which is:

$LL = \sum_{i=1}^{n}\left [log\binom{N}{k}+k_{i}log(p)+(N-k_{i})log(1-p)  \right ]$

In R this would be 
 
```{r eval=FALSE}
sum(dbinom(rfp_sub$surv, size=N, prob=p, log=TRUE)). 
```

There is only one parameter in this calculation, *p*, because we know how many individuals we started with (*N* = 10 for each trial) and how many survived in each trial (k = 7, 5, 9, and 9).  So we want to solve for the most likely value of *p* given our observations of *N* and *surv*.  In essence we do this by picking a possible value of *p* (which can only range from 0 to 1), calculating the log-likelihood (LL) using the equation above, picking another value of p, completing the equation, etc. until we exhaust all possible values of p and identify the one having the highest likelihood value. Of course R has useful built in functions to help us!  

The dbinom() function calculates the binomial likelihood for a specified data set, specifically a vector of the number of successes (or events) k, probability p, and number of trials N. Specify your vector of successes (here a success means being eaten by a predator!):

```{r}
num_killed <- 10-rfp_sub$surv
num_killed
```

Given our observed *k* and *N* = 10 for a given trial, what is the likelihood that *p* = 0.5 for each of our trials?    

```{r}
dbinom(num_killed,size=10,prob=0.5)
```


[1] 0.117187500 0.246093750 0.009765625 0.009765625

We can see that given our data, fixed sample size, and model (with *p* = 0.5), our observed outcomes are very unlikely.

What is the likelihood of observing all 4 of our outcomes, i.e, the joint probability of our data?

```{r}
prod(dbinom(num_killed,size=10,prob=0.5))
```

The joint likelihood values will be less than 1, and gets smaller and smaller each time we add more data (can you see why?). This is why we prefer to work with log-likelihoods (which yield larger numbers having better mathematical properties). And taking the log of value <1 yields a negative number, which is why we speak in terms of log-likelihoods.      

For now, we can build on this above process to estimate the likelihood function over the entire possible parameter space from 0 to 1.  
First we make a sequence of 100 probabilities from 0.01 to 1.

```{r}
p <- seq(0.01, 1, by=0.01)
```

Then we make an empty storage vector for the likelihoods we’ll calculate

```{r}
Lik <- numeric(length=100)
```

Now for the **for** loop! For every value of p (a sequence of 100 values) we will calculate the binomial probability and store it in the “Lik” vector.

```{r}
for(i in 1:100){
  Lik[i] <- prod(dbinom(num_killed,size=10,prob=p[i]))
}
plot(Lik~p,lty="solid",type="l", xlab="Predation Probability", ylab="Likelihood")
```

But we want to maximize the log-likelihood:

```{r}
p <- seq(0.01, 1, by=0.01)
LogLik <- numeric(length=100)
for(i in 1:100){
  LogLik[i] <- sum(dbinom(num_killed, size=10, 
  prob=p[i],log=TRUE))
}
plot(LogLik~p,lty="solid",type="l", xlab="Predation Probability", ylab="Log Likelihood")
```


We can ask R to tell us at which value of p the LL is maximized:

```{r}
p[which(LogLik==max(LogLik))]
```

And we can add an abline() to the maximum Log-Likelihood estimate:


```{r}
plot(LogLik~p,lty="solid",type="l", xlab="Predation Probability", ylab="Log Likelihood")
abline(v=0.25,lwd=3)
```


Alternatively, we can use the optim() or mle2() functions to find the maximum likelihood estimate.  Although we seek the most likely, or maximum likelihood estimate, in practice we generally minimize the negative log-likelihood. To do so, first write a function to calculate the binomial negative log-likelihood function and estimate parameter p.

```{r}
binomNLL1 <- function(p, k, N) {
  -sum(dbinom(k, size=N, prob=p, log=TRUE))
}
```

As we did in class, you can use the optim() function to minimize your negative log-likelihood function (binomNLL1) given a vector of starting parameters and your data.  The starting parameters need not be accurate, but do need to be reasonable for the function to work, that’s why we spent time in class eyeballing curves and calculating the method of moments.  Given that there is only one estimable parameter, p, in the binomial function, you need only provide a starting estimate for it.  Calculate the negative log-likelihood:

```{r}
opt1 <- optim(fn=binomNLL1, par = c(p=0.5), N = 10, k = num_killed, method = "BFGS") 
```

You may get several warning messages, can you think why?  opt1 returns a list that stores information about your optimization process.

```{r}
opt1
```


The important bits are whether or not the process achieved convergence and the parameter estimate that was converged upon.  
```{r}
opt1$convergence
```
Here a value of 0 means convergence has been achieved, a value of 1 means the process failed to converge.  We’ll learn more about convergence and alternative optimization options in Chapter 7.  

Your best fit estimate of p is:
```{r}
opt1$par
```

This numerically computed answer is almost, but not exactly, equal to the theoretical answer of 0.25.  The value of the function you optimized, binomNLL1, is:

```{r}
opt1$value
```

which is the negative log-likelihood for the model.  And, as we already know, the absolute likelihood of this particular outcome (5, 7, 9 and 9 out of 10 tadpoles eaten in four replicates) is quite low:

```{r}
exp(opt1$value)
```

Plot your observed outcomes against your predictions under the maximum likelihood model:

```{r}
hist(num_killed,xlim=c(0,10),freq=F)
curve(dbinom(x,prob=0.75,size=10),add=T,from=0,to=10,n=11)
```

Note that freg=F scales the y-axis to density, which is necessary to overlay the curve.

#### Exercise 1

> Develop a function that returns the data likelihood (*likelihood function*) for the following scenario: you visit three known-occupied wetland sites ten times and for each site you record the number of times a particular frog species is detected within a 5 minute period. Assuming that all sites are occupied continously, compute the likelihood of these data: [3,2 and 6 detections for sites 1, 2, and 3 respectively] for a given detection probability $p$. Assume that all sites have the same (unknown) detection probability. Using this likelihood function, answer the following questions: 

1. What is the maximum likelihood estimate for the *p* (detection probability) parameter? 
2. Using the "rule of 2", determine the approximate 95% confidence interval for the *p* parameter.     

Provide your code *in markdown*, with all code fully commented. 

## Adding in a Deterministic Relationship

So we’ve looked at how to obtain the likelihood of getting our dataset given a stochastic model (the binomial distribution), but now we want to consider more interesting ecological questions like when the mean or variance of the model parameters vary among groups or depend upon covariates.  Recall that we subset our data above because we expected survival to be (in part) density-dependent.   Here we’ll consider how to model the probability of tadpole survival as a function of the initial density of tadpoles in the population.  To do so, we need to incorporate a deterministic function into our stochastic model.  

Read in the functional response dataset and look at the first few lines:

```{r}
rffr <- read.csv("ReedfrogFuncResp.csv")
head(rffr)
```

First, let’s look at the stochastic distribution of the data, which is the probability of being killed.

```{r}
hist(rffr$Killed/rffr$Initial)
```


Based on the way this histogram looks (and what we know mechanistically about the data), we’ll use a binomial distribution to describe the random error.

Plot the number killed by the initial density (using plot()) to see what sort of deterministic function would describe the pattern.  It looks like it could be linear, but because we know that this is a predation response, and that predators become handling limited at high prey densities.   On page 182 Bolker indicates that if predation rate= $aN/(1+ahN)$ (which follows a Type II functional response), this means that the per capita predation rate of tadpoles decreases hyperbolically with tadpole density $(= a/(1 + ahN))$.  We’ll use this deterministic function for our data.     

First, let’s see what that curve would look like over our data points with an initial guess at the parameters.  Recall that the a parameter of this hyperbolic function indicates the initial slope, which we’ll guess to be around 0.5, and the h parameter indicates 1/asymptote, which we fiddled around with to match the data (so try 1/80).

```{r}
Holl2<-function(x, a, h){(a*x)/(1+(a*h*x))}
plot(rffr$Killed~rffr$Initial)
curve(Holl2(x, a=0.5, h=1/80), add=TRUE,col="red")
```


This looks pretty good, but we want to actually fit the line to the data instead of making guesses, and we’ll use likelihood to do that. Just like before, we’ll write a *negative log likelihood function*, but this time we’ll incorporate the deterministic model.

```{r}
binomNLL2<-function(params,N,k){
	a=params[1]
	h=params[2]
	predprob=a/(1+a*h*N)	
	-sum(dbinom(k,prob=predprob,size=N,log=TRUE))
}
```

This likelihood function says that the structure of the data is described by a binomial distribution (either killed or not), and that the probability of predation (the number killed divided by the initial number) is explained by the Holling type II equation.  (The equation is specified differently here than when we used it to make a curve, can you think of why?)

Now we’ll find the parameter values that best describe these data using optim(). We’ll use the same initial values for a and h that we used to plot the curve.  N is the initial number of tadpoles, and k is the number of tadpoles killed.


```{r}
opt2 <- optim(fn=binomNLL2,  par=c(a=0.5,h=(1/80)), N=rffr$Initial, k=rffr$Killed)  #use default simplex algorithm
opt2
```

 
The results are not that different from our starting values, so we made a good guess.  

#### Exercise 2

- Overlay this line onto your data points to see how different the two lines are.
- Try some different starting values. Can you find any starting values that cause the optimization algorithm ("optim" function) to fail?
- Now visualize prediction intervals around this line to make a plot like Figure 6.5a in the book.  We could use a loop to do this, but try using vectors as it’s usually faster to do so in R. First we’ll need vectors of predictor and response values precisely along the best fit line. Next, use qbinom() to estimate the 95% confidence intervals of the binomial distribution. In qbinom(), for the "prob" argument use the ratio of the x and y values to get a probability value from 0-1. For example:

```{r eval=FALSE}
upper<-qbinom(0.975,prob=yvec/xvec, size=xvec)
lower<-qbinom(0.025,prob=yvec/xvec, size=xvec)
```

- Now use the lines() function to plot the confidence intervals.

**Aside**: Bolker calls these types of confidence intervals *plug-in confidence intervals* because they ignore the uncertainty in the *a* and *h* parameters and just use the uncertainty in the binomial distribution.

### Review of the MLE process

**Step 1**.  Identify the response and explanatory variables: Predation probability and Initial Population Size. Just stating what the response and explanatory variables are will help you start modeling.

**Step 2**.  Determine the stochastic distribution: Binomial.  In this case, the stochastic distribution was easy to identify because we chose it mechanistically.  Other times it may not be so clear what the best distribution is, and looking at the histogram and plotting different distributions over the top will be helpful.

**Step 3**.  Specify the deterministic function: Holling type II.  Again, we chose this function mechanistically, but we could have chosen different functions just by looking at the plot of the points.

**Step 4**.  Specify the likelihood of the data given our deterministic expectations and the stochastic distribution: binomNLL2.  Our negative *log likelihood function* combined the stochastic and deterministic elements together by having the stochastic parameter (in this case the binomial probability, *p*) be dependent upon the deterministic parameters.

**Step 5**.  Make a guess for the initial parameters: *a*=0.5, *h*=1/80.  You need to have an initial guess at the parameters to make optim() work, and we plotted the Holling curve to make our guess.  Sometimes you will also need to make a guess at the parameters for the stochastic distribution.  In these cases, the *method of moments* is often the best option.

**Step 6**.  Estimate the best fit parameters using maximum likelihood: opt2.  We used optim() to search through all the possible value combinations of parameters a and h to estimates for those parameters that correspond to the minimized negative log-likelihood.

**Step 7**.  Add confidence intervals around your estimates.  We calculated some *plug-in estimates* to put confidence regions around our estimates based on the stochastic function.  


## Exploring Likelihood Surfaces and Confidence Regions

Let's continue the myxomatosis virus titer example from the optimization lecture.  The difference is that this time we'll model a deterministic process (decay of viral loads over time) in addition to the stochastic process.  Our goal is to fit a model to data on viral titers through time for the viruses that are grade 1. Biological common sense, and data from other titer levels, tells us to expect titer levels to start at zero, increase over time to a peak, and then to decline.  Given those expectations, we’ll fit a Ricker model to these data, following Bolker’s example and extending it just a bit.  

Our goals are to:   

-	Find the maximum likelihood estimates of the parameters of the Ricker model fit to the myxomytosis data.
-	Visualize the fit of this model to the data by:
    -	Plotting the data
    - Adding the predicted Ricker curve 
    - Adding plug-in confidence intervals
    - Plot the 2-dimensional likelihood surface for the parameters of the Ricker and add the bivariate 95% confidence interval

You can add the data from the emdbook package:

```{r}
library(emdbook)
data(MyxoTiter_sum)
head(MyxoTiter_sum)
```

Select just the grade 1 titers:

```{r}
myxdat <- subset(MyxoTiter_sum, grade==1)

plot(myxdat$titer~myxdat$day)
```



#### Exercise 3: 
**Fit a Ricker model to the myxomatosis data**

Our question is: how does a virus titer change in rabbits over time after infection? This is almost exactly the same problem as we just did, but we’re using different distributions and functions.  To solve the problem, you’ll need to go through the same steps outlined above.

**Step 1**. *Identify the response and explanatory variables*. 
 
**Step 2**. *Determine the stochastic distribution.*   
Start by plotting the histogram of the response variable. Bolker suggests a gamma distribution – does it look like a gamma would work?  Write down the parameters for the gamma distribution (page 133). (For the gamma distribution, use with the shape and scale parameters (not the rate)). If you prefer (it is easier!), you may model the error distribution as normal! 

**Step 3**. *Specify the deterministic function*.  
Plot the data points (hint: look at figure 6.5b).  Bolker suggests the Ricker curve – does it look like the Ricker curve would work?  (Note that grade 1 virus is so virulent that most rabbits die before the titer has a chance to drop off entirely) Write down the equation and parameters for the Ricker curve (page 94):

**Step 4**. *Specify the likelihood of the data given our deterministic expectations and the stochastic distribution.*
Take a moment to think how the parameters of the stochastic distribution are determined by the parameters of the deterministic function. For the gamma distribution, both the shape and scale parameters are related to the mean of the distribution, i.e., mean =  shape × scale (page 133).  So how will you specify that the deterministic function (the Ricker model) should represent the mean? What parameters do you need your (negative) log-likelihood function to estimate? Write out your negative log-likelihood function to solve for the likelihood.

Note that the shape parameter can be specified as: $\frac{mean^2}{var}$ and the scale parameter can be specified as $\frac{var}{mean}$

**Step 5**.  *Make a guess for the initial parameters*.
We need initial parameters to put into optim().  Remember that the Ricker curve parameters can be estimated based on the initial slope and maximum (see pg. 95). Try plotting the curve over the points to get an approximate fit. There really isn’t any easier way to get there than trial and error for the deterministic function.     

**Step 6**.  *Estimate the best fit parameters using maximum likelihood*.	
Now use optim() to get your maximum likelihood parameter estimates.

**Step 7**.  *Add plug-in prediction intervals around your estimates*.  
After your run of optim (did you achieve convergence?), plot your fitted Ricker curve to your data.  Revisit the earlier prediction interval code to add *plug-in* confidence intervals around your predicted curve based on gamma distributed errors (should resemble Figure 6.5b on page 184 of text).  

Use the qgamma or qnorm functions (depending on how you are modeling your error...)

```{r eval=FALSE}
upper<-qgamma(0.975,shape=?, scale=?)
lower<-qbinom(0.025,shape=?, scale=?)
```


#### Exercise 4: 2-dimensional likelihood surface
Making plug-in confidence intervals looks nice on the plot, but if we also want to visualize parameter uncertainty (and not just the stochastic data generating process) we need to consider an *n*-dimensional likelihood surface.  Pick two parameters from your myxomatosis model (holding any remaining parameters constant) and visualize how the likelihood changes as we change these two parameters at the same time.  Imagine making a graph like the one at the beginning of the lab showing the likelihood being affected by the parameter *p*.  We want to add another dimension to that plot by having likelihood also be affected by another parameter.  What we’ll end up with will look something like Figure 6.7 in the Bolker book.

Plot the 2-D likelihood surface (you may use code examples from lecture). Use nested **for** loops to explore these two dimensions of parameter space.  Calculate the likelihood for many values along both dimensions and store those likelihoods in a 2-dimensional matrix. Finally create a line for the 95% bivariate confidence interval (the CI for both parameters), use contour(). Specify the level of contour by the -LL cutoff value derived from the chi-square distribution (use qchisq(), and see. pg. 194 for how Bolker made his 95% bivariate confidence region).

  
#### Optional Exercise 5: profile likelihood
Pick one of the parameters in your model (selected parameter) and construct a profile likelihood for this parameter. You need to modify your likelihood and optim functions to estimate only the other parameters (because you’ll be fixing the selected parameter). Use the profile likelihood to compute a 95% confidence interval for this parameter. 
















