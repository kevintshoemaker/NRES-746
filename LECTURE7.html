<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-10-18" />

<title>Bayesian Analysis #2: MCMC</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection</a>
    </li>
    <li>
      <a href="Time_Series_Lab.html">Time-series mini-lab</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="GAMs.html">GAMs</a>
    </li>
    <li>
      <a href="RMarkdown_FigureDemo.html">Publication-quality figures in R</a>
    </li>
    <li>
      <a href="Bayesian Networks.pptx">Bayesian Networks</a>
    </li>
    <li>
      <a href="GraphTheory.html">Graph Theory</a>
    </li>
    <li>
      <a href="NRES746_IPMs.pptx">Integrated Population Models</a>
    </li>
    <li>
      <a href="TimeSeries_heckler.html">Time Series Analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Bayesian Analysis #2: MCMC</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>October 18, 2016</em></h4>

</div>


<div id="markov-chain-monte-carlo" class="section level3">
<h3>Markov Chain Monte Carlo</h3>
<p>Now in many cases, we simply won’t have the computational power to partition our parameter space into discrete pixels and completely evaluate the posterior probability for all <em>n</em>-dimensional pixels in that space. In these cases, we tend to harness ingenious algorithms known as Markov-Chain Monte Carlo. This approach uses stochastic jumps in parameter space to (eventually) settle on a stationary posterior distribution. The key to MCMC is the following:</p>
<blockquote>
<p>The ratio of successful jump probabilities is proportional to the ratio of the posterior probabilities.</p>
</blockquote>
<p>The jump probability can be characterized as:</p>
<p><span class="math inline">\(Prob(jump) * Prob(accept)\)</span></p>
<p>The ratio of jump probabilities can be characterized as:</p>
<p><span class="math inline">\(\frac{Prob(jump_{b\rightarrow a})\cdot Prob(accept a|b)}{Prob(jump_{a\rightarrow b})\cdot Prob(accept b|a)}\)</span></p>
<p>This ratio MUST be equal to the ratio of the posterior probabilities:</p>
<p><span class="math inline">\(\frac{Posterior(A)}{Posterior(B)}\)</span></p>
<p>If this rule is met, then in the long run the chain will spend a lot of time occupying high-probability parts of parameter space. With enough jumps, the long-term distribution will match the joint posterior probability distribution.</p>
<blockquote>
<p>MCMC is essentially a type of random number generator that is designed to sample from difficult-to-describe (e.g., multivariate, hierarchical) probability distributions. In many/most cases, the posterior distribution for ecological problems is a very difficult-to-describe probability distribution.</p>
</blockquote>
</div>
<div id="metropolis-hastings-algorithm" class="section level2">
<h2>Metropolis-Hastings algorithm</h2>
<p>This algorithm is very similar to the simulated annealing algorithm! The main difference: the “temperature” doesn’t decrease over time and the parameter <em>k</em> is set to 1.</p>
<p>The M-H algorithm can be expressed as:</p>
<p><span class="math inline">\(Prob(accept A|B) = min(1,\frac{Posterior(B)}{Posterior(A)}\cdot \frac{Prob(b\rightarrow a)}{Prob(a\rightarrow b)})\)</span></p>
<p>Note that essentially this is the same as the Metropolis simulated-annealing algorithm, with the posterior probabilities substituted for the likelihood and the <em>k</em> parameter set to 1</p>
</div>
<div id="bivariate-normal-example" class="section level2">
<h2>Bivariate normal example</h2>
<p>This example is modified from <a href="http://www.mas.ncl.ac.uk/~ndjw1/teaching/sim/gibbs/gibbs.html">this link by Prof Darren Wilkinson</a></p>
<p>Remember that MCMC samplers are just a type of random number generator. We can use a Metropolis-Hastings sampler to develop our own random number generator for a fairly simple known distribution. In this example, we use a M-H Sampler to generate random numbers from a standard bivariate normal probability distribution.</p>
<p>We don’t need an MCMC sampler for this simple example. One way to do this would be to use the following code, which draws and visualizes an arbitrary number of independent samples from the bivariate standard normal distribution.</p>
<pre class="r"><code>rbvn&lt;-function (n, rho)   #function for drawing an arbitrary number of independent samples from the bivariate standard normal distribution. 
{
        x &lt;- rnorm(n, 0, 1)
        y &lt;- rnorm(n, rho * x, sqrt(1 - rho^2))
        cbind(x, y)
}

bvn&lt;-rbvn(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<pre class="r"><code>library(mvtnorm)</code></pre>
<pre><code>## 
## Attaching package: &#39;mvtnorm&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:emdbook&#39;:
## 
##     dmvnorm</code></pre>
<pre class="r"><code>metropolisHastings &lt;- function (n, rho=0.98){    # a MCMC sampler implementation of a bivariate random number generator
    mat &lt;- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
    x &lt;- 0
    y &lt;- 0
    prev &lt;- dmvnorm(c(x,y),mean=c(0,0),sigma = matrix(c(1,rho,rho,1),ncol=2))
    mat[1, ] &lt;- c(x, y)        # initialize the markov chain
    counter &lt;- 1
    while(counter&lt;=n) {
      newx &lt;- rnorm(1,x,0.5)     # make a jump
      newy &lt;- rnorm(1,y,0.5)
      
      newprob &lt;- dmvnorm(c(newx,newy),sigma = matrix(c(1,rho,rho,1),ncol=2))    # assess whether the new jump is good!
      ratio &lt;- newprob/prev
      
      prob.accept &lt;- min(1,ratio)     # decide whether to accept the new jump!
      rand &lt;- runif(1)
      if(rand&lt;=prob.accept){
        x=newx;y=newy
        mat[counter,] &lt;- c(x,y) 
        counter=counter+1
        prev &lt;- newprob
      }
      
    }
    return(mat)
}</code></pre>
<p>Then we can use the M-H sampler to get random samples from this known distribution…</p>
<pre class="r"><code>bvn&lt;-metropolisHastings(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>Okay, enough with super simple examples- let’s try it for a non-trivial problem, like the Myxomatosis example from the Bolker book!</p>
<div id="myxomatosis-revisited-again" class="section level3">
<h3>Myxomatosis revisited (again!)</h3>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>Recall that we are modeling the distribution of measured titers (virus loads) for Australian rabbits. Bolker chose to use a Gamma distribution. Here is the empirical distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>We need to estimate the gamma rate and shape parameters that best fit this empirical distribution. Here is one example of a Gamma fit to this distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Recall that the 2-D (log) likelihood surface looks something like this:</p>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Here is an implementation of the M-H algorithm to find the joint posterior distribution!</p>
<p>First, we need a likelihood function (our old friend!)</p>
<pre class="r"><code>GammaLikelihoodFunction &lt;- function(params){
  prod(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=F))
}

params &lt;- c(shape=40,scale=0.15) 
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] 2.906766e-22</code></pre>
<p>Then, we need a prior distribution for our parameters! Let’s assign relatively flat priors for both of our parameters. In this case, let’s assign a <span class="math inline">\(gamma(shape=0.01,scale=100)\)</span> for the shape parameter and a <span class="math inline">\(gamma(shape=0.1,scale=10)\)</span> distribution for the scale parameter:</p>
<pre class="r"><code>GammaPriorFunction &lt;- function(params){
  prior &lt;- c(shape=NA,scale=NA)
  prior[&#39;shape&#39;] &lt;- dgamma(params[&#39;shape&#39;],shape=0.01,scale=100)
  prior[&#39;scale&#39;] &lt;- dgamma(params[&#39;scale&#39;],shape=0.001,scale=1000)
  # prior[&#39;shape&#39;] &lt;- dunif(params[&#39;shape&#39;],3,100)
  # prior[&#39;scale&#39;] &lt;- dunif(params[&#39;scale&#39;],0.01,0.5)
  return(prod(prior))
}

curve(dgamma(x,shape=0.01,scale=1000),3,100)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>params &lt;- c(shape=40,scale=0.15) 
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaPriorFunction(params)</code></pre>
<pre><code>## [1] 1.104038e-06</code></pre>
<p>Note that we are also assuming (fairly standard assumption) that the shape and scale are <em>independent</em> in the prior (multiplicative probabilities for the joint prior).</p>
<p>Then, we need a function that can compute the ratio of posterior probabilities for any given jump in parameter space. Because we are dealing with a <em>ratio</em> of posterior probabilities, we do NOT need to compute the normalization constant. Without the need for a normalization constant, we just need to compute the ratio of weighted likelihoods (that is, the likelihood weighted by the prior)</p>
<pre class="r"><code>PosteriorRatio &lt;- function(oldguess,newguess){
  oldLik &lt;- max(1e-90,GammaLikelihoodFunction(oldguess))
  oldPrior &lt;- max(1e-90,GammaPriorFunction(oldguess))
  newLik &lt;- GammaLikelihoodFunction(newguess)
  newPrior &lt;- GammaPriorFunction(newguess)
  return((newLik*newPrior)/(oldLik*oldPrior))
}

oldguess &lt;- params
newguess &lt;- c(shape=39,scale=0.15)

PosteriorRatio(oldguess,newguess)</code></pre>
<pre><code>## [1] 0.01423757</code></pre>
<p>Then we need a function for making new guesses, or jumps in parameter space:</p>
<pre class="r"><code>     # function for making new guesses
newGuess &lt;- function(oldguess){
  sdshapejump &lt;- 4
  sdscalejump &lt;- 0.07
  jump &lt;- c(shape=rnorm(1,mean=0,sd=sdshapejump),scale=rnorm(1,0,sdscalejump))
  newguess &lt;- abs(oldguess + jump)
  return(newguess)
}
  # set a new &quot;guess&quot; near to the original guess

newGuess(oldguess=params)     # each time is different- this is the first optimization procedure with randomness built in</code></pre>
<pre><code>##      shape      scale 
## 29.7815940  0.1980339</code></pre>
<pre class="r"><code>newGuess(oldguess=params)</code></pre>
<pre><code>##       shape       scale 
## 40.37349215  0.08592807</code></pre>
<pre class="r"><code>newGuess(oldguess=params)</code></pre>
<pre><code>##      shape      scale 
## 42.5866789  0.1065055</code></pre>
<p>Now we are ready to implement the Metropolis-Hastings MCMC algorithm:</p>
<p>We need a starting point:</p>
<pre class="r"><code>startingvals &lt;- c(shape=75,scale=0.28)    # starting point for the algorithm</code></pre>
<p>Let’s play with the different functions we have so far…</p>
<pre class="r"><code>newguess &lt;- newGuess(startingvals)    # take a jump in parameter space
newguess</code></pre>
<pre><code>##      shape      scale 
## 71.2901411  0.3355522</code></pre>
<pre class="r"><code>PosteriorRatio(startingvals,newguess)   # difference in posterior ratio</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Now let’s look at the Metropolis routine:</p>
<pre class="r"><code>chain.length &lt;- 10
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Let’s run it for longer…</p>
<pre class="r"><code>chain.length &lt;- 100
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>How about for even longer??</p>
<pre class="r"><code>chain.length &lt;- 1000
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>This looks better! The search algorithm is finding the high-likelihood parts of parameter space pretty well!</p>
<p>Now, let’s look at the chain for the “shape” parameter</p>
<pre class="r"><code>plot(1:chain.length,guesses[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>And for the scale parameter…</p>
<pre class="r"><code>plot(1:chain.length,guesses[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Can we say that these chains have converged on the posterior distribution for the shape parameter??</p>
<p>First of all, the beginning of the chain “remembers” the starting value, and is therefore not a stationary distribution. We need to remove the first part of the chain, called the <strong>‘burn-in’</strong>.</p>
<pre class="r"><code>burn.in &lt;- 100
MCMCsamples &lt;- guesses[-c(1:burn.in),]

chain.length=chain.length-burn.in
plot(1:chain.length,MCMCsamples[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<p>But it still doesn’t look all that great. Let’s run it for even longer, and see if we get something that looks more like a proper random number generator (white noise)…</p>
<pre class="r"><code>chain.length &lt;- 20000
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Let’s first remove the first 5000 samples as a burn-in</p>
<pre class="r"><code>burn.in &lt;- 5000
MCMCsamples &lt;- guesses[-c(1:burn.in),]
chain.length=chain.length-burn.in</code></pre>
<p>Now, let’s look at the chains again</p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
<p>When evaluating these trace plots, we are hoping to see a “stationary distribution” that looks like white noise. This trace plot looks like it might have a little autocorrelation. One way to “fix” this is to thin the MCMC samples:</p>
<pre class="r"><code>thinnedMCMC &lt;- MCMCsamples[seq(1,chain.length,by=10),]
plot(1:nrow(thinnedMCMC),thinnedMCMC[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>plot(1:nrow(thinnedMCMC),thinnedMCMC[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<p>Now we can examine our posterior distribution!</p>
<pre class="r"><code>plot(density(thinnedMCMC[,&#39;scale&#39;]),main=&quot;scale parameter&quot;,xlab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>plot(density(thinnedMCMC[,&#39;shape&#39;]),main=&quot;shape parameter&quot;,xlab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
<p>And we can visualize as before.</p>
<pre class="r"><code>par(mfrow=c(3,2))
plot(thinnedMCMC,col=1:10000)
plot(thinnedMCMC,type=&quot;l&quot;)
plot(ts(thinnedMCMC[,1]))
plot(ts(thinnedMCMC[,2]))
hist(thinnedMCMC[,1],40)
hist(thinnedMCMC[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>Hopefully it is clear that the Metropolis-Hastings MCMC method could be modified to fit arbitrary numbers of free parameters for arbitrary models. However, the M-H algorithm is not necessarily the most easily generalizable to a host of different model types. That award tends to go to the <strong>Gibbs sampler</strong>. In lab we will play around with Gibbs samplers, mostly using an amazing piece of software called <strong>BUGS</strong> (<strong>B</strong>ayesian <strong>I</strong>nference <strong>U</strong>sing <strong>G</strong>ibbs <strong>S</strong>ampling).</p>
<p>NOTE: BUGS implementations (e.g., JAGS) actually tend to use a combination of M-H and Gibbs sampling!</p>
</div>
</div>
<div id="gibbs-sampler" class="section level2">
<h2>Gibbs sampler</h2>
<p>The Gibbs sampler is amazingly straightforward and powerful. Basically, the algorithm successively samples from the <em>full conditional</em> probability distribution – that is, the posterior distribution for arbitrary parameter <em>i</em> conditional on known values for all other parameters in the model.</p>
<p>In many cases, we can’t work out the full posterior distribution for our model directly, but we <strong>CAN</strong> work out the conditional posterior distribution analytically if all parameters except for the parameter in question were known with certainty. This is especially true if we use conjugate priors for our model specification. Even if not, the full conditional is often analytically tractable. Nonetheless, even if it’s not analytically tractable, we can use a univariate M-H procedure as a “brute force” last resort!</p>
</div>
<div id="bivariate-normal-example-1" class="section level2">
<h2>Bivariate normal example</h2>
<p>Again, remember that MCMC samplers are just a type of random number generator. We can use a Gibbs sampler to develop our own random number generator for a fairly simple known distribution. In this example (same as before), we use a Gibbs Sampler to generate random numbers from a standard bivariate normal probability distribution. Notice that the Gibbs sampler is in many ways more simple and straightforward than the M-H algorithm.</p>
<p>We don’t need a Gibbs Sampler for this simple example. One way to do this would be to use the following code, which draws and visualizes an arbitrary number of independent samples from the bivariate standard normal distribution.</p>
<pre class="r"><code>rbvn&lt;-function (n, rho){  #function for drawing an arbitrary number of independent samples from the bivariate standard normal distribution. 
        x &lt;- rnorm(n, 0, 1)
        y &lt;- rnorm(n, rho * x, sqrt(1 - rho^2))
        cbind(x, y)
}

bvn&lt;-rbvn(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<pre class="r"><code>gibbs&lt;-function (n, rho){    # a gibbs sampler implementation of a bivariate random number generator
    mat &lt;- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
    x &lt;- 0
    y &lt;- 0
    mat[1, ] &lt;- c(x, y)        # initialize the markov chain
    for (i in 2:n) {
            x &lt;- rnorm(1, rho * y, sqrt(1 - rho^2))        # sample from x conditional on y
            y &lt;- rnorm(1, rho * x, sqrt(1 - rho^2))        # sample from y conditional on x
            mat[i, ] &lt;- c(x, y)
    }
    mat
}</code></pre>
<p>Then we can use the Gibbs sampler to get random samples from this known distribution…</p>
<pre class="r"><code>bvn&lt;-gibbs(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>There is quite a bit of apparent autocorrelation in the samples of the markov chain here. Gibbs samplers frequently have this issue!</p>
<div id="back-to-myxomatosis" class="section level3">
<h3>Back to Myxomatosis!</h3>
<div id="aside-the-bugs-language" class="section level4">
<h4>Aside: the BUGS language</h4>
<p>Finally, let’s build a Gibbs sampler for our favorite Myxomatosis example! To do this, we will use the BUGS language, as implemented in JAGS, to help us!</p>
<p>The BUGS language looks simlar to R, but there are several key differences:</p>
<ul>
<li>First of all, BUGS is a compiled language, so the order of operations in your code doesn’t really matter</li>
<li>BUGS is not vectorized- you need to use FOR loops!</li>
<li>Several probability distributions are parameterized very differently in BUGS. Notably, the normal distribution is parameterized with a mean and a precision (<span class="math inline">\(1/Variance\)</span>).</li>
</ul>
<p>Here is the myxomatosis example, as implemented in the BUGS language:</p>
<pre class="r"><code>model {
  
  #############
  # LIKELIHOOD
  ############
  for(obs in 1:n.observations){
    titer[obs] ~ dgamma(shape,rate)
  }
  
  #############
  # PRIORS
  ############
  shape ~ dgamma(0.001,0.001)
  scale ~ dgamma(0.01,0.01)
  rate &lt;- 1/scale   # convert the scale parameter to a &quot;rate&quot; for BUGS
}</code></pre>
<p>We can use the “sink” function in R to write out this model to a text file:</p>
<pre class="r"><code>sink(&quot;BUGSmodel.txt&quot;)
  cat(&quot;
    model {
      
      #############
      # LIKELIHOOD
      ############
      for(obs in 1:n.observations){
        titer[obs] ~ dgamma(shape,rate)
      }
      
      #############
      # PRIORS
      ############
      shape ~ dgamma(0.001,0.001)
      scale ~ dgamma(0.01,0.01)
      rate &lt;- 1/scale
    }
  &quot;)</code></pre>
<pre><code>## 
##     model {
##       
##       #############
##       # LIKELIHOOD
##       ############
##       for(obs in 1:n.observations){
##         titer[obs] ~ dgamma(shape,rate)
##       }
##       
##       #############
##       # PRIORS
##       ############
##       shape ~ dgamma(0.001,0.001)
##       scale ~ dgamma(0.01,0.01)
##       rate &lt;- 1/scale
##     }
## </code></pre>
<pre class="r"><code>sink()</code></pre>
<p>Now that we have the BUGS model packaged as a text file, we “package” the data into a single list object that contains all the relevant data referenced in the BUGS code:</p>
<pre class="r"><code>myx.data.for.bugs &lt;- list(
  titer = Myx$titer,
  n.observations = length(Myx$titer)
)

myx.data.for.bugs</code></pre>
<pre><code>## $titer
##  [1] 5.207 5.734 6.613 5.997 6.612 6.810 5.930 6.501 7.182 7.292 7.819
## [12] 7.489 6.918 6.808 6.235 6.916 4.196 7.682 8.189 7.707 7.597 7.112
## [23] 7.354 7.158 7.466 7.927 8.499
## 
## $n.observations
## [1] 27</code></pre>
<p>Then we need to define the initial values for all parameters. It is convenient to define this as a function, so that each MCMC chain can be initialized with different starting values. This will become clear later!</p>
<pre class="r"><code>init.vals.for.bugs &lt;- function(){
  init.list &lt;- list(
    shape=runif(1,20,100),
    scale=runif(1,0.05,0.3)
  )
  return(init.list)
}

init.vals.for.bugs()</code></pre>
<pre><code>## $shape
## [1] 32.31097
## 
## $scale
## [1] 0.2416489</code></pre>
<pre class="r"><code>init.vals.for.bugs()</code></pre>
<pre><code>## $shape
## [1] 38.70362
## 
## $scale
## [1] 0.293828</code></pre>
<pre class="r"><code>init.vals.for.bugs()</code></pre>
<pre><code>## $shape
## [1] 57.86737
## 
## $scale
## [1] 0.07734904</code></pre>
<p>Now we can call JAGS!</p>
<pre class="r"><code>library(R2jags)

library(coda)

params.to.store &lt;- c(&quot;shape&quot;,&quot;scale&quot;)

jags.fit &lt;- jags(data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,n.iter=5000,model.file=&quot;BUGSmodel.txt&quot;,n.chains = 3,n.burnin = 0 )</code></pre>
<pre><code>## module glm loaded</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 27
##    Unobserved stochastic nodes: 2
##    Total graph size: 37
## 
## Initializing model</code></pre>
<pre class="r"><code>jagsfit.mcmc &lt;- as.mcmc(jags.fit)   # convert to &quot;MCMC&quot; object (coda package)

summary(jagsfit.mcmc)</code></pre>
<pre><code>## 
## Iterations = 1:4996
## Thinning interval = 5 
## Number of chains = 3 
## Sample size per chain = 1000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean       SD Naive SE Time-series SE
## deviance 77.7411  2.55422 0.046633        0.34042
## scale     0.1652  0.06165 0.001126        0.01109
## shape    46.6595 13.90224 0.253819        2.06740
## 
## 2. Quantiles for each variable:
## 
##              2.5%    25%     50%   75%   97.5%
## deviance 75.38751 76.035 76.9887 78.45 85.1954
## scale     0.09326  0.123  0.1518  0.19  0.3372
## shape    20.91365 36.616 45.7822 56.28 73.8625</code></pre>
<pre class="r"><code>plot(jagsfit.mcmc)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
</div>
<div id="assessing-convergence" class="section level3">
<h3>Assessing convergence</h3>
<p>This is probably a good time to talk about convergence of MCMC chains on the stationary posterior distribution. The above plots don’t look great. We want to see white noise, and we want to see chains that look similar to one another.</p>
<p>The first check is just visual- we look for the following to assess convergence:</p>
<ul>
<li>The chains for each parameter, when viewed as a “trace plot” should look like white noise, or similar.</li>
<li>Multiple chains with different starting conditions should look the same!!</li>
</ul>
<p>One way we might be able to do a better job here is to run the chains longer and discard the initial samples as a <em>burn in</em>!</p>
<p>We can also try to reduce serial autocorrelation by thinning our chain- here we retain only 1 out of every 20 samples.</p>
<pre class="r"><code>jags.fit &lt;- jags(data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,n.iter=50000,model.file=&quot;BUGSmodel.txt&quot;,n.chains = 3, n.burnin=10000,n.thin = 20)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 27
##    Unobserved stochastic nodes: 2
##    Total graph size: 37
## 
## Initializing model</code></pre>
<pre class="r"><code>jagsfit.mcmc &lt;- as.mcmc(jags.fit)   # convert to &quot;MCMC&quot; object (coda package)

summary(jagsfit.mcmc)</code></pre>
<pre><code>## 
## Iterations = 10001:49981
## Thinning interval = 20 
## Number of chains = 3 
## Sample size per chain = 2000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##            Mean       SD  Naive SE Time-series SE
## deviance 77.282  1.89294 0.0244378       0.049514
## scale     0.155  0.04343 0.0005607       0.002057
## shape    47.994 12.71093 0.1640973       0.613051
## 
## 2. Quantiles for each variable:
## 
##              2.5%     25%     50%     75%   97.5%
## deviance 75.38151 75.9135 76.7214 78.0754 82.3545
## scale     0.09061  0.1236  0.1479  0.1782  0.2593
## shape    26.89894 38.7795 46.8371 55.8687 75.7883</code></pre>
<pre class="r"><code>plot(jagsfit.mcmc)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Just visually, this looks better. Now we can use some more quantitative convergence metrics.</p>
<div id="the-gelman-rubin-diagnostic" class="section level4">
<h4>The Gelman-Rubin diagnostic</h4>
<p>One simple and intuitive convergence diagnostic is the <em>Gelman-Rubin diagnostic</em>, which assesses whether chains are more different from one another than they should be on the basis of simple Monte Carlo error:</p>
<pre class="r"><code>gelman.diag(jagsfit.mcmc)</code></pre>
<pre><code>## Potential scale reduction factors:
## 
##          Point est. Upper C.I.
## deviance       1.00       1.01
## scale          1.01       1.04
## shape          1.01       1.02
## 
## Multivariate psrf
## 
## 1.01</code></pre>
<p>In general, values of 1.1 or higher are considered poorly converged (sometimes the less stringent criterion of 1.2 is used). If so, you should try running longer chains!</p>
<p>So this model looks pretty good!</p>
</div>
</div>
</div>
<div id="useful-links-for-mcmc" class="section level2">
<h2>Useful links for MCMC</h2>
<p><a href="https://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/">Darren Winkinson’s research blog</a><br />
<a href="https://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/">Where do the full conditionals come from in Gibbs sampling?</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
