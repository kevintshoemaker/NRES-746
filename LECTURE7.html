<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-10-18" />

<title>Bayesian Analysis #2: MCMC</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="Generalized Additive Models (GAMs).pdf">GAMs</a>
    </li>
    <li>
      <a href="RMarkdown_FigureDemo.html">Publication-quality figures in R</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Bayesian Analysis #2: MCMC</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>October 18, 2016</em></h4>

</div>


<div id="markov-chain-monte-carlo" class="section level3">
<h3>Markov Chain Monte Carlo</h3>
<p>Now in many cases, we simply won’t have the computational power to partition our parameter space into discrete pixels and completely evaluate the posterior probability for all <em>n</em>-dimensional pixels in that space. In these cases, we tend to harness ingenious algorithms known as Markov-Chain Monte Carlo. This approach uses stochastic jumps in parameter space to (eventually) settle on a stationary posterior distribution. The key to MCMC is the following:</p>
<blockquote>
<p>The ratio of successful jump probabilities is proportional to the ratio of the posterior probabilities.</p>
</blockquote>
<p>The jump probability can be characterized as:</p>
<p><span class="math inline">\(Prob(jump) * Prob(accept)\)</span></p>
<p>The ratio of jump probabilities can be characterized as:</p>
<p><span class="math inline">\(\frac{Prob(jump_{b\rightarrow a})\cdot Prob(accept a|b)}{Prob(jump_{a\rightarrow b})\cdot Prob(accept b|a)}\)</span></p>
<p>This ratio MUST be equal to the ratio of the posterior probabilities:</p>
<p><span class="math inline">\(\frac{Posterior(A)}{Posterior(B)}\)</span></p>
<p>If this rule is met, then in the long run the chain will spend a lot of time occupying high-probability parts of parameter space. With enough jumps, the long-term distribution will match the joint posterior probability distribution.</p>
</div>
<div id="metropolis-hastings-algorithm" class="section level2">
<h2>Metropolis-Hastings algorithm</h2>
<p>This algorithm is very similar to the simulated annealing algorithm! The main difference: the “temperature” doesn’t decrease over time and the parameter <em>k</em> is set to 1.</p>
<p>The M-H algorithm can be expressed as:</p>
<p><span class="math inline">\(Prob(accept A|B) = min(1,\frac{Posterior(B)}{Posterior(A)}\cdot \frac{Prob(b\rightarrow a)}{Prob(a\rightarrow b)})\)</span></p>
<p>Note that essentially this is the same as the Metropolis simulated-annealing algorithm, with the posterior probabilities substituted for the likelihood and the <em>k</em> parameter set to 1</p>
<div id="myxomatosis-revisited-again" class="section level3">
<h3>Myxomatosis revisited (again!)</h3>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>Recall that we are modeling the distribution of measured titers (virus loads) for Australian rabbits. Bolker chose to use a Gamma distribution. Here is the empirical distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We need to estimate the gamma rate and shape parameters that best fit this empirical distribution. Here is one example of a Gamma fit to this distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Recall that the 2-D (log) likelihood surface looks something like this:</p>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Here is an implementation of the M-H algorithm to find the joint posterior distribution!</p>
<p>First, we need a likelihood function (our old friend!)</p>
<pre class="r"><code>GammaLikelihoodFunction &lt;- function(params){
  prod(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=F))
}

params &lt;- c(shape=40,scale=0.15) 
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] 2.906766e-22</code></pre>
<p>Then, we need a prior distribution for our parameters! Let’s assign relatively flat priors for both of our parameters. In this case, let’s assign a <span class="math inline">\(gamma(shape=0.01,scale=100)\)</span> for the shape parameter and a <span class="math inline">\(gamma(shape=0.1,scale=10)\)</span> distribution for the scale parameter:</p>
<pre class="r"><code>GammaPriorFunction &lt;- function(params){
  prior &lt;- c(shape=NA,scale=NA)
  prior[&#39;shape&#39;] &lt;- dgamma(params[&#39;shape&#39;],shape=0.001,scale=1000)
  prior[&#39;scale&#39;] &lt;- dgamma(params[&#39;scale&#39;],shape=0.01,scale=100)
  # prior[&#39;shape&#39;] &lt;- dunif(params[&#39;shape&#39;],3,100)
  # prior[&#39;scale&#39;] &lt;- dunif(params[&#39;scale&#39;],0.01,0.5)
  return(prod(prior))
}

curve(dgamma(x,shape=0.01,scale=1000),3,100)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>params &lt;- c(shape=40,scale=0.15) 
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaPriorFunction(params)</code></pre>
<pre><code>## [1] 1.502831e-06</code></pre>
<p>Note that we are also assuming (fairly standard assumption) that the shape and scale are independent in the prior (multiplicative probabilities for the joint prior)</p>
<p>Then, we need a function that can compute the ratio of posterior probabilities for any given jump in parameter space. Because we are dealing with a <em>ratio</em> of posterior probabilities, we do NOT need to compute the normalization constant. Without the need for a normalization constant, we just need to compute the ratio of weighted likelihoods (that is, the likelihood weighted by the prior)</p>
<pre class="r"><code>PosteriorRatio &lt;- function(oldguess,newguess){
  oldLik &lt;- max(1e-90,GammaLikelihoodFunction(oldguess))
  oldPrior &lt;- max(1e-90,GammaPriorFunction(oldguess))
  newLik &lt;- GammaLikelihoodFunction(newguess)
  newPrior &lt;- GammaPriorFunction(newguess)
  return((newLik*newPrior)/(oldLik*oldPrior))
}

oldguess &lt;- params
newguess &lt;- c(shape=39,scale=0.15)

PosteriorRatio(oldguess,newguess)</code></pre>
<pre><code>## [1] 0.01423757</code></pre>
<p>Then we need a function for making new guesses, or jumps in parameter space:</p>
<pre class="r"><code>     # function for making new guesses
newGuess &lt;- function(oldguess){
  sdshapejump &lt;- 4
  sdscalejump &lt;- 0.07
  jump &lt;- c(shape=rnorm(1,mean=0,sd=sdshapejump),scale=rnorm(1,0,sdscalejump))
  newguess &lt;- abs(oldguess + jump)
  return(newguess)
}
  # set a new &quot;guess&quot; near to the original guess

newGuess(oldguess=params)     # each time is different- this is the first optimization procedure with randomness built in</code></pre>
<pre><code>##      shape      scale 
## 38.9558275  0.1756608</code></pre>
<pre class="r"><code>newGuess(oldguess=params)</code></pre>
<pre><code>##      shape      scale 
## 40.5363219  0.1557664</code></pre>
<pre class="r"><code>newGuess(oldguess=params)</code></pre>
<pre><code>##       shape       scale 
## 39.50265801  0.06916286</code></pre>
<p>Okay, now we are ready to implement the Metropolis-Hastings MCMC algorithm:</p>
<p>First we need a starting point:</p>
<pre class="r"><code>startingvals &lt;- c(shape=75,scale=0.28)    # starting point for the algorithm</code></pre>
<p>Let’s play with the different functions we have so far…</p>
<pre class="r"><code>newguess &lt;- newGuess(startingvals)
newguess</code></pre>
<pre><code>##      shape      scale 
## 72.9776409  0.2814788</code></pre>
<pre class="r"><code>PosteriorRatio(startingvals,newguess)   # difference in posterior ratio</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Now let’s look at the Metropolis routine:</p>
<pre class="r"><code>chain.length &lt;- 100
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Let’s run it for longer…</p>
<pre class="r"><code>chain.length &lt;- 1000
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>This looks better! The search algorithm is finding the high-likelihood parts of parameter space pretty well!</p>
<p>Now, let’s look at the chain for the “shape” parameter</p>
<pre class="r"><code>plot(1:chain.length,guesses[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>And for the scale parameter…</p>
<pre class="r"><code>plot(1:chain.length,guesses[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Can we say that these chains have converged on the posterior distribution for the shape parameter??</p>
<p>First of all, the beginning of the chain “remembers” the starting value, and is therefore not a stationary distribution. We need to remove the first part of the chain, called the <strong>‘burn-in’</strong>.</p>
<pre class="r"><code>burn.in &lt;- 100
MCMCsamples &lt;- guesses[-c(1:burn.in),]

chain.length=chain.length-burn.in
plot(1:chain.length,MCMCsamples[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<p>Let’s run it for even longer…</p>
<pre class="r"><code>chain.length &lt;- 10000
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)

counter &lt;- 1
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Let’s first remove the first 1000 samples as a burn-in</p>
<pre class="r"><code>burn.in &lt;- 1000
MCMCsamples &lt;- guesses[-c(1:burn.in),]
chain.length=chain.length-burn.in</code></pre>
<p>Now, let’s look at the chains again</p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
<p>When evaluating these trace plots, we are hoping to see a “stationary distribution” that looks like white noise. This trace plot looks like it might have a little autocorrelation. One way to “fix” this is to thin the MCMC samples:</p>
<pre class="r"><code>thinnedMCMC &lt;- MCMCsamples[seq(1,chain.length,by=10),]
plot(1:nrow(thinnedMCMC),thinnedMCMC[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>plot(1:nrow(thinnedMCMC),thinnedMCMC[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<p>Now we can examine our posterior distribution!</p>
<pre class="r"><code>plot(density(thinnedMCMC[,&#39;scale&#39;]),main=&quot;scale parameter&quot;,xlab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>plot(density(thinnedMCMC[,&#39;shape&#39;]),main=&quot;shape parameter&quot;,xlab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
