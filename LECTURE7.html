<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 746" />


<title>Bayesian Analysis #2: MCMC</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
    <li>
      <a href="GIT_tutorial.html">Demo: version control in Git</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Bayesian Analysis #2: MCMC</h1>
<h4 class="author">NRES 746</h4>
<h4 class="date">Fall 2023</h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a
href="LECTURE7.R">click here</a> for the companion R script for this
lecture.</p>
<div id="markov-chain-monte-carlo" class="section level3">
<h3>Markov Chain Monte Carlo</h3>
<p>MCMC is a special type of random number generator that is designed to
sample from difficult-to-describe (e.g., multivariate, hierarchical)
probability distributions. In many/most cases, the posterior
distribution for ecological problems is a very difficult-to-describe
probability distribution.</p>
<p>MCMC is kind of magical in that it allows you to sample from
probability distributions that are impossible to fully define
mathematically! The MCMC approach uses random jumps in parameter space
that eventually end up sampling from the posterior distribution! The key
to MCMC is the following:</p>
<blockquote>
<p>The probability of a successful jump in parameter space from point A
to point B is proportional to the ratio of the posterior probabilities
at these two points.</p>
</blockquote>
<p>The probability of a successful jump in parameter space can be
characterized as:</p>
<p><span class="math inline">\(Prob(jump) * Prob(accept)\)</span></p>
<p>That is: in order to jump, you need to <em>propose</em> a specific
jump (according to some probability distribution) and then accept that
jump!</p>
<p>The ratio of jump probabilities can be characterized as:</p>
<p><span class="math inline">\(\frac{Prob(jump_{a\rightarrow b})\cdot
Prob(accept b|a)}{Prob(jump_{b\rightarrow a})\cdot Prob(accept
a|b)}\)</span></p>
<p>For this procedure to (eventually) extract random samples from the
posterior distribution, <em>this ratio MUST be equal to the ratio of the
posterior probabilities at points A and B</em>:</p>
<p><span
class="math inline">\(\frac{Posterior(B)}{Posterior(A)}\)</span></p>
<p>If this rule is met, then in the long run the chain will explore each
segment of parameter space in accordance with the posterior probability
of that segment- thereby eventually capturing the entire posterior
distribution (if run long enough).</p>
<p>Note: if the proposal distribution is symmetrical, then the
probability of proposing a jump from A to B is the same as the
probability of proposing a jump from B to A. Therefore, the proposal
probabilities drop out of the above equation…</p>
<p>Amazingly, MCMC at its core is not that difficult to describe or even
to implement. Let’s look at a simple MCMC algorithm.</p>
</div>
<div id="metropolis-hastings-algorithm" class="section level2">
<h2>Metropolis-Hastings algorithm</h2>
<p>This algorithm is essentially the same as the simulated annealing
algorithm we discussed in the “optimization” lecture! The main
difference: the “temperature” doesn’t decrease over time and the
temperature parameter <em>k</em> is always set to 1.</p>
<p>The M-H algorithm can be expressed as:</p>
<p><span class="math inline">\(Prob(accept B|A) =
min(1,\frac{Posterior(B)}{Posterior(A)}\cdot \frac{Prob(b\rightarrow
a)}{Prob(a\rightarrow b)})\)</span></p>
<p><strong>Q</strong>: Does it make sense that this algorithm meets the
basic rule of MCMC, that is, that the ratio of jump probabilities
to/from any two points in parameter/model space is equal to the ratio of
posterior probabilities evaluated at those two points?</p>
</div>
<div id="bivariate-normal-example" class="section level2">
<h2>Bivariate normal example</h2>
<p>This example is modified from <a
href="http://www.mas.ncl.ac.uk/~ndjw1/teaching/sim/gibbs/gibbs.html">this
link by Darren Wilkinson</a></p>
<p>Remember that MCMC samplers are just a type of random number
generator. We can use a Metropolis-Hastings sampler to develop our own
random number generator for a simple known distribution. In this
example, we use a M-H Sampler to generate random numbers from a standard
bivariate normal probability distribution.</p>
<p>We don’t need an MCMC sampler for this simple example (we already
have ‘perfect’ random number generators, like the one below). One way to
do this would be to use the following code, which draws and visualizes
an arbitrary number of independent samples from the bivariate standard
normal distribution with a correlation parameter, <span
class="math inline">\(\rho\)</span>.</p>
<pre class="r"><code># Simple example of MCMC sampling -----------------------

# first, let&#39;s build a function that generates random numbers from a bivariate standard normal distribution

rbvn&lt;-function (n, rho)   #function for drawing an arbitrary number of independent samples from the bivariate standard normal distribution. 
{
        x &lt;- rnorm(n, 0, 1)
        y &lt;- rnorm(n, rho * x, sqrt(1 - rho^2))
        cbind(x, y)
}

# Now, plot the random draws from this distribution, make sure this makes sense!

bvn&lt;-rbvn(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<pre class="r"><code># Metropolis-Hastings implementation of bivariate normal sampler... 

library(mvtnorm)    # load a package that allows us to compute probability densities for mv normal distribution 

metropolisHastings &lt;- function (n, rho=0.98){    # an MCMC sampler implementation of a bivariate random number generator
    mat &lt;- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
    x &lt;- 0   # initial values for all parameters
    y &lt;- 0
    prev &lt;- dmvnorm(c(x,y),mean=c(0,0),sigma = matrix(c(1,rho,rho,1),ncol=2))   # probability density of the distribution at the starting values
    mat[1, ] &lt;- c(x, y)        # initialize the markov chain
    counter &lt;- 1
    while(counter&lt;=n) {
      newx &lt;- rnorm(1,x,0.5)     # make a jump. Note the symmetrical proposal distribution
      newy &lt;- rnorm(1,y,0.5)
      
      newprob &lt;- dmvnorm(c(newx,newy),sigma = matrix(c(1,rho,rho,1),ncol=2))    # assess whether the new jump is good!
      ratio &lt;- newprob/prev   # compute the ratio of probabilities at the old (jump from) and proposed (jump to) locations. 
      
      prob.accept &lt;- min(1,ratio)     # decide the probability of accepting the new jump!
      rand &lt;- runif(1)
      if(rand&lt;=prob.accept){
        x=newx;y=newy    # set x and y to the new location
        mat[counter,] &lt;- c(x,y)    # store this in the storage array 
        counter=counter+1
        prev &lt;- newprob    # get ready for the next iteration
      }
      
    }
    return(mat)
}</code></pre>
<p>Then we can use the M-H sampler to get random samples from this known
distribution…</p>
<pre class="r"><code># Test the new M-H sampler

bvn&lt;-metropolisHastings(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p><strong>Q</strong>: Why does the MCMC routine produce ‘lower quality’
random numbers than the first (bivariate normal) sampler we built?</p>
<p>Okay, enough with super simple examples- let’s try it for a
non-trivial problem, like the Myxomatosis example from the Bolker
book!</p>
<div id="myxomatosis-revisited-again" class="section level3">
<h3>Myxomatosis revisited (again!)</h3>
<pre class="r"><code># MCMC implementation of the Myxomatosis example from the Bolker book --------------

library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>Recall that we are modeling the distribution of measured titers
(virus loads) for Australian rabbits. Bolker chose to use a Gamma
distribution. Here is the empirical distribution:</p>
<pre class="r"><code># Visualize the Myxomatosis data for the 100th time!

hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We need to estimate the gamma shape and scale parameters that best
fit this empirical distribution. Here is one example of a Gamma fit to
this distribution:</p>
<pre class="r"><code># ... and overlay a proposed data-generating model (gamma distribution)

hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Recall that the 2-D (log) likelihood surface looks something like
this:</p>
<pre class="r"><code># define 2-D parameter space!

shapevec &lt;- seq(3,100,by=0.1)   
scalevec &lt;- seq(0.01,0.5,by=0.001)

# define the likelihood surface  -------------

GammaLogLikelihoodFunction &lt;- function(params){
  sum(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=T))
}
surface2D &lt;- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable

newparams &lt;- c(shape=50,scale=0.2)
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  for(j in 1:length(scalevec)){
    newparams[&#39;scale&#39;] &lt;- scalevec[j]
    surface2D[i,j] &lt;- GammaLogLikelihoodFunction(newparams) 
  }
}

# Visualize the likelihood surface

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Here is an implementation of the M-H algorithm to find the joint
posterior distribution!</p>
<p>First, we need a likelihood function (our old friend!)- this time, we
will return real probabilities– NOT log-transformed probabilities</p>
<pre class="r"><code># Write a non-log-transformed likelihood function ------------

GammaLikelihoodFunction &lt;- function(params){
  prod(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=F))   
}

  # and here&#39;s the log likelihood function
GammaLogLikelihoodFunction &lt;- function(params){
  sum(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=T))   
}

params &lt;- c(shape=40,scale=0.15) 
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] 2.906766e-22</code></pre>
<pre class="r"><code>GammaLogLikelihoodFunction(params)</code></pre>
<pre><code>## [1] -49.58983</code></pre>
<p>Then, we need a prior distribution for our parameters! Let’s assign
relatively flat priors for both of our parameters. In this case, let’s
assign a <span
class="math inline">\(gamma(shape=0.001,scale=1000)\)</span> for both
parameters (mean of 1 and very large variance):</p>
<pre class="r"><code># Function for returning the prior probability density for any point in parameter space 

GammaPriorFunction &lt;- function(params){
  prior &lt;- c(shape=NA,scale=NA)
  prior[&#39;shape&#39;] &lt;- dgamma(params[&#39;shape&#39;],shape=0.001,scale=1000)
  prior[&#39;scale&#39;] &lt;- dgamma(params[&#39;scale&#39;],shape=0.001,scale=1000)
  # prior[&#39;shape&#39;] &lt;- dunif(params[&#39;shape&#39;],3,100)        # alternative: could use uniform prior!
  # prior[&#39;scale&#39;] &lt;- dunif(params[&#39;scale&#39;],0.01,0.5)
  return(prod(prior))
}

GammaLogPriorFunction &lt;- function(params){
  prior &lt;- c(shape=NA,scale=NA)
  prior[&#39;shape&#39;] &lt;- dgamma(params[&#39;shape&#39;],shape=0.001,scale=1000,log=T)
  prior[&#39;scale&#39;] &lt;- dgamma(params[&#39;scale&#39;],shape=0.001,scale=1000,log=T)
  # prior[&#39;shape&#39;] &lt;- dunif(params[&#39;shape&#39;],3,100)        # alternative: could use uniform prior!
  # prior[&#39;scale&#39;] &lt;- dunif(params[&#39;scale&#39;],0.01,0.5)
  return(sum(prior))
}

curve(dgamma(x,shape=0.001,scale=1000),3,100)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>params &lt;- c(shape=40,scale=0.15) 
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaPriorFunction(params)</code></pre>
<pre><code>## [1] 1.583765e-07</code></pre>
<pre class="r"><code>prior2D &lt;- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable

newparams &lt;- c(shape=50,scale=0.2)
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  for(j in 1:length(scalevec)){
    newparams[&#39;scale&#39;] &lt;- scalevec[j]
    prior2D[i,j] &lt;- GammaPriorFunction(newparams) 
  }
}

# Visualize the prior likelihood surface

image(x=shapevec,y=scalevec,z=prior2D,zlim=c(0.000000001,0.001),col=topo.colors(12))</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<pre class="r"><code>#contour(x=shapevec,y=scalevec,z=prior2D,levels=c(-30,-40,-80,-500),add=T)</code></pre>
<p>Note that we are also assuming that the shape and scale are
<em>independent</em> in the prior (multiplicative probabilities for the
joint prior). However, this does not impose the same assumption on the
posterior- parameter correlations in the likelihood can and will be
reflected in the posterior distribution.</p>
<p>Then, we need a function that can compute the ratio of posterior
probabilities for any given jump in parameter space. Because we are
dealing with a <em>ratio</em> of posterior probabilities, <em>we do NOT
need to compute the normalization constant</em>.</p>
<p>Without the need for a normalization constant, we just need to
compute the ratio of weighted likelihoods (that is, the likelihood
weighted by the prior)</p>
<pre class="r"><code># Function for computing the ratio of posterior densities -----------------

PosteriorRatio &lt;- function(oldguess,newguess){
  oldLik &lt;- max(1e-90,GammaLikelihoodFunction(oldguess))   # compute likelihood and prior density at old guess
  oldPrior &lt;- max(1e-90,GammaPriorFunction(oldguess))
  newLik &lt;- GammaLikelihoodFunction(newguess)             # compute likelihood and prior density at new guess
  newPrior &lt;- GammaPriorFunction(newguess)
  return((newLik*newPrior)/(oldLik*oldPrior))          # compute ratio of weighted likelihoods
}

PosteriorRatio2 &lt;- function(oldguess,newguess){
  oldLogLik &lt;- GammaLogLikelihoodFunction(oldguess)   # compute likelihood and prior density at old guess
  oldLogPrior &lt;- GammaLogPriorFunction(oldguess)
  newLogLik &lt;- GammaLogLikelihoodFunction(newguess)             # compute likelihood and prior density at new guess
  newLogPrior &lt;- GammaLogPriorFunction(newguess)
  return(exp((newLogLik+newLogPrior)-(oldLogLik+oldLogPrior)))          # compute ratio of weighted likelihoods
}

oldguess &lt;- params
newguess &lt;- c(shape=39,scale=0.15)

PosteriorRatio(oldguess,newguess)</code></pre>
<pre><code>## [1] 0.01423757</code></pre>
<pre class="r"><code>PosteriorRatio2(oldguess,newguess)</code></pre>
<pre><code>## [1] 0.01423757</code></pre>
<p>Then we need a function for making new guesses, or jumps in parameter
space:</p>
<pre class="r"><code># Define proposal distribution --------------------------
    #for jumps in parameter space (use normal distribution)!

     # function for making new guesses
newGuess &lt;- function(oldguess){
  sdshapejump &lt;- 4
  sdscalejump &lt;- 0.07
  jump &lt;- c(shape=rnorm(1,mean=0,sd=sdshapejump),scale=rnorm(1,0,sdscalejump))
  newguess &lt;- abs(oldguess + jump)    # note: by taking the abs val to avoid negative numbers, our proposal jump probs are not strictly symmetrical, but this should not present a big issue in practice
  return(newguess)
}
  # set a new &quot;guess&quot; near to the original guess

newGuess(oldguess=params)   </code></pre>
<pre><code>##       shape       scale 
## 38.32598095  0.04727806</code></pre>
<pre class="r"><code>newGuess(oldguess=params)</code></pre>
<pre><code>##       shape       scale 
## 41.53852865  0.05725767</code></pre>
<pre class="r"><code>newGuess(oldguess=params)</code></pre>
<pre><code>##      shape      scale 
## 46.8468931  0.1689683</code></pre>
<p>Now we are ready to implement the Metropolis-Hastings MCMC
algorithm:</p>
<p>We need a starting point:</p>
<pre class="r"><code># Set a starting point in parameter space -------------------

startingvals &lt;- c(shape=75,scale=0.28)    # starting point for the algorithm</code></pre>
<p>Let’s play with the different functions we have so far…</p>
<pre class="r"><code># Try our new functions  ------------------

newguess &lt;- newGuess(startingvals)    # take a jump in parameter space
newguess</code></pre>
<pre><code>##      shape      scale 
## 71.6866159  0.1617311</code></pre>
<pre class="r"><code>PosteriorRatio2(startingvals,newguess)   # difference in posterior ratio</code></pre>
<pre><code>## [1] 8.73916e+291</code></pre>
<p>Now let’s look at the Metropolis-Hastings routine:</p>
<pre class="r"><code># Visualize the Metropolis-Hastings routine: ---------------

chain.length &lt;- 11
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)
guesses[1,] &lt;- startingvals
counter &lt;- 2
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio2(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Let’s run it for longer…</p>
<pre class="r"><code># Get more MCMC samples --------------

chain.length &lt;- 100
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)
guesses[1,] &lt;- startingvals

counter &lt;- 2
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio2(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>How about for even longer??</p>
<pre class="r"><code># And more... -------------------

chain.length &lt;- 1000
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)
guesses[1,] &lt;- startingvals

counter &lt;- 2
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio2(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>This looks better! The search algorithm is finding the
high-likelihood parts of parameter space pretty well!</p>
<p>Now, let’s look at the chain for the “shape” parameter</p>
<pre class="r"><code># Evaluate &quot;traceplot&quot; for the MCMC samples... ---------------------

## Shape parameter

plot(1:chain.length,guesses[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>And for the scale parameter…</p>
<pre class="r"><code>## Scale parameter

plot(1:chain.length,guesses[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Can we say that these chains have converged on the posterior
distribution for the shape parameter??</p>
<p>First of all, the beginning of the chain “remembers” the starting
value, and is therefore not a stationary distribution. We need to remove
the first part of the chain, called the <strong>‘burn-in’</strong>.</p>
<pre class="r"><code># Remove &quot;burn-in&quot; (allow MCMC routine some time to get to the posterior) --------------

burn.in &lt;- 100
MCMCsamples &lt;- guesses[-c(1:burn.in),]

chain.length=chain.length-burn.in
plot(1:chain.length,MCMCsamples[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<p>But it still doesn’t look all that great. Let’s run it for even
longer, and see if we get something that looks more like a proper random
number generator (white noise)…</p>
<pre class="r"><code># Try again- run for much longer ---------------------

chain.length &lt;- 20000
oldguess &lt;- startingvals
guesses &lt;- matrix(0,nrow=chain.length,ncol=2)
colnames(guesses) &lt;- names(startingvals)
guesses[1,] &lt;- startingvals

counter &lt;- 2
while(counter &lt;= chain.length){
  newguess &lt;- newGuess(oldguess)
  post.rat &lt;- PosteriorRatio2(oldguess,newguess)
  prob.accept &lt;- min(1,post.rat)
  rand &lt;- runif(1)
  if(rand&lt;=prob.accept){
    oldguess &lt;- newguess
    guesses[counter,] &lt;- newguess 
    counter=counter+1
  }
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Let’s first remove the first 5000 samples as a burn-in</p>
<pre class="r"><code># Use longer &quot;burn-in&quot; ------------------

burn.in &lt;- 5000
MCMCsamples &lt;- guesses[-c(1:burn.in),]
chain.length=chain.length-burn.in</code></pre>
<p>Now, let’s look at the chains again</p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>plot(1:chain.length,MCMCsamples[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<p>When evaluating these trace plots, we are hoping to see a “stationary
distribution” that looks like white noise. This trace plot looks like it
might have a little autocorrelation. One way to “fix” this is to thin
the MCMC samples:</p>
<pre class="r"><code># &quot;thin&quot; the MCMC samples  -----------------------

thinnedMCMC &lt;- MCMCsamples[seq(1,chain.length,by=5),]
plot(1:nrow(thinnedMCMC),thinnedMCMC[,&#39;shape&#39;],type=&quot;l&quot;,main=&quot;shape parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>plot(1:nrow(thinnedMCMC),thinnedMCMC[,&#39;scale&#39;],type=&quot;l&quot;,main=&quot;scale parameter&quot;,xlab=&quot;iteration&quot;,ylab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
<p>Now we can examine our posterior distribution!</p>
<pre class="r"><code># Visualize the posterior!

plot(density(thinnedMCMC[,&#39;scale&#39;]),main=&quot;scale parameter&quot;,xlab=&quot;scale&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>plot(density(thinnedMCMC[,&#39;shape&#39;]),main=&quot;shape parameter&quot;,xlab=&quot;shape&quot;)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
<p>And we can visualize as before.</p>
<pre class="r"><code># More visual posterior checks... -----------------

par(mfrow=c(3,2))
plot(thinnedMCMC,col=1:10000)
plot(thinnedMCMC,type=&quot;l&quot;)
plot(ts(thinnedMCMC[,1]))
plot(ts(thinnedMCMC[,2]))
hist(thinnedMCMC[,1],40)
hist(thinnedMCMC[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>Hopefully it is clear that the Metropolis-Hastings MCMC method could
be modified to fit arbitrary numbers of free parameters for arbitrary
models. However, the M-H algorithm by itself is not necessarily the most
efficient. The <strong>Gibbs sampler</strong> tends to be a more
efficient sampler. In lab we will play around with Gibbs samplers,
mostly using a modeling language called <strong>BUGS</strong>
(<strong>B</strong>ayesian <strong>I</strong>nference
<strong>U</strong>sing <strong>G</strong>ibbs
<strong>S</strong>ampling).</p>
<p>NOTE: BUGS implementations (e.g., JAGS) actually tend to use a
combination of M-H and Gibbs sampling!</p>
<p>NOTE: M-H and Gibbs samplers aren’t the only MCMC routines out there.
For example, the popular program ‘stan’ uses a modification of M-H
sampling called ’Hamiltonian Monte Carlo”, which tends to explore
parameter space more efficiently</p>
</div>
</div>
<div id="gibbs-sampler" class="section level2">
<h2>Gibbs sampler</h2>
<p>The Gibbs sampler is amazingly straightforward and efficient.
Basically, the algorithm successively samples from the <em>full
conditional</em> probability distribution – that is, the posterior
distribution for arbitrary parameter <em>i</em> conditional on known
values for all other parameters in the model.</p>
<p>In many cases, we can’t work out the full posterior distribution for
our model directly, but we <strong>CAN</strong> work out the conditional
posterior distribution analytically if all parameters except for the
parameter in question were known with certainty. This is especially true
if we use <em>conjugate priors</em> for our model specification (what
BUGS/JAGS tries to do!). Even if not, the full conditional is often
analytically tractable. Nonetheless, even if it’s not analytically
tractable, we can use a M-H procedure as a “brute force” last
resort!</p>
<p><strong>Q</strong>: Why is a Gibbs sampler usually much more
efficient than a pure M-H sampler?</p>
</div>
<div id="bivariate-normal-example-1" class="section level2">
<h2>Bivariate normal example</h2>
<p>Again, remember that MCMC samplers are just a type of random number
generator. We can use a Gibbs sampler to develop our own random number
generator for a fairly simple known distribution. In this example (same
as before), we use a Gibbs Sampler to generate random numbers from a
standard bivariate normal probability distribution. Notice that the
Gibbs sampler is in many ways more simple and straightforward than the
M-H algorithm.</p>
<pre class="r"><code># Simple example of a Gibbs sampler ----------------

# first, recall our simple bivariate normal sampler

rbvn&lt;-function (n, rho){  #function for drawing an arbitrary number of independent samples from the bivariate standard normal distribution. 
        x &lt;- rnorm(n, 0, 1)
        y &lt;- rnorm(n, rho * x, sqrt(1 - rho^2))
        cbind(x, y)
}

bvn&lt;-rbvn(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<pre class="r"><code># Now construct a Gibbs sampler alternative ---------------

gibbs&lt;-function (n, rho){    # a gibbs sampler implementation of a bivariate random number generator
    mat &lt;- matrix(ncol = 2, nrow = n)   # matrix for storing the random samples
    x &lt;- 0
    y &lt;- 0
    mat[1, ] &lt;- c(x, y)        # initialize the markov chain
    for (i in 2:n) {
            x &lt;- rnorm(1, rho * y, sqrt(1 - rho^2))        # sample from x conditional on y
            y &lt;- rnorm(1, rho * x, sqrt(1 - rho^2))        # sample from y conditional on x
            mat[i, ] &lt;- c(x, y)
    }
    mat
}</code></pre>
<p>Then we can use the Gibbs sampler to get random samples from this
known distribution…</p>
<pre class="r"><code># Test the Gibbs sampler ------------------

bvn&lt;-gibbs(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000)
plot(bvn,type=&quot;l&quot;)
plot(ts(bvn[,1]))
plot(ts(bvn[,2]))
hist(bvn[,1],40)
hist(bvn[,2],40)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
<p>There is quite a bit of apparent autocorrelation in the samples of
the Markov chain here. Gibbs samplers frequently have this issue!</p>
<div id="back-to-myxomatosis" class="section level3">
<h3>Back to Myxomatosis!</h3>
<div id="aside-the-bugs-language" class="section level4">
<h4>Aside: the BUGS language</h4>
<p>Finally, let’s build a Gibbs sampler for our favorite Myxomatosis
example! To do this, we will use the BUGS language (as implemented in
JAGS), to help us!</p>
<p>The BUGS language looks similar to R, but there are several key
differences:</p>
<ul>
<li>First of all, BUGS is a compiled language, so the order of
operations in your code doesn’t really matter</li>
<li>BUGS is not vectorized- you need to use FOR loops for just about
everything!</li>
<li>Several probability distributions are parameterized very differently
in BUGS. Notably, the normal distribution is parameterized with a mean
and a precision (<span class="math inline">\(1/Variance\)</span>).</li>
</ul>
<p>Here is the myxomatosis example, as implemented in the BUGS
language:</p>
<p>NOTE: this code looks a little like R, but don’t be confused- this is
not R!</p>
<pre><code>
model {
  
  #############
  # LIKELIHOOD
  ############
  for(obs in 1:n.observations){
    titer[obs] ~ dgamma(shape,rate)
  }
  
  #############
  # PRIORS
  ############
  shape ~ dgamma(0.001,0.001)
  scale ~ dgamma(0.001,0.001)
  rate &lt;- 1/scale   # convert the scale parameter to a &quot;rate&quot; for BUGS
}
</code></pre>
<p>We can use the “cat” function in R to write out this model to a text
file in your working directory:</p>
<pre class="r"><code># Myxomatosis example in BUGS modeling language ---------------

# Write the BUGS model to file

cat(&quot;
  model {
    
    #############
    # LIKELIHOOD
    ############
    for(obs in 1:n.observations){
      titer[obs] ~ dgamma(shape,rate)
    }
    
    #############
    # PRIORS
    ############
    shape ~ dgamma(0.001,0.001)
    scale ~ dgamma(0.001,0.001)
    rate &lt;- 1/scale
  }
&quot;, file=&quot;BUGSmodel.txt&quot;)</code></pre>
<p>Now that we have the BUGS model packaged as a text file, we bundle
the data into a single list object that contains all the relevant data
referenced in the BUGS code:</p>
<pre class="r"><code># Encapsulate the data into a single &quot;list&quot; object ------------------

myx.data.for.bugs &lt;- list(
  titer = Myx$titer,
  n.observations = length(Myx$titer)
)

myx.data.for.bugs</code></pre>
<pre><code>## $titer
##  [1] 5.207 5.734 6.613 5.997 6.612 6.810 5.930 6.501 7.182 7.292 7.819 7.489
## [13] 6.918 6.808 6.235 6.916 4.196 7.682 8.189 7.707 7.597 7.112 7.354 7.158
## [25] 7.466 7.927 8.499
## 
## $n.observations
## [1] 27</code></pre>
<p>Then we need to define the initial values for all parameters. It is
convenient to define this as a function, so that each MCMC chain can be
initialized with different starting values. This will become clear
later!</p>
<pre class="r"><code># Function for generating random initial values --------------

init.vals.for.bugs &lt;- function(){
  list(
    shape=runif(1,20,100),
    scale=runif(1,0.05,0.3)
  )
}

init.vals.for.bugs()</code></pre>
<pre><code>## $shape
## [1] 29.34063
## 
## $scale
## [1] 0.1670234</code></pre>
<pre class="r"><code>init.vals.for.bugs()</code></pre>
<pre><code>## $shape
## [1] 35.34258
## 
## $scale
## [1] 0.09382479</code></pre>
<pre class="r"><code>init.vals.for.bugs()</code></pre>
<pre><code>## $shape
## [1] 42.82044
## 
## $scale
## [1] 0.2720501</code></pre>
<p>Now we can call JAGS!</p>
<pre class="r"><code># Run JAGS!!!!  ------------------

#library(R2jags)
library(jagsUI)

library(coda)

params.to.store &lt;- c(&quot;shape&quot;,&quot;scale&quot;)

jags.fit &lt;- jags(model=&quot;BUGSmodel.txt&quot;,data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,n.iter=5000,n.chains = 3,n.adapt = 100,n.burnin = 0)</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 27
##    Unobserved stochastic nodes: 2
##    Total graph size: 33
## 
## Initializing model
## 
## Adaptive phase, 100 iterations x 3 chains 
## If no progress bar appears JAGS has decided not to adapt 
##  
## No burn-in specified 
##  
## Sampling from joint posterior, 5000 iterations x 3 chains 
##  
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code>jagsfit.mcmc &lt;- jags.fit$samples   # extract &quot;MCMC&quot; object

summary(jagsfit.mcmc)</code></pre>
<pre><code>## 
## Iterations = 101:5100
## Thinning interval = 1 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean       SD  Naive SE Time-series SE
## shape    49.1477 13.22589 0.1079890       1.852842
## scale     0.1518  0.04294 0.0003506       0.005888
## deviance 77.3206  1.88662 0.0154042       0.097958
## 
## 2. Quantiles for each variable:
## 
##              2.5%     25%     50%     75%   97.5%
## shape    27.45608 39.3182 47.5946 57.8432 76.8916
## scale     0.09013  0.1196  0.1456  0.1763  0.2521
## deviance 75.38302 75.9368 76.7888 78.1168 82.2395</code></pre>
<pre class="r"><code>plot(jagsfit.mcmc)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
</div>
<div id="assessing-convergence" class="section level3">
<h3>Assessing convergence</h3>
<p>This is probably a good time to talk about convergence of MCMC chains
on the stationary posterior distribution. The above plots don’t look
great. We want to see white noise, and we want to see chains that look
similar to one another.</p>
<p>The first check is just visual- we look for the following to assess
convergence:</p>
<ul>
<li>The chains for each parameter, when viewed as a “trace plot” should
look like white noise (fuzzy caterpillar!), or similar.</li>
<li>Multiple chains with different starting conditions should look the
same!!</li>
</ul>
<p>One way we might be able to do a better job here is to run the chains
for longer and discard the initial samples as a <em>burn in</em>!</p>
<p>We can also try to reduce serial autocorrelation by thinning our
chain- here we retain only 1 out of every 100 samples.</p>
<pre class="r"><code># Run the chains for longer! -----------------

jags.fit &lt;- jags(model=&quot;BUGSmodel.txt&quot;,data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,
                     n.iter = 100000,n.chains = 3,n.adapt = 1000,n.burnin = 10000,
                     n.thin=100,parallel=T )</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Beginning parallel processing using 3 cores. Console output will be suppressed.
## 
## Parallel processing completed.
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code>jagsfit.mcmc &lt;- jags.fit$samples   # convert to &quot;MCMC&quot; object (coda package)

summary(jagsfit.mcmc)</code></pre>
<pre><code>## 
## Iterations = 10100:1e+05
## Thinning interval = 100 
## Number of chains = 3 
## Sample size per chain = 900 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##             Mean     SD  Naive SE Time-series SE
## shape    47.6273 13.138 0.2528351       0.487647
## scale     0.1573  0.046 0.0008852       0.001747
## deviance 77.3699  1.990 0.0382902       0.048006
## 
## 2. Quantiles for each variable:
## 
##              2.5%     25%     50%     75%   97.5%
## shape    25.66686 38.4330 46.2101 55.7803 75.8888
## scale     0.09118  0.1246  0.1498  0.1806  0.2721
## deviance 75.38681 75.9127 76.7803 78.1943 82.8007</code></pre>
<pre class="r"><code>plot(jagsfit.mcmc)</code></pre>
<p><img src="LECTURE7_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Just visually, this looks better. Now we can use some more
quantitative convergence metrics.</p>
<div id="the-gelman-rubin-diagnostic" class="section level4">
<h4>The Gelman-Rubin diagnostic</h4>
<p>One simple and intuitive convergence diagnostic is the
<em>Gelman-Rubin diagnostic</em>, which assesses whether chains are more
different from one another than they should be on the basis of simple
Monte Carlo error:</p>
<pre class="r"><code># Run convergence diagnostics  ------------------

gelman.diag(jagsfit.mcmc)</code></pre>
<pre><code>## Potential scale reduction factors:
## 
##          Point est. Upper C.I.
## shape          1.01       1.02
## scale          1.01       1.03
## deviance       1.01       1.03
## 
## Multivariate psrf
## 
## 1.01</code></pre>
<p>In general, values of 1.1 or higher are considered poorly converged.
Compute the G-R diagnostic for all free parameters in your model. If
your tests fail, you should try running longer chains!</p>
<p>So this model looks pretty good!!</p>
</div>
</div>
</div>
<div id="useful-links-for-mcmc" class="section level2">
<h2>Useful links for MCMC</h2>
<p><a
href="https://darrenjw.wordpress.com/2011/07/16/gibbs-sampler-in-various-languages-revisited/">Darren
Winkinson’s research blog</a></p>
<p><a href="LECTURE8.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
