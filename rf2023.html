<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Elena cox, Katarena Matos Meira, Mahipal Reddy Ramireddy" />


<title>Random Forest Algorithm</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #204a87; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #204a87; font-weight: bold; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #ce5c00; font-weight: bold; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>







<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
    <li>
      <a href="GIT_tutorial.html">Demo: version control in Git</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Student led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GLMM.html">GLMMs</a>
    </li>
    <li>
      <a href="GLMM_Lab.html">GLMMs (lab)</a>
    </li>
    <li>
      <a href="GAMs_Lab.html">GAMs (lab)</a>
    </li>
    <li>
      <a href="Rastering.html">RSFs (raster demo)</a>
    </li>
    <li>
      <a href="RSF_Lab.html">RSFs (lab)</a>
    </li>
    <li>
      <a href="rf2023.html">Random Forest (demo)</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Random Forest Algorithm</h1>
<h4 class="author">Elena cox, Katarena Matos Meira, Mahipal Reddy
Ramireddy</h4>

</div>


<p>R code for this demo is available <a href="rf2023.R">here</a></p>
<div id="decision-tree" class="section level1">
<h1>Decision Tree</h1>
<p>We are loading the following libraries in order to perform the
analysis.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(rpart)          <span class="co"># library to plot a decision tree.</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)   <span class="co"># RandomForest library </span></span></code></pre></div>
<pre><code>## randomForest 4.7-1.1</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">library</span>(rfUtilities)    <span class="co"># library to use utility functions on Random forest model to analyse model perfomance and evaluation.</span></span></code></pre></div>
<pre><code>## Warning: package &#39;rfUtilities&#39; was built under R version 4.3.2</code></pre>
<p>Loading the dataset “Iris” from R.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="fu">head</span>(iris)</span></code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<p>We are building a decision tree with the Iris</p>
<p><strong>rpart(formula, data, weights, subset, na.action = na.rpart,
method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost,
…)</strong></p>
<ul>
<li><p><strong>cp - </strong>is complexity parameter which would be
checked for at every node to continue futher growing the tree. It is
difference variance.</p></li>
<li><p><strong>minsplit -</strong> is the parameter controlling the
growth by determining no.of observation to be present at each node to
proceed with the split.</p></li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># method is classification for categorical variable species</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co">#control- To control the growth of tree</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co">#  - cp - is complexity parameter which would be checked for at every node to continue futher growing the tree. It is difference variance.</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co">#  - minsplit - is the parameter controlling the growth by determining no.of observation to be present at each node to proceed with the split.</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species<span class="sc">~</span>.,<span class="at">method =</span> <span class="st">&#39;class&#39;</span>,<span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp=</span><span class="dv">0</span>,<span class="at">minsplit =</span> <span class="dv">1</span>),<span class="at">data =</span> iris)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="fu">par</span>(<span class="at">xpd=</span> <span class="cn">NA</span>) <span class="co"># setting the plot parameter not to expand(To avoid text being cut out at the corners)</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="fu">plot</span>(tree)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="co"># adding the text to the tree</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a><span class="co"># use.n =T to plot the number of obs assosicated with each class at each node.</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="fu">text</span>(tree,<span class="at">use.n =</span> T) </span></code></pre></div>
<p><img src="rf2023_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="pruned-tree" class="section level1">
<h1>Pruned tree:</h1>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species<span class="sc">~</span>.,<span class="at">method =</span> <span class="st">&#39;class&#39;</span>,<span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp=</span><span class="fl">0.05</span>,<span class="at">minsplit =</span> <span class="dv">1</span>),<span class="at">data =</span> iris)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">xpd=</span> <span class="cn">NA</span>)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="fu">plot</span>(tree)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co"># adding the text to the tree</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="co"># use.n =T to plot the number of obs assosicated with each class at each node.</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="fu">text</span>(tree,<span class="at">use.n =</span> T) </span></code></pre></div>
<p><img src="rf2023_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="random-forest-classification" class="section level1">
<h1>Random Forest Classification</h1>
<p>Iris dataset has 150 observations with 5 variables.</p>
<ol style="list-style-type: decimal">
<li>Species - factor variable.</li>
<li>Sepal.Length.</li>
<li>Sepal.Width.</li>
<li>Petal.Length.</li>
<li>Petal.Width.</li>
</ol>
<p><strong>Random Forest:</strong> The random forest model is an
ensemble tree-based learning algorithm; that is, the algorithm averages
predictions over many individual trees. The individual trees are built
on bootstrap samples rather than on the original sample.This is called
bootstrap aggregating or simply bagging, and it reduces
overfitting.classification is based on the majority vote of all the
members (trees in forest).Many poor learners can collectively be a good
learner.</p>
<div class="float">
<img src="bagging.png" alt="Bootstrap aggregation and bagging" />
<div class="figcaption">Bootstrap aggregation and bagging</div>
</div>
<p><strong>Boot Strap agreggating or Bagging:</strong></p>
<pre><code>
for i in 1 to B do
Draw a bootstrap sample of size N from the training data;
  while node size != minimum node size do
  randomly select a subset of m predictor variables from total p;
    for j in 1 to m do
      if jth predictor optimizes splitting criterion then
        split internal node into two child nodes;
        break;
      end
    end
  end
end
return the ensemble tree of all B subtrees generated in the outer for loop;

</code></pre>
<ol style="list-style-type: decimal">
<li><strong>Node Splitting:</strong>
<ul>
<li>During the construction of a decision tree within a Random Forest,
the algorithm recursively splits nodes based on certain criteria, such
as the Gini impurity or information gain.</li>
</ul></li>
<li><strong>Minimum Node Size:</strong>
<ul>
<li>At each node, the algorithm checks whether the number of
observations in the node is greater than or equal to a specified minimum
node size. If the number of observations falls below this threshold, the
node is not split further, and it becomes a terminal node (leaf).</li>
</ul></li>
<li><strong>Effect on Tree Complexity:</strong>
<ul>
<li>A smaller minimum node size allows the tree to be more detailed and
fit the training data more closely. However, smaller nodes can also lead
to overfitting, especially if the tree is capturing noise in the
data.</li>
</ul></li>
<li><strong>Regularization:</strong>
<ul>
<li>By increasing the minimum node size, you are regularizing the tree.
Larger terminal nodes can result in a simpler tree that generalizes
better to new, unseen data.</li>
</ul></li>
</ol>
<H3>
Now we train a random forest model using Iris data.
<H3>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>) <span class="co">#setting the intial value for Random number generator</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> .,<span class="at">data =</span> iris,<span class="at">mtry =</span> <span class="dv">4</span>,<span class="at">ntrees =</span> <span class="dv">100</span>,<span class="at">proximity=</span><span class="cn">TRUE</span>,<span class="at">importance=</span> <span class="cn">TRUE</span> )</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="fu">print</span>(rf)</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = Species ~ ., data = iris, mtry = 4, ntrees = 100,      proximity = TRUE, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 4%
## Confusion matrix:
##            setosa versicolor virginica class.error
## setosa         50          0         0        0.00
## versicolor      0         47         3        0.06
## virginica       0          3        47        0.06</code></pre>
</div>
<div id="plotting-the-out-of-bagoob-error-rate" class="section level1">
<h1>Plotting the Out of Bag(OOB) Error rate</h1>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">plot</span>(rf) <span class="co"># plotting OOB error rate of all three species based on no.of tree generated by Random Forest</span></span></code></pre></div>
<p><img src="rf2023_files/figure-html/unnamed-chunk-6-1.png" width="672" />
In the above Plot:</p>
<ol style="list-style-type: decimal">
<li>setosa - Red</li>
<li>versicolor - green</li>
<li>virginica - blue</li>
</ol>
<p>You can see that Setosa has 100% classification accuracy.</p>
</div>
<div id="tunerf-tune-randomforest-for-the-optimal-mtry-parameter"
class="section level1">
<h1>tuneRF: Tune randomForest for the optimal mtry parameter</h1>
<H5>
We are tuning the rf model by trying different step factor, mtry and
ntrees
</H5>
<p><strong>tuneRF(x, y, mtryStart, ntreeTry=50, stepFactor=2,
improve=0.05,</strong> <strong>trace=TRUE, plot=TRUE, doBest=FALSE,
…)</strong></p>
<p><strong>Arguments</strong></p>
<ul>
<li><p><strong>x -</strong> matrix or data frame of predictor
variables.</p></li>
<li><p><strong>y -</strong>response vector (factor for classification,
numeric for regression).</p></li>
<li><p><strong>mtryStart -</strong> starting value of mtry; default is
the same as in randomForest.</p></li>
<li><p><strong>ntreeTry -</strong> number of trees used at the tuning
step.</p></li>
<li><p><strong>stepFactor -</strong> at each iteration, mtry is inflated
(or deflated) by this value. <strong>stepfactor cannot exceed the no.of
predictor variables.</strong></p></li>
<li><p><strong>improve -</strong> the (relative) improvement in OOB
error must be by this much for the search to continue.</p></li>
<li><p><strong>trace -</strong> whether to print the progress of the
search</p></li>
<li><p><strong>plot -</strong> whether to plot the OOB error as function
of mtry</p></li>
<li><p><strong>doBest -</strong> whether to run a forest using the
optimal mtry found.</p>
<p><strong>- If doBest=FALSE (default)</strong>, it returns a matrix
whose first column contains the mtry values searched, and the second
column the corresponding OOB error.</p>
<p><strong>- If doBest=TRUE</strong>, it returns the randomForest object
produced with the optimal mtry.</p></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="fu">tuneRF</span>(iris[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)],iris<span class="sc">$</span>Species,<span class="at">mtryStart =</span> <span class="dv">3</span>,<span class="at">stepFactor =</span> <span class="dv">2</span>,<span class="at">trace =</span> <span class="cn">TRUE</span>,<span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## mtry = 3  OOB error = 4% 
## Searching left ...
## mtry = 2     OOB error = 5.33% 
## -0.3333333 0.05 
## Searching right ...
## mtry = 4     OOB error = 4% 
## 0 0.05</code></pre>
<p><img src="rf2023_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre><code>##       mtry   OOBError
## 2.OOB    2 0.05333333
## 3.OOB    3 0.04000000
## 4.OOB    4 0.04000000</code></pre>
</div>
<div
id="rf.crossvalidation-random-forest-classification-or-regression-model-cross-validation"
class="section level1">
<h1>rf.crossValidation: Random Forest Classification or Regression Model
Cross-validation</h1>
<p>Description- Implements a permutation test cross-validation for
Random Forests models</p>
<div class="float">
<img src="crossvalidation.png" alt="k-fold_cross_validation" />
<div class="figcaption">k-fold_cross_validation</div>
</div>
<p>Usage</p>
’‘’rf.crossValidation(x, xdata, ydata = NULL, p = 0.1, n = 99, seed =
NULL, normalize = FALSE, bootstrap = FALSE, trace = FALSE,…)’’’
<H4>
Arguments:
</H4>
<p><strong>x -</strong> random forest object</p>
<p><strong>xdata -</strong> x data used in model</p>
<p><strong>ydata -</strong> optional y data used in model, default is to
use x$y from model object</p>
<p><strong>p -</strong> Proportion data withhold (default p=0.10)</p>
<p><strong>n -</strong> Number of cross validations (default n=99)</p>
<p><strong>seed -</strong> Sets random seed in R global environment</p>
<p><strong>normalize -</strong> (FALSE/TRUE) For regression, should
rmse, mbe and mae be normalized using (max(y) - min(y))</p>
<p><strong>bootstrap -</strong> (FALSE/TRUE) Should a bootstrap sampling
be applied. If FALSE, an n-th percent withold will be conducted</p>
<p><strong>trace -</strong> Print iterations</p>
<H3>
Interpreting the output:
</H4>
<H4>
For Classification:
</H4>
<pre><code>cross.validation$cv.users.accuracy  - Class-level users accuracy for the subset cross validation data

cross.validation$cv.producers.accuracy -  Class-level producers accuracy for the subset cross validation data

cross.validation$cv.oob - Global and class-level OOB error for the subset cross validation data

model$model.users.accuracy -  Class-level users accuracy for the model

model$model.producers.accuracy Class-level producers accuracy for the model

model$model.oob  - Global and class-level OOB error for the model
</code></pre>
<H4>
For Regression :
</H4>
<pre><code>fit.var.exp - Percent variance explained from specified fit model

fit.mse - Mean Squared Error from specified fit model

y.rmse - Root Mean Squared Error (observed vs. predicted) from each Bootstrap iteration (cross-validation)

y.mbe - Mean Bias Error from each Bootstrapped model

y.mae - Mean Absolute Error from each Bootstrapped model

D - Test statistic from Kolmogorov-Smirnov distribution Test (y and estimate)

p.val - p-value for Kolmogorov-Smirnov distribution Test (y and estimate)

model.mse - Mean Squared Error from each Bootstrapped model

model.varExp - Percent variance explained from each Bootstrapped model
</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># Arguments</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="co"># x - random forest object</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="co"># xdata - x data used in model</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="co"># ydata - optional y data used in model, default is to use x$y from model object</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="co"># p - Proportion data withhold (default p=0.10)</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="co"># n - Number of cross validations (default n=99)</span></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="co"># seed - Sets random seed in R global environment</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="co"># normalize - (FALSE/TRUE) For regression, should rmse, mbe and mae be normalized using (max(y) - min(y))</span></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="co"># bootstrap - (FALSE/TRUE) Should a bootstrap sampling be applied. If FALSE, an n-th percent withold will be conducted</span></span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a><span class="co"># trace - Print iterations</span></span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a><span class="fu">rf.crossValidation</span>(<span class="at">x=</span> rf, <span class="at">xdata =</span> iris[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)],<span class="at">ydata =</span> iris<span class="sc">$</span>Species,<span class="at">p =</span> <span class="fl">0.2</span>, <span class="at">n =</span> <span class="dv">99</span>, <span class="at">seed =</span> <span class="dv">123</span>)</span></code></pre></div>
<pre><code>## running: classification cross-validation with 99 iterations</code></pre>
<pre><code>## Classification accuracy for cross-validation 
##  
##                    setosa versicolor virginica
## users.accuracy        100        100       100
## producers.accuracy    100        100       100
##  
## Cross-validation Kappa = 0.9255 
## Cross-validation OOB Error = 0.04964539 
## Cross-validation error variance = 7.699906e-05 
##  
##  
## Classification accuracy for model 
##  
##                    setosa versicolor virginica
## users.accuracy        100       93.6      91.5
## producers.accuracy    100       91.7      93.5
##  
## Model Kappa = 0.9255 
## Model OOB Error = 0.04964539 
## Model error variance = 5.457125e-05</code></pre>
<ul>
<li><p>A model is trained using k-1 of the folds as training
data;</p></li>
<li><p>The resulting model is validated on the remaining part of the
data (i.e., it is used as a test set to compute a performance measure
such as accuracy).</p></li>
</ul>
<H3>
For Classification Problems:
</H3>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="fu">print</span>(rf<span class="sc">$</span>confusion)</span></code></pre></div>
<pre><code>##            setosa versicolor virginica class.error
## setosa         50          0         0        0.00
## versicolor      0         47         3        0.06
## virginica       0          3        47        0.06</code></pre>
<ol style="list-style-type: decimal">
<li><strong>Total Observed Accuracy (PCC):</strong>
<ul>
<li>This is the percentage of correctly classified observations out of
the total number of observations.</li>
<li>The sum of the diagonal of the confusion matrix represents correctly
classified observations</li>
<li>Formula:<span class="math display">\[ \text{PCC} =
\frac{\text{Number of Correct Observations}}{\text{Total Number of
Observations}} \]</span></li>
</ul></li>
<li><strong>User’s Accuracy:</strong>
<ul>
<li>Represents the error of commission(inclusion), i.e., observations
erroneously included in a given class i,e False Positive .</li>
<li>The commission errors are represented by row sums of the
matrix.</li>
<li>Formula: <span class="math display">\[ \text{User&#39;s Accuracy} =
\frac{\text{Number of Correct}}{\text{Total Number of Correct and
Comission Errors}} \]</span></li>
</ul></li>
<li><strong>Producer’s Accuracy:</strong>
<ul>
<li>Corresponds to the error of omission(exclusion), i.e., observations
erroneously excluded from a given class,i,e False Neagitive.</li>
<li>The omission errors are represented by column sums of the
matrix.</li>
<li>Formula: <span class="math display">\[ \text{Producer&#39;s
Accuracy} = \frac{\text{Number of Correct}}{\text{Total Number of
Correct and Omission Errors}} \]</span></li>
</ul></li>
<li><strong>Kappa Statistic:</strong></li>
</ol>
<p>The Kappa statistic, also known as Cohen’s Kappa, is a
chance-corrected metric used to assess the level of agreement between
the observed and expected classifications in a classification problem.
It’s particularly useful when dealing with imbalanced datasets or when
accuracy alone might be misleading. In the context of cross-validation,
Kappa can help account for chance agreement beyond just the observed
accuracy.</p>
<p><strong>Kappa Statistic Calculation:</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Observed Agreement <span
class="math inline">\((O)\)</span>:</strong>
<ul>
<li><span class="math inline">\((O)\)</span> is the proportion of
observed instances that were classified in the same categoryi,e
Correctly classified.</li>
<li><span class="math display">\[ O = \frac{\text{Sum of Diagonal
Elements in Confusion Matrix}}{\text{Total Number of Observations}}
\]</span></li>
</ul></li>
<li><strong>Expected Agreement <span
class="math inline">\((E)\)</span>:</strong>
<ul>
<li><span class="math inline">\((E)\)</span> is the proportion of
instances that would be expected to be classified in the same category
by chance.</li>
<li><span class="math display">\[E = \frac{\text{Sum of Row Totals}
\times \text{Sum of Column Totals}}{\text{(Total Number of
Observations)}^2} \]</span></li>
</ul></li>
<li><strong>Kappa Statistic (<span
class="math inline">\((\kappa)\)</span>):</strong>
<ul>
<li>The Kappa statistic is then calculated using the formula:</li>
<li><span class="math display">\[\kappa = \frac{O - E}{1 - E}
\]</span></li>
<li><span class="math inline">\((\kappa)\)</span> ranges from -1 to 1,
where 1 indicates perfect agreement, 0 indicates agreement equivalent to
chance, and negative values indicate agreement worse than chance.</li>
</ul></li>
</ol>
<p>Here’s how the Kappa statistic is interpreted:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Perfect Agreement (k = 1):</strong> If ?? equals 1, it
indicates perfect agreement between the two raters or classifiers. This
means that the observed agreement is exactly what would be expected by
chance, and there is no disagreement.</p></li>
<li><p><strong>No Agreement Beyond Chance (k = 0):</strong> If ?? equals
0, it suggests that the observed agreement is no better than what would
be expected by chance alone. In other words, any agreement observed is
purely due to random chance, and there is no systematic
agreement.</p></li>
<li><p><strong>Agreement Below Chance (k &lt; 0):</strong> It’s rare to
see a Kappa statistic less than zero, but it can happen. It suggests
that there is less agreement than would be expected by chance,
indicating a systematic disagreement between raters or
classifiers.</p></li>
<li><p><strong>Substantial Agreement (0.61 &lt;= k &lt;= 0.80):</strong>
Generally, a Kappa value between 0.61 and 0.80 is considered to indicate
substantial agreement. This suggests that there is agreement beyond what
would be expected by chance, though it may not be perfect.</p></li>
<li><p><strong>Moderate Agreement (0.41 &lt;= k &lt;= 0.60):</strong> A
Kappa value between 0.41 and 0.60 is considered to indicate moderate
agreement. This suggests a moderate level of agreement beyond
chance.</p></li>
<li><p><strong>Fair Agreement (0.21 &lt;= k &lt;= 0.40):</strong> A
Kappa value between 0.21 and 0.40 is considered fair agreement. This
suggests agreement beyond what would be expected by chance, but it is
still relatively modest.</p></li>
<li><p><strong>Slight Agreement (0.00 &lt;= k &lt;= 0.20):</strong> A
Kappa value between 0.00 and 0.20 is considered slight agreement. This
suggests minimal agreement beyond chance.</p></li>
</ol>
<p>The Kappa statistic in cross-validation helps assess the model’s
agreement beyond what would be expected by random chance, providing a
more robust measure of classification performance, particularly in
situations with imbalanced datasets.</p>
<H3>
For Regression Problems:
</H3>
<p>A Bootstrap is constructed and the subset models MSE and percent
variance explained is reported. Additionally, the RMSE between the
withheld response variable (y) and the predicted subset model</p>
<ol style="list-style-type: decimal">
<li><strong>Bootstrap:</strong>
<ul>
<li>A resampling technique where subsets of the dataset are sampled with
replacement to estimate the variability of a statistic.</li>
</ul></li>
<li><strong>MSE (Mean Squared Error):</strong>
<ul>
<li>A measure of the average squared difference between predicted and
actual values.</li>
<li>Formula: <span class="math display">\[ \text{MSE} = \frac{1}{n}
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></li>
<li>Where <span class="math inline">\((n)\)</span> is the number of
observations, <span class="math inline">\((y_i)\)</span> is the actual
response, and <span class="math inline">\((\hat{y}_i)\)</span> is the
predicted response.</li>
</ul></li>
<li><strong>Percent Variance Explained:</strong>
<ul>
<li>Indicates the proportion of the variance in the dependent variable
that is predictable from the independent variable(s).</li>
</ul></li>
<li><strong>RMSE (Root Mean Squared Error):</strong>
<ul>
<li>The square root of the MSE, providing an average measure of the
prediction error.</li>
<li>Formula: <span class="math display">\[\text{RMSE} =
\sqrt{\text{MSE}}\]</span></li>
</ul></li>
</ol>
</div>
<div id="variable-importance-values" class="section level1">
<h1>Variable Importance values</h1>
<H3>
importance(x, type=NULL, class=NULL, scale=TRUE, …)
</H3>
<H4>
Arguments
</H4>
<ul>
<li><strong>x -</strong> an object of class randomForest</li>
<li><strong>type -</strong> either 1 or 2, specifying the type of
importance measure (1=mean decrease in accuracy, 2=mean decrease in node
impurity).</li>
<li><strong>class -</strong> for classification problem, which
class-specific measure to return.</li>
<li><strong>scale -</strong> For permutation based measures, should the
measures be divided their ``standard errors’’?</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">importance</span>(rf)</span></code></pre></div>
<pre><code>##                setosa versicolor virginica MeanDecreaseAccuracy
## Sepal.Length  0.00000   7.014497  1.364325             6.701961
## Sepal.Width   0.00000  -4.499507  4.900252             1.302199
## Petal.Length 22.15382  37.385434 28.698477            33.560602
## Petal.Width  23.00254  35.556041 29.785807            32.362392
##              MeanDecreaseGini
## Sepal.Length         1.279315
## Sepal.Width          1.400030
## Petal.Length        45.226805
## Petal.Width         51.374250</code></pre>
<H4>
Interpreting the output
</H4>
<ul>
<li><p><strong>MeanDecreaseAccuracy </strong>The first measure is
computed from permuting OOB data: For each tree, the prediction error on
the out-of-bag portion of the data is recorded (error rate for
classification, MSE for regression). Then the same is done after
permuting each predictor variable. The difference between the two are
then averaged over all trees, and normalized by the standard deviation
of the differences. If the standard deviation of the differences is
equal to 0 for a variable, the division is not done (but the average is
almost always equal to 0 in that case).</p></li>
<li><p><strong>MeanDecreaseGini - </strong>The second measure is the
total decrease in node impurities from splitting on the variable,
averaged over all trees. For classification, the node impurity is
measured by the Gini index. For regression, it is measured by residual
sum of squares.</p></li>
</ul>
</div>
<div id="variables-used" class="section level1">
<h1>Variables Used</h1>
<H5>
We print the frequency of predictor variables used in the trees.
</H5>
<H3>
varUsed(x, by.tree=FALSE, count=TRUE)
</H3>
<H4>
Arguments:
</H4>
<p><strong>x -</strong> An object of class randomForest. <strong>by.tree
-</strong> Should the list of variables used be broken down by trees in
the forest? <strong>count -</strong> Should the frequencies that
variables appear in trees be returned?</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="fu">names</span>(iris)</span></code></pre></div>
<pre><code>## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot;  &quot;Species&quot;</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="fu">varUsed</span>(rf, <span class="at">by.tree=</span><span class="cn">FALSE</span>, <span class="at">count=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1]  364  408 1137 1089</code></pre>
</div>
<div id="variable-importance-plot" class="section level1">
<h1>Variable Importance Plot</h1>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf)</span></code></pre></div>
<p><img src="rf2023_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div
id="random-forest-probability-scaled-partial-dependency-plots-rf.partial.prob"
class="section level1">
<h1>Random Forest probability scaled partial dependency plots
(rf.partial.prob)</h1>
<p>Produces partial dependency plots with probability distribution based
on scaled margin distances.</p>
<pre><code>rf.partial.prob(x, pred.data, xname, which.class, w, prob = TRUE,
  plot = TRUE, smooth, conf = TRUE, smooth.parm = NULL,
  pts = FALSE, raw.line = FALSE, rug = FALSE, n.pt, xlab, ylab, main,
  ...)</code></pre>
<H4>
Arguments:
</H4>
<p><strong>x -</strong> Object of class randomForest</p>
<p><strong>pred.data -</strong> Training data.frame used for
constructing the plot,</p>
<p><strong>xname -</strong> Name of the variable for calculating partial
dependence</p>
<p><strong>which.class -</strong> The class to focus on</p>
<p><strong>w -</strong> Weights to be used in averaging (if not
supplied, mean is not weighted)</p>
<p><strong>prob -</strong> Scale distances to probabilities</p>
<p><strong>plot -</strong> (TRUE/FALSE) Plot results</p>
<p><strong>smooth -</strong> c(spline, loess) Apply spline.smooth or
loess to</p>
<p><strong>conf -</strong> (TRUE/FALSE) Should confidence intervals be
calculated for smoothing</p>
<p><strong>smooth.parm -</strong> An appropriate smoothing parameter
passed to loess or smooth.spline</p>
<p><strong>pts</strong>FALSE/TRUE) Add raw points</p>
<p><strong>raw.line -</strong> (FALSE/TRUE) Plot raw line
(non-smoothed)</p>
<p><strong>rug -</strong> Draw hash marks on plot representing deciles
of x</p>
<p><strong>n.pt -</strong> Number of points on the grid for evaluating
partial dependence.</p>
<p><strong>xlab -</strong> x-axis plot label</p>
<p><strong>ylab -</strong> y-axis plot label</p>
<p><strong>main -</strong> Plot label for main</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="fu">rf.partial.prob</span>(rf, <span class="at">pred.data =</span> iris, <span class="at">xname =</span> <span class="st">&#39;Petal.Width&#39;</span>,<span class="at">which.class =</span> <span class="st">&#39;setosa&#39;</span>, <span class="at">smooth =</span> <span class="st">&#39;spline&#39;</span> )</span></code></pre></div>
<p><img src="rf2023_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
