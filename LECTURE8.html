<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 746" />


<title>Model Selection</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
    <li>
      <a href="GIT_tutorial.html">Demo: version control in Git</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Model Selection</h1>
<h4 class="author">NRES 746</h4>
<h4 class="date">Fall 2023</h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a
href="LECTURE8.R">click here</a> for the companion R script for this
lecture.</p>
<p><strong>Model selection</strong> or <strong>model comparison</strong>
is a very common problem in ecology- that is, we often have multiple
competing hypotheses about how our data were generated and we want to
see which model is best supported by the available evidence.</p>
<p>If we can describe our data generating process explicitly as a set of
deterministic and stochastic components (i.e., likelihood functions),
then we can use likelihood-based methods (e.g., LRT, AIC, BIC, Bayesian
model selection) to infer which data generating model(s) could most
plausibly have generated our observed data.</p>
<div id="principle-of-parsimony-occams-razor" class="section level2">
<h2>Principle of Parsimony (“Occam’s razor”)</h2>
<p>We will discuss several alternative approaches to model selection in
ecology. However, all approaches follow the same basic principle- that –
all things equal, we should prefer the simpler model over any more
complex alternative. This is known as the principle of parsimony.</p>
<div id="example-data-balsam-fir-data-from-ny" class="section level3">
<h3>Example data: Balsam fir data from NY</h3>
<p>Bolker uses a study of balsam fir in New York to illustrate model
selection. Perhaps it’s time to move on from Myxomatosis anyway…</p>
<p>Let’s load up the data first</p>
<pre class="r"><code># Load the balsam fir dataset (finally, no more rabbits and virus titers!)

library(emdbook)
data(FirDBHFec)
fir &lt;- na.omit(FirDBHFec[,c(&quot;TOTCONES&quot;,&quot;DBH&quot;,&quot;WAVE_NON&quot;)])
fir$TOTCONES &lt;- round(fir$TOTCONES)
head(fir)</code></pre>
<pre><code>##   TOTCONES  DBH WAVE_NON
## 1       19  9.4        n
## 2       42 10.6        n
## 3       40  7.7        n
## 4       68 10.6        n
## 5        5  8.7        n
## 6        0 10.1        n</code></pre>
<p>We can examine the fecundity (total cones) as a function of the tree
size (DBH):</p>
<pre class="r"><code>plot(fir$TOTCONES ~ fir$DBH)   # fecundity as a function of tree size (diameter at breast height)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>One additional point of complexity in this data set- some trees were
sampled from areas that have undergone a recent wave-like die-off
(recruitment of balsam fir tends to occur after die-offs, resulting in
stands with most trees coming from the same cohort). Other trees were
sampled from areas that have not undergone recent die-offs.</p>
<pre class="r"><code># tree fecundity by size, categorized into two site-level categories: &quot;wave&quot; and &quot;non-wave&quot; 

ndx &lt;- fir$WAVE_NON==&quot;w&quot;   # logical vector indicating which observations were from &quot;wave&quot; sites
plot(fir$TOTCONES[ndx] ~ fir$DBH[ndx],xlab=&quot;DBH&quot;,ylab=&quot;Tot Cones&quot;)
points(fir$DBH[!ndx],fir$TOTCONES[!ndx],pch=4,col=&quot;red&quot;)
legend(&quot;topleft&quot;,pch=c(1,4),col=c(&quot;black&quot;,&quot;red&quot;),legend=c(&quot;Wave&quot;,&quot;Non-wave&quot;),bty=&quot;n&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Let’s assume (following Bolker) that fecundity increases as a
power-law relationship with DBH:</p>
<p><span class="math inline">\(\mu = a\cdot DBH^{b}\)</span></p>
<p>Let’s also assume that the fecundity follows a negative binomial
distribution:</p>
<p><span class="math inline">\(Y = NegBin(\mu,k)\)</span></p>
<p>We can model each of these parameters (<em>a</em>, <em>b</em>, and
<em>k</em>) separately for trees from wave and non-wave populations.</p>
<p>We can also run simpler models in which these parameters are modeled
as the same for both populations.</p>
<p>Then we can ask the question: <strong>which model is the “best
model”?</strong></p>
<div id="full-model" class="section level4">
<h4>FULL MODEL</h4>
<p>Here is a likelihood function for the <em>full model</em> – that is,
the most complex model (6-dimensional likelihood surface):</p>
<pre class="r"><code># build likelihood function for the full model: CONES ~ negBINOM( a(wave)*DBH^b(wave), dispersion(wave))

   
NegBinomLik_full &lt;- function(params){
  wave.code &lt;- as.numeric(fir$WAVE_NON)      # convert to ones and twos    # note: we are hard-coding the data into our likelihood function here!
  a &lt;- c(params[1],params[2])[wave.code]     # a parameters (two for wave and one for non-wave)
  b &lt;- c(params[3],params[4])[wave.code]      # b parameter (two for wave and one for non-wave)
  k &lt;- c(params[5],params[6])[wave.code]       # over-dispersion parameters (two for wave and one for non-wave)
  expcones &lt;- a*fir$DBH^b   # expected number of cones (deterministic component)
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))     # add stochastic component: full data likelihood
}

params &lt;- c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1)

NegBinomLik_full(params)</code></pre>
<pre><code>## [1] 1762.756</code></pre>
<p>We can fit the full model using “optim” (using a derivative based
optimization routine), just like we have done before:</p>
<pre class="r"><code>#### Find the MLE -----------------------

MLE_full &lt;- optim(fn=NegBinomLik_full,par=c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1),method=&quot;L-BFGS-B&quot;)

MLE_full$par</code></pre>
<pre><code>##       a.n       a.w       b.n       b.w       k.n       k.w 
## 0.2875039 0.4083306 2.3554748 2.1487169 1.6545962 1.3250989</code></pre>
<pre class="r"><code>MLE_full$value</code></pre>
<pre><code>## [1] 1135.01</code></pre>
</div>
<div id="reduced-models" class="section level4">
<h4>REDUCED MODELS</h4>
<p>Let’s run a simpler model now. This time, let’s model the b parameter
as equal for wave and nonwave populations:</p>
<pre class="r"><code># build likelihood function for a reduced model: CONES ~ negBINOM( a(wave)*DBH^b, dispersion(wave))


NegBinomLik_constb &lt;- function(params){
  wave.code &lt;- as.numeric(fir$WAVE_NON)      # convert to ones and twos
  a &lt;- c(params[1],params[2])[wave.code]      # a parameters
  b &lt;- params[3]                              # b parameter (not a function of wave/nonwave)
  k &lt;- c(params[4],params[5])[wave.code]      # dispersion parameters
  expcones &lt;- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params &lt;- c(a.n=1,a.w=1,b=1,k.n=1,k.w=1)

NegBinomLik_constb(params)</code></pre>
<pre><code>## [1] 1762.756</code></pre>
<p>And we can fit the full model using “optim”:</p>
<pre class="r"><code>### Find the MLE

MLE_constb &lt;- optim(fn=NegBinomLik_constb,par=c(a.n=1,a.w=1,b=1,k.n=1,k.w=1),method=&quot;L-BFGS-B&quot;)

MLE_constb$par</code></pre>
<pre><code>##       a.n       a.w         b       k.n       k.w 
## 0.3477240 0.3217906 2.2699275 1.6530928 1.3230276</code></pre>
<pre class="r"><code>MLE_constb$value</code></pre>
<pre><code>## [1] 1135.134</code></pre>
<p>Let’s compute the <em>-2 times the log likelihood</em> (<span
class="math inline">\(-2*log(likelihood)\)</span>) of the two models at
the MLE – That is, we compute <span
class="math inline">\(-2*log(Lik@MLE)\)</span> for each alternative
model.</p>
<pre class="r"><code># compute -2*loglik for each model at the MLE

ms_full &lt;- 2*MLE_full$value     # this is 2 * min.nll = -2*logLik_at_MLE

ms_constb &lt;- 2*MLE_constb$value

ms_full</code></pre>
<pre><code>## [1] 2270.02</code></pre>
<pre class="r"><code>ms_constb</code></pre>
<pre><code>## [1] 2270.267</code></pre>
<p>Note here that the log-likelihood of the full model is lower
(“better”: that is, the data are more likely to be produced under this
model) than the the reduced model. <em>This should always be the
case</em>- if not, something is wrong. That is, the plausibility of
producing the observed data under the fitted model should
<em>always</em> increase when more free parameters are added!</p>
<p>This is where the principle of parsimony comes into play!</p>
<p>What if we wanted to test which model was better supported by the
evidence? One way is to use our old friend, the Likelihood Ratio Test
(LRT)!</p>
</div>
</div>
<div id="likelihood-ratio-test-frequentist-solution"
class="section level3">
<h3>Likelihood-ratio test (frequentist solution)</h3>
<p>We have encountered the LRT once before, in the context of generating
confidence intervals from likelihood surfaces (at and near the MLE). The
same principle applies for model selection. The LRT tests whether the
extra goodness-of-fit of the full model is worth the extra complexity of
the additional parameters.</p>
<p>As you recall, the likelihood ratio is defined (obviously) as the
ratio of two likelihoods. The numerator represents the likelihood of a
reduced model (fewer free parameters) that is nested within a full model
– which in turn serves as the denominator:</p>
<p><span
class="math inline">\(\frac{Likelihood_{reduced}}{Likelihood_{full}}\)</span></p>
<p>Because the raw likelihood ratio under the null hypothesis does not
have a known distribution, we first convert the likelihood ratio to
<strong>-2X the difference in log likelihoods</strong>:</p>
<p><span
class="math inline">\(-2ln(\frac{Likelihood_{reduced}}{Likelihood_{full}})\)</span></p>
<p>Which can also be written as:</p>
<p><span class="math inline">\(-2ln(Likelihood_{reduced}) -
-2ln(Likelihood_{full})\)</span></p>
<p>When expressed this way, this likelihood ratio statistic
(asymptotically) should be approximately Chi-squared distributed with df
equal to the number of fixed dimensions (difference in free parameters
between the full and reduced model).</p>
<p>The LRT can be used for <em>two-way model comparison</em> as long as
one model is nested within the other (full model vs. reduced model). If
the models are not nested then the LRT doesn’t really make sense.</p>
<pre class="r"><code># Likelihood-Ratio test (frequentist) -----------------------

Deviance &lt;- ms_constb - ms_full 
Deviance</code></pre>
<pre><code>## [1] 0.2467524</code></pre>
<pre class="r"><code>Chisq.crit &lt;- qchisq(0.95,1)
Chisq.crit</code></pre>
<pre><code>## [1] 3.841459</code></pre>
<pre class="r"><code>Deviance&gt;=Chisq.crit   # perform the LRT</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>1-pchisq(Deviance,1)   # p-value</code></pre>
<pre><code>## [1] 0.6193711</code></pre>
<pre class="r"><code># Visualize the likelihood ratio test- compare the observed deviance with the distribution of deviances expected under the null hypothesis

curve(dchisq(x,df=1),0,5)
abline(v=Deviance,col=&quot;red&quot;,lwd=4)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Clearly, the gain in model performance is not worth the extra
complexity in this case (the observed deviance could easily be produced
by random chance!). Therefore, we favor the reduced model.</p>
<p>What about if we try a different reduced model? This time, we decide
to fix the a, b, and k parameters, so the “wave” factor is not
considered.</p>
<pre class="r"><code># Try a different reduced model: CONES ~ negBINOM( a*DBH^b, dispersion)

NegBinomLik_nowave &lt;- function(params){
  a &lt;- params[1]      # a parameters
  b &lt;- params[2]      # b parameter (not a function of wave/nonwave)
  k &lt;- params[3]      # dispersion parameters
  expcones &lt;- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params &lt;- c(a=1,b=1,k=1)

NegBinomLik_nowave(params)</code></pre>
<pre><code>## [1] 1762.756</code></pre>
<p>And we use “optim” to locate the maximum likelihood estimate:</p>
<pre class="r"><code># Find the MLE

MLE_nowave &lt;- optim(fn=NegBinomLik_nowave,par=params,method=&quot;L-BFGS-B&quot;)

MLE_nowave$par</code></pre>
<pre><code>##         a         b         k 
## 0.3036727 2.3197228 1.5029500</code></pre>
<pre class="r"><code>MLE_nowave$value</code></pre>
<pre><code>## [1] 1136.015</code></pre>
<p>Now we can perform a LRT to see which model is better!</p>
<pre class="r"><code># Perform LRT -- this time with three fewer free parameters in the reduced model

ms_full &lt;- 2*MLE_full$value

ms_nowave &lt;- 2*MLE_nowave$value

Deviance &lt;- ms_nowave - ms_full 
Deviance</code></pre>
<pre><code>## [1] 2.009946</code></pre>
<pre class="r"><code>Chisq.crit &lt;- qchisq(0.95,df=3)   # now three additional params in the more complex model!
Chisq.crit</code></pre>
<pre><code>## [1] 7.814728</code></pre>
<pre class="r"><code>Deviance&gt;=Chisq.crit</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>1-pchisq(Deviance,df=3)   # p-value</code></pre>
<pre><code>## [1] 0.570345</code></pre>
<pre class="r"><code># Visualize the likelihood ratio test (test statistic and sampling distribution under the null)
curve(dchisq(x,df=3),0,15)
abline(v=Deviance,col=&quot;red&quot;,lwd=4)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Again, the difference in deviance does not justify the additional
parameters. This difference in deviance between the full and restricted
model could be produced easily by random chance.</p>
<p>Remember <em>this is a frequentist test</em>. The null hypothesis is
that there is no difference between the restricted model and the more
complex model. So we are imagining multiple alternative universes where
we are collecting thousands of datasets under the null hypothesis
(simpler model is correct– that is, the extra parameters are junk) and
determining a maximum likelihood estimate for the full model (with
meaningless free params) and a reduced model where we <em>fix</em> the
value of one or more of our junk free parameters at the
<strong>true</strong> parameter value of zero. Even though the data
generating process is the same each time, each dataset we collect will
yield a slightly different MLE for the full model and the reduced model.
The deviance (-2*log likelihood ratio) between the restricted model and
the full model (under the null hypothesis) should be chi-squared
distributed with df = number of dimensions that were “fixed”!</p>
<p>As you can imagine, there are a lot of pairwise comparisons that
could be generated, even in this simple example. For instance, there are
15 pairwise comparisons that could be produced from even this simple
example. What about more complex models? Clearly this can get a bit
unwieldy!</p>
<p>In addition, not all models we wish to compare will necessarily be
nested. For example, consider the model selection exercise we were
performing in lab- comparing the M-M fit to the Ricker fit. Since these
models aren’t nested, there is not clearly a “reduced” and a “full”
model and we can’t perform a LRT.</p>
</div>
<div id="information-theoretic-metrics" class="section level3">
<h3>Information-theoretic metrics</h3>
<p>Information-theoretic metrics for model comparison, like Akaike’s
Information Criterion (AIC), provide a way to get around the issues with
LRT. These metrics allow us to make tables for comparing multiple models
simultaneously. However, these metrics <em>have no strict frequentist
interpretation</em>.</p>
<p>Metrics like AIC represent (theoretically) the distance between some
particular model and the “perfect” or “true” model (which fits the data
perfectly). Information-theoretic metrics are composed of a
negative-log-likelihood component (e.g., -2Ln(L), in which lower values
mean better fit) and a <em>parsimony penalty term</em> (for which lower
values mean more parsimony). For AIC, the likelihood component is (<span
class="math inline">\(-2*logL\)</span>) and the penalty term is twice
the number of parameters (<span class="math inline">\(2*k\)</span>). The
models with the lowest values of the information criterion are considere
the best model (marrying good fit and parsimony)</p>
<div id="akaikes-information-criterion-aic" class="section level4">
<h4>Akaike’s Information Criterion (AIC)</h4>
<p>AIC is computed using the following equation:</p>
<p><span class="math inline">\(AIC = -2 \cdot log\cal L +
2k\)</span></p>
<p>AIC is the most commonly used information criterion.</p>
<p>logL is the log-likelhood at the MLE</p>
<p>k is the number of parameters in the model</p>
<p>As with all information-theoretic metrics, we look for the model
associated with the minimum value (lowest value corresponds with best
model fit). This is the “best model”! So simple!</p>
<p>For small sample sizes, Burnham and Anderson (2002) recommend that a
finite-size correction should be used:</p>
<p><span class="math inline">\(AIC_c = AIC +
\frac{2k(k+1)}{n-k-1}\)</span></p>
<p>A common <em>rule of thumb</em> is that models within 2 AIC units (or
AICc units) of the best model are “reasonable” (does this “rule of 2”
sound familiar?)</p>
<p>However, some statisticians caution that models within ca. 7 AIC
units of the best model can be useful and may warrant further
consideration!</p>
</div>
<div id="schwarz-information-criterion-bic" class="section level4">
<h4>Schwarz information criterion (BIC)</h4>
<p>Another common I-T metric is the Schwarz, or <em>Bayesian</em>
information criterion. The penalty term for BIC is (log n)*k.</p>
<p><span class="math inline">\(BIC = -2logL + (log(n))\cdot
k\)</span></p>
<p>In general, BIC is more conservative than AIC- that is, more likely
to select the simpler model (since the penalty term is generally
greater).</p>
<p>Although the Schwarz criterion has a Bayesian justification (as does
AIC), it is computed from a point estimate (using the log-likelihood at
the MLE) and so it doesn’t pass any real test for being Bayesian – true
Bayesian analyses don’t treat parameters as points, but as full
probability distributions.</p>
</div>
<div id="aic-in-action" class="section level4">
<h4>AIC in action</h4>
<p>Let’s return to the fir fecundity model, and use AIC to select among
a set of models. Let’s first fit a couple more candidate models…</p>
<p>This time, we decide to fix the <em>a</em>, and <em>k</em>
parameters, so the “wave” factor is only considered for the <em>b</em>
parameter.</p>
<pre class="r"><code># Information-theoretic metrics for model-selection ------------------------------

# Akaike&#39;s Information Criterion (AIC)

#### First, let&#39;s build another likelihood function: whereby only the &quot;b&quot; parameter differs by &quot;wave&quot; sites

NegBinomLik_constak &lt;- function(params){
  wave.code &lt;- as.numeric(fir$WAVE_NON)      # convert to ones and twos
  a &lt;- params[1]                             # a parameters
  b &lt;- c(params[2],params[3])[wave.code]                              # b parameter (not a function of wave/nonwave)
  k &lt;- params[4]                               # dispersion parameters
  expcones &lt;- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params &lt;- c(a=1,b.n=1,b.w=1,k=1)  

NegBinomLik_constak(params)</code></pre>
<pre><code>## [1] 1762.756</code></pre>
<p>And we can fit the full model using “optim”:</p>
<pre class="r"><code>### Fit the new model

MLE_constak &lt;- optim(fn=NegBinomLik_constak,par=params)

MLE_constak$par</code></pre>
<pre><code>##         a       b.n       b.w         k 
## 0.3448975 2.2745907 2.2327297 1.5057655</code></pre>
<pre class="r"><code>MLE_constak$value</code></pre>
<pre><code>## [1] 1135.758</code></pre>
<pre class="r"><code>ms_constak &lt;- 2*MLE_constak$value</code></pre>
<p>Finally, let’s fit a model with no “wave” effect, but where we assume
the error is Poisson distributed…</p>
<pre class="r"><code>### Now, let&#39;s build and fit one more final model- this time with no wave effect and a Poisson error distribution

PoisLik_nowave &lt;- function(params){
  a &lt;- params[1]      # a parameters
  b &lt;- params[2]      # b parameter (not a function of wave/nonwave)
  expcones &lt;- a*fir$DBH^b
  -sum(dpois(fir$TOTCONES,lambda=expcones,log=TRUE))
}

params &lt;- c(a=1,b=1)

PoisLik_nowave(params)</code></pre>
<pre><code>## [1] 15972.6</code></pre>
<pre class="r"><code>MLE_pois &lt;- optim(fn=PoisLik_nowave,par=params)

MLE_pois$par</code></pre>
<pre><code>##         a         b 
## 0.2613297 2.3883860</code></pre>
<pre class="r"><code>MLE_pois$value</code></pre>
<pre><code>## [1] 3161.832</code></pre>
<pre class="r"><code>ms_pois &lt;- 2*MLE_pois$value</code></pre>
<p><strong>Note</strong> we could not compare the Poisson model to the
Negative Binomial model using LRT- one is not nested within the
other!</p>
<p>Now we can compare the five models we have run so far using AIC</p>
<pre class="r"><code># Compare all five models using AIC!

AIC_constak &lt;- ms_constak + 2*4
AIC_full &lt;- ms_full + 2*6
AIC_constb &lt;- ms_constb + 2*5
AIC_nowave &lt;- ms_nowave + 2*3
AIC_pois &lt;- ms_pois + 2*2

AICtable &lt;- data.frame(
  Model = c(&quot;Full&quot;,&quot;Constant b&quot;,&quot;Constant a and k&quot;,&quot;All constant&quot;,&quot;Poisson&quot;),
  AIC = c(AIC_full,AIC_constb,AIC_constak,AIC_nowave,AIC_pois),
  LogLik = c(ms_full/-2,ms_constb/-2,ms_constak/-2,ms_nowave/-2,ms_pois/-2),
  params = c(6,5,4,3,2),
  stringsAsFactors = F
)

AICtable$DeltaAIC &lt;- AICtable$AIC-AICtable$AIC[which.min(AICtable$AIC)]

AICtable$Weights &lt;- round(exp(-0.5*AICtable$DeltaAIC) / sum(exp(-0.5*AICtable$DeltaAIC)),3)

AICtable$AICc &lt;- AICtable$AIC + ((2*AICtable$params)*(AICtable$params+1))/(nrow(fir)-AICtable$params-1)

AICtable[order(AICtable$AIC),c(1,7,2,5,6,4,3)]</code></pre>
<pre><code>##              Model     AICc      AIC    DeltaAIC Weights params    LogLik
## 4     All constant 2278.131 2278.030    0.000000   0.516      3 -1136.015
## 3 Constant a and k 2279.684 2279.515    1.484647   0.246      4 -1135.758
## 2       Constant b 2280.521 2280.267    2.236807   0.169      5 -1135.134
## 1             Full 2282.378 2282.020    3.990054   0.070      6 -1135.010
## 5          Poisson 6327.714 6327.664 4049.633378   0.000      2 -3161.832</code></pre>
<p>This AIC table shows us that the simplest model is best! Despite the
fact that the deviance is lowest for the full model! (principle of
parsimony at work)</p>
</div>
</div>
</div>
<div id="bayesian-model-selection" class="section level2">
<h2>Bayesian Model Selection</h2>
<p>Can we do model selection in a Bayesian framework? The answer is
yes!</p>
<p>One metric that <em>is</em> used by Bayesians for model selection is
the <em>Bayes Factor</em> (below). The Bayes factor is defined as the
ratio of <em>marginal likelihoods</em>.</p>
<p>In addition, there are some I-T metrics for Bayesian analyses that
make model selection pretty much as easy as building an AIC table!</p>
<p>Note that BIC (Schwarz Information Criterion) is no more Bayesian
than AIC. Bayesians generally do not use BIC for model selection…</p>
<div id="bayes-factor" class="section level3">
<h3>Bayes Factor</h3>
<p>Recall that our I-T metrics, as well as likelihood ratio tests, used
the value of the likelihood surface at the MLE. That is, we are only
taking into account a single point on the likelihood surface to
represent what our data have to say about our model.</p>
<p>Bayesians compute a posterior distribution that takes into account
the entire likelihood surface (and the prior distribution)– that is, we
now are working with an entire posterior distribution rather than just a
single point.</p>
<p>The <em>marginal likelihood</em> represents the average data
likelihood across parameter space.</p>
<p><span class="math inline">\(\overline{\mathcal{L}} = \int
\mathcal{L}(x)\cdot Prior(x) dx\)</span></p>
<p>The marginal likelihood represents the <em>average quality of fit of
a given model to the data</em>.</p>
<p>This should look familiar! It is the denominator of Bayes rule – also
known as the probability of the data, or the <strong>model
evidence</strong>.</p>
<p>The higher the marginal likelihood, the more likely that model is to
produce the data.</p>
<p>The ratio of marginal likelihoods is known as the <strong>Bayes
factor</strong> and is an elegant (yet often troublesome in practice)
method for comparing models in a Bayesian context.</p>
<p><span class="math inline">\(\overline{\mathcal{L}}_1 /
\overline{\mathcal{L}}_2\)</span></p>
<p>This is interpreted as <em>the odds in favor of model 1 versus model
2</em></p>
<p>This simple formula elegantly penalizes over-parameterization.
Simpler models will generally have higher marginal likelihoods than more
complex models when all of parameter space is taken into account. We
have already seen why this might be. More complex models will always
have a higher likelihood at the MLE, but generally will have much lower
likelihoods in other parts of parameter space.</p>
<p>Interestingly, 2*logarithm of the Bayes factor (putting it on the
deviance scale) is comparable to AIC (with a fairly strong prior) and is
comparable to BIC (with a fairly weak prior). This argument, based on
Bayes factors, has been used to justify both AIC and BIC (and is why BIC
is called ‘Bayesian’).</p>
<p>In practice, computing marginal likelihoods can be tricky, involving
multi-dimensional integration! Recall that we can’t generally estimate
the denominator of Bayes rule for most ecological parameter estimation
problems– and Bayes factors are computed entirely from the denominators
of Bayesian parameter estimation problems!</p>
<div id="bayes-factor-example" class="section level4">
<h4>Bayes Factor Example</h4>
<p>A simple binomial distribution example can illustrate Bayes factors
quite nicely.</p>
<p>Imagine we conduct a tadpole experiment where we are interested in
estimating the mortality rate of some treatment, on the basis of the
number of dead tadpoles observed out of 10 in a tank. We are interested
in comparing a simple model where <em>p</em> is fixed at 0.5 (a ‘point
null’ model) with a more complex model where <em>p</em> is a <em>free
parameter</em> estimated from data.</p>
<p>First let’s look at the simple model.</p>
<pre class="r"><code># Bayes factor example  ---------------------

# take a basic binomial distribution with parameter p fixed at 0.5:

probs1 &lt;- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab=&quot;probability&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>## Q: What is the *marginal likelihood* under this simple model for an observation of 2 mortalities out of 10? 

## A:

dbinom(2,10,0.5)</code></pre>
<pre><code>## [1] 0.04394531</code></pre>
<p><strong>Q</strong> What is the <em>marginal likelihood</em> under
this simple model for an observation of 2 mortalities out of 10? How
about 3 mortalities?</p>
<p><strong>A</strong> It is exactly 0.0439453. That is, there is no
marginalizing to do since there is no free parameter to marginalize
over.</p>
<p>Now let’s consider a more ‘complex’ model, where <em>p</em> is a free
parameter (one additional free parameter relative to the simple model).
First, let’s assume that the parameter <em>p</em> is assigned a uniform
<span class="math inline">\(beta(1,1)\)</span> prior across parameter
space:</p>
<pre class="r"><code>## Now we can consider a model whereby &quot;p&quot; is a free parameter
curve(dbeta(x,1,1))  # uniform prior on &quot;p&quot;</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>What is the marginal likelihood of observing 2 mortalities under this
model?</p>
<p>In other words, what is the average probability of observing 2
mortalities, averaged across all possible values of <em>p</em>???</p>
<p>Intuitively, because all values of <em>p</em> are equally likely, all
possible observations (0 mortalities, 2 mortalities, N mortalities)
should all be equally likely! That is, neither the likelihood function
nor the prior distribution favors any particular observation (0 to 10)
over any other.</p>
<p>We can do this mathematically…</p>
<p>For two observed mortalities, the marginal likelihood is:</p>
<pre class="r"><code># Compute the marginal likelihood of observing 2 mortalities

# ?integrate
binom2 &lt;- function(x) dbinom(x=2,size=10,prob=x)
marginal_likelihood &lt;- integrate(f=binom2,0,1)$value    # use &quot;integrate&quot; function in R
marginal_likelihood  # equal to 0.0909 = 1/11</code></pre>
<pre><code>## [1] 0.09090909</code></pre>
<p>For three observed mortalities, the marginal likelihood (probability
of observing a “3” across all possible values of p) is:</p>
<pre class="r"><code># Compute the marginal likelihood of observing 3 mortalities

binom3 &lt;- function(x) dbinom(x=3,size=10,prob=x)
marginal_likelihood &lt;- integrate(f=binom3,0,1)$value    # use &quot;integrate&quot; function
marginal_likelihood   # equal to 0.0909 = 1/11</code></pre>
<pre><code>## [1] 0.09090909</code></pre>
<p>Basically, if p could be anything between 0 and 1, no particular
observation is favored over any other prior to observing any real
data:</p>
<pre class="r"><code># simulate data from the model across all possible values of the parameter &quot;p&quot;

lots=1000000
a_priori_data &lt;- rbinom(lots,10,prob=rbeta(lots,1,1))   # no particular observation is favored
for_hist &lt;- table(a_priori_data)/lots
barplot(for_hist,xlab=&quot;Potential Observation&quot;,ylab=&quot;Marginal likelihood&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Therefore, since there are 11 possible observations (outcomes of the
binomial distribution of size=10), the marginal likelihood of any
particular data observation under the model with p as a free parameter
should be 1/11 = 0.0909091.</p>
<p>Here is a visualization of the marginal likelihood across all
possible data observations:</p>
<pre class="r"><code># Visualize the marginal likelihood of all possible observations

probs2 &lt;- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab=&quot;probability&quot;,ylim=c(0,1))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Now let’s overlay the marginal likelihoods for the simpler model:</p>
<pre class="r"><code># Overlay the marginal likelihood of the simpler model, with p fixed at 0.5

probs2 &lt;- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab=&quot;probability&quot;,ylim=c(0,1))

probs1 &lt;- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab=&quot;probability&quot;,add=T,col=&quot;red&quot;,density=20)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Assuming we observed 2 mortalities, what is the Bayes Factor? Which
model is better?</p>
<pre class="r"><code># Finally, compute the bayes factor given that we observed 2 mortalities. Which model is better?

probs2 &lt;- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab=&quot;probability&quot;,ylim=c(0,1))

probs1 &lt;- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab=&quot;probability&quot;,add=T,col=&quot;red&quot;,density=20)

abline(v=3,col=&quot;green&quot;,lwd=4 )</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code>BayesFactor = (1/11)/dbinom(2,10,0.5)   
BayesFactor</code></pre>
<pre><code>## [1] 2.068687</code></pre>
<p>Here, the Bayes factor of around 2 indicates that the data lend
support to the more complex model!</p>
<p>What if the data instead were 3 mortalities? Which model is the best
model?</p>
<pre class="r"><code># Compute the bayes factor given that we observed 3 mortalities. Which model is better now?

probs2 &lt;- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab=&quot;probability&quot;,ylim=c(0,1))

probs1 &lt;- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab=&quot;probability&quot;,add=T,col=&quot;red&quot;,density=20)

abline(v=4.3,col=&quot;green&quot;,lwd=4 )</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code>BayesFactor = dbinom(3,10,0.5)/(1/11)
BayesFactor</code></pre>
<pre><code>## [1] 1.289063</code></pre>
<p>This time, <em>The simpler model wins</em>!!! That would never happen
if we compared maximum likelihood estimates- the deviance of the
(fitted) more highly-parameterized model will always be lower (i.e., the
data likelihood at p=0.3 is higher than the data likelihood at p=0.5)!
That’s why we have to penalize or <em>regularize</em> more
highly-parameterized models.</p>
<p>We can visualize this! First of all, what is the maximum likelihood
estimate for p under the model with 3 mortality observations?</p>
<pre class="r"><code># Visualize the likelihood ratio

# probs2 &lt;- rep(1/11,times=11)          
# names(probs2) = 0:10
# barplot(probs2,ylab=&quot;probability&quot;,ylim=c(0,1))

probs1 &lt;- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab=&quot;probability&quot;,col=&quot;red&quot;,density=20,ylim=c(0,1))

probs3 &lt;- dbinom(0:10,10,0.3)          
names(probs3) = 0:10
barplot(probs3,ylab=&quot;probability&quot;,add=T,col=&quot;green&quot;,density=10,angle = -25)

abline(v=4.3,col=&quot;green&quot;,lwd=4 )</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>So clearly the likelihood ratio favors the more complex model (fitted
parameter “p”) vs the simple, 0-parameter model!</p>
<p>What does the likelihood-ratio test say?</p>
<pre class="r"><code># LRT: simple model (p fixed at 0.5) vs complex model (p is free parameter)

Likelihood_simple &lt;- dbinom(3,10,0.5)
Likelihood_complex &lt;- dbinom(3,10,0.3)
Likelihood_simple</code></pre>
<pre><code>## [1] 0.1171875</code></pre>
<pre class="r"><code>Likelihood_complex</code></pre>
<pre><code>## [1] 0.2668279</code></pre>
<pre class="r"><code>-2*log(Likelihood_simple)--2*log(Likelihood_complex)</code></pre>
<pre><code>## [1] 1.645658</code></pre>
<pre class="r"><code>qchisq(0.95,1)</code></pre>
<pre><code>## [1] 3.841459</code></pre>
<pre class="r"><code>pchisq(1.64,1)    # very high p value, simpler model is preferred</code></pre>
<pre><code>## [1] 0.7996745</code></pre>
<p>What about AIC?</p>
<pre class="r"><code># AIC: simple model (p fixed at 0.5) vs complex model (p is free parameter)

AIC_simple &lt;- -2*log(Likelihood_simple) + 2*0
AIC_complex &lt;-  -2*log(Likelihood_complex) + 2*1

AIC_simple</code></pre>
<pre><code>## [1] 4.28796</code></pre>
<pre class="r"><code>AIC_complex    </code></pre>
<pre><code>## [1] 4.642303</code></pre>
<p>What about AICc?</p>
<p><span class="math inline">\(AIC_c = AIC +
\frac{2k(k+1)}{n-k-1}\)</span></p>
<pre class="r"><code>### Alternatively, use AICc

AICc_simple &lt;- -2*log(Likelihood_simple) + 0 + 0
AICc_complex &lt;-  -2*log(Likelihood_complex) + 1 + ((2*2)/(3-1-1))

AICc_simple</code></pre>
<pre><code>## [1] 4.28796</code></pre>
<pre class="r"><code>AICc_complex    </code></pre>
<pre><code>## [1] 7.642303</code></pre>
<p>What about BIC?</p>
<pre class="r"><code># Alternatively, try BIC

BIC_simple &lt;- -2*log(Likelihood_simple) + log(10)*0
BIC_complex &lt;-  -2*log(Likelihood_complex) + log(10)*1

BIC_simple</code></pre>
<pre><code>## [1] 4.28796</code></pre>
<pre class="r"><code>BIC_complex    </code></pre>
<pre><code>## [1] 4.944888</code></pre>
<p>All these methods give the same basic answer- that the simple model
is better, even though the complex model fits better!</p>
<p><strong>Q</strong> How is the principle of parsimony naturally
incorporated in the Bayes factor? That is, why don’t we need to impose a
penalty term?</p>
</div>
</div>
<div id="deviance-information-criterion-dic" class="section level3">
<h3>Deviance Information Criterion (DIC)</h3>
<p>DIC is computed by default in JAGS and WinBUGS. It is analogous to
other I-T metrics like AIC (and therefore easy to interpret and
use)!</p>
<p>… but it is often unreliable for complex hierarchical models, so use
caution when applying DIC to model selection problems…</p>
<p>Let’s run an example anyway…</p>
<p>First, let’s write a BUGS model for the fir data</p>
<pre class="r"><code># Bayesian model selection: Bolker&#39;s fir dataset

cat(&quot;

model  {

### Likelihood

  for(i in 1:n.obs){
    expected.cones[i] &lt;- a[wave[i]]*pow(DBH[i],b[wave[i]])   # power function: a*DBH^b
    p[i] &lt;- r[wave[i]] / (r[wave[i]] + expected.cones[i])
    observed.cones[i] ~ dnegbin(p[i],r[wave[i]])
  }
  
  
  ### Priors
  for(j in 1:2){   # estimate separately for wave and non-wave
    a[j] ~ dunif(0.001,2)
    b[j] ~ dunif(0.5,4)
    r[j] ~ dunif(0.5,5)
  }
  
}
    
&quot;,file=&quot;BUGS_fir.txt&quot;)</code></pre>
<p>Then we need to package the data for JAGS</p>
<pre class="r"><code># Package the data for JAGS

data.package1 &lt;- list(
  observed.cones = fir$TOTCONES,
  n.obs = nrow(fir),
  wave = as.numeric(fir$WAVE_NON),
  DBH = fir$DBH
)
#data.package</code></pre>
<p>Now we make a function for generating initial values:</p>
<pre class="r"><code># Make a function for generating initial guesses

init.generator1 &lt;- function(){ list(
  a = runif(2, 0.2,0.5),
  b = runif(2, 2,3),
  r = runif(2, 1,2)
  
  )
}
init.generator1()</code></pre>
<pre><code>## $a
## [1] 0.2639883 0.4827500
## 
## $b
## [1] 2.951072 2.872453
## 
## $r
## [1] 1.332014 1.474701</code></pre>
<p>Then we can run the model!</p>
<pre class="r"><code># Run the model in JAGS

library(jagsUI)    # load packages
library(coda)
library(lattice)

params.to.monitor &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;r&quot;)

jags.fit1 &lt;- jags(data=data.package1,inits=init.generator1,parameters.to.save=params.to.monitor,n.adapt=1000, n.iter=10000,model.file=&quot;BUGS_fir.txt&quot;,n.chains = 2,n.burnin = 2000,n.thin=5,parallel=TRUE )</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Beginning parallel processing using 2 cores. Console output will be suppressed.
## 
## Parallel processing completed.
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code>jagsfit1.mcmc &lt;- jags.fit1$samples   # extract &quot;MCMC&quot; object (coda package)

summary(jagsfit1.mcmc)</code></pre>
<pre><code>## 
## Iterations = 2005:10000
## Thinning interval = 5 
## Number of chains = 2 
## Sample size per chain = 1600 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##               Mean     SD Naive SE Time-series SE
## a[1]        0.5394 0.4056 0.007171       0.097679
## a[2]        0.6399 0.3691 0.006525       0.036243
## b[1]        2.1864 0.3150 0.005568       0.057089
## b[2]        2.0070 0.2852 0.005041       0.027698
## r[1]        1.6518 0.1979 0.003498       0.003621
## r[2]        1.3362 0.2035 0.003597       0.003651
## deviance 2276.7626 3.8485 0.068032       0.348040
## 
## 2. Quantiles for each variable:
## 
##               2.5%       25%       50%       75%    97.5%
## a[1]        0.1042    0.2712    0.4146    0.6669    1.748
## a[2]        0.1746    0.3711    0.5583    0.8110    1.623
## b[1]        1.5577    1.9824    2.1929    2.3801    2.819
## b[2]        1.4671    1.8178    1.9974    2.2021    2.577
## r[1]        1.2860    1.5126    1.6465    1.7759    2.057
## r[2]        0.9846    1.1930    1.3207    1.4649    1.766
## deviance 2271.4162 2273.8904 2276.0349 2278.8218 2286.123</code></pre>
<pre class="r"><code>#plot(jagsfit1.mcmc)</code></pre>
<pre class="r"><code># Visualize the model fit

plot(jagsfit1.mcmc)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-40-1.png" width="480" style="display: block; margin: auto;" /><img src="LECTURE8_files/figure-html/unnamed-chunk-40-2.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>lattice::densityplot(jagsfit1.mcmc)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-40-3.png" width="480" style="display: block; margin: auto;" /></p>
<p>First of all, is there any evidence that the dispersion of cone data
from wave sites is different than that of the non-wave sites?</p>
<pre class="r"><code>hist(jags.fit1$sims.list$r[,1],main=&quot;dispersion param&quot;,ylab=&quot;Prob Density&quot;,xlab=&quot;dispersion param&quot;,freq = F,ylim=c(0,2),xlim=c(0.5,2.5))
hist(jags.fit1$sims.list$r[,2],density=20,col=&quot;green&quot;,add=T,freq=F)
legend(&quot;topright&quot;,col=c(&quot;green&quot;,&quot;white&quot;),density=c(20,0),legend=c(&quot;wave&quot;,&quot;nonwave&quot;),bty=&quot;n&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>What is the DIC for this model?</p>
<pre class="r"><code># Extract the DIC for the full model!

DIC_full &lt;- jags.fit1$DIC
DIC_full</code></pre>
<pre><code>## [1] 2284.016</code></pre>
<p>Now let’s build the reduced model and compare DIC values!</p>
<pre class="r"><code># Build JAGS code for the reduced model --------------

cat(&quot;

model  {

### Likelihood

  for(i in 1:n.obs){
    expected.cones[i] &lt;- a*pow(DBH[i],b)   # a*DBH^b
    p[i] &lt;- r / (r + expected.cones[i])
    observed.cones[i] ~ dnegbin(p[i],r)
  }
  
  
  ### Priors
  
  a ~ dunif(0.001,2)
  b ~ dunif(0.5,4)
  r ~ dunif(0.5,5)

  
}
    
&quot;,file=&quot;BUGS_fir_reduced.txt&quot;)</code></pre>
<p>Then we need to package the data for JAGS</p>
<pre class="r"><code># Package data for JAGS

data.package2 &lt;- list(
  observed.cones = fir$TOTCONES,
  n.obs = nrow(fir),
  #wave = as.numeric(fir$WAVE_NON),
  DBH = fir$DBH
)</code></pre>
<p>Now we make a function for generating initial values:</p>
<pre class="r"><code># Function for generating initial guesses for all params

init.generator2 &lt;- function(){ list(
  a = runif(1, 0.2,0.5),
  b = runif(1, 2,3),
  r = runif(1, 1,2)
  
  )
}
init.generator2()</code></pre>
<pre><code>## $a
## [1] 0.307953
## 
## $b
## [1] 2.299594
## 
## $r
## [1] 1.667578</code></pre>
<p>Then we can run the model!</p>
<pre class="r"><code># Run the reduced model and visualize the JAGS fit

params.to.monitor &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;r&quot;)

jags.fit2 &lt;- jags(data=data.package2,inits=init.generator2,parameters.to.save=params.to.monitor,n.adapt=1000, n.iter=10000,model.file=&quot;BUGS_fir_reduced.txt&quot;,n.chains = 2,n.burnin = 2000,n.thin=5 )</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 242
##    Unobserved stochastic nodes: 3
##    Total graph size: 837
## 
## Initializing model
## 
## Adaptive phase, 1000 iterations x 2 chains 
## If no progress bar appears JAGS has decided not to adapt 
##  
## 
##  Burn-in phase, 2000 iterations x 2 chains 
##  
## 
## Sampling from joint posterior, 8000 iterations x 2 chains 
##  
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code>jagsfit2.mcmc &lt;- jags.fit2$samples   # &quot;MCMC&quot; object (coda package)

summary(jagsfit2.mcmc)</code></pre>
<pre><code>## 
## Iterations = 3005:11000
## Thinning interval = 5 
## Number of chains = 2 
## Sample size per chain = 1600 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##               Mean     SD Naive SE Time-series SE
## a           0.3906 0.1417 0.002505       0.012640
## b           2.2349 0.1695 0.002996       0.015070
## r           1.5062 0.1449 0.002562       0.002562
## deviance 2275.0823 2.4262 0.042890       0.095397
## 
## 2. Quantiles for each variable:
## 
##               2.5%       25%       50%       75%     97.5%
## a           0.1779    0.2847    0.3725    0.4766    0.7148
## b           1.9218    2.1119    2.2281    2.3504    2.5760
## r           1.2422    1.4056    1.4986    1.6005    1.8162
## deviance 2272.2667 2273.3250 2274.5216 2276.1849 2281.3700</code></pre>
<pre class="r"><code>plot(jagsfit2.mcmc[,&quot;a&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit2.mcmc[,&quot;b&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-46-2.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit2.mcmc[,&quot;r&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-46-3.png" width="672" /></p>
<pre class="r"><code>lattice::densityplot(jagsfit2.mcmc)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-47-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>What is the DIC for this model?</p>
<pre class="r"><code># Compute DIC

DIC_reduced &lt;- jags.fit2$DIC

DIC_reduced</code></pre>
<pre><code>## [1] 2278.023</code></pre>
<pre class="r"><code>DIC_full</code></pre>
<pre><code>## [1] 2284.016</code></pre>
<p>Is there a good reason to prefer the full model now?</p>
<p>What would happen if we re-ran the model? Would the DIC be the
same?</p>
<p>What would happen if we changed the priors? Would the DIC be the
same?</p>
<p>What about AIC? Is the AIC the same every time?</p>
</div>
<div id="widely-applicable-information-criterion-waic"
class="section level3">
<h3>Widely Applicable Information Criterion (WAIC)</h3>
<p>The WAIC (also known as the Wattanabe-Akaike information criterion,
or the ‘widely applicable information criterion’) metric is also
interpretable just like AIC, BIC and DIC and allows us to compare models
fitted in a Bayesian framework via MCMC.</p>
<p>The WAIC metric, while not computed by default in JAGS, is more
widely applicable more widely than DIC.</p>
<p>Let’s compute the WAIC for the above example, and see how it
compares! For this, we will use the “loo” package in R.</p>
<pre class="r"><code># Use WAIC for bayesian model selection!

library(loo)    # load the &quot;loo&quot; package, which allows us to compute WAIC from JAGS output&#39;


####
# First, re-make the JAGS code, this time recording the likelihood as a derived parameter

cat(&quot;

model  {

### Likelihood

  for(i in 1:n.obs){
    expected.cones[i] &lt;- a[wave[i]]*pow(DBH[i],b[wave[i]])   # power function: a*DBH^b
    p[i] &lt;- r[wave[i]] / (r[wave[i]] + expected.cones[i])
    observed.cones[i] ~ dnegbin(p[i],r[wave[i]])
    LogLik[i] &lt;- log(dnegbin(observed.cones[i],p[i],r[wave[i]]))   # add log likelihood computation for each observation!
  }
  
  
  ### Priors
  for(j in 1:2){   # estimate separately for wave and non-wave
    a[j] ~ dunif(0.001,2)
    b[j] ~ dunif(0.5,4)
    r[j] ~ dunif(0.5,5)
  }
  
}
    
&quot;,file=&quot;BUGS_fir.txt&quot;)


# Build JAGS code for the reduced model ------------

cat(&quot;

model  {

### Likelihood

  for(i in 1:n.obs){
    expected.cones[i] &lt;- a*pow(DBH[i],b)   # a*DBH^b
    p[i] &lt;- r / (r + expected.cones[i])
    observed.cones[i] ~ dnegbin(p[i],r)
    LogLik[i] &lt;- log(dnegbin(observed.cones[i],p[i],r))   # add log likelihood computation for each observation!
  }
  
  
  ### Priors
  
  a ~ dunif(0.001,2)
  b ~ dunif(0.5,4)
  r ~ dunif(0.5,5)

  
}
    
&quot;,file=&quot;BUGS_fir_reduced.txt&quot;)</code></pre>
<pre class="r"><code># re-fit the models

params.to.monitor &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;r&quot;,&quot;LogLik&quot;)    # now monitor the log likelihood

jags.fit1 &lt;- jags(data=data.package1,inits=init.generator1,parameters.to.save=params.to.monitor,n.adapt=1000,n.iter=10000,model.file=&quot;BUGS_fir.txt&quot;,n.chains = 2,n.burnin = 2000,n.thin=5 )</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 242
##    Unobserved stochastic nodes: 6
##    Total graph size: 1690
## 
## Initializing model
## 
## Adaptive phase, 1000 iterations x 2 chains 
## If no progress bar appears JAGS has decided not to adapt 
##  
## 
##  Burn-in phase, 2000 iterations x 2 chains 
##  
## 
## Sampling from joint posterior, 8000 iterations x 2 chains 
##  
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code>jags.fit2 &lt;- jags(data=data.package2,inits=init.generator2,parameters.to.save=params.to.monitor,n.adapt=1000,n.iter=10000,model.file=&quot;BUGS_fir_reduced.txt&quot;,n.chains = 2,n.burnin = 2000,n.thin=5 )</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 242
##    Unobserved stochastic nodes: 3
##    Total graph size: 1313
## 
## Initializing model
## 
## Adaptive phase, 1000 iterations x 2 chains 
## If no progress bar appears JAGS has decided not to adapt 
##  
## 
##  Burn-in phase, 2000 iterations x 2 chains 
##  
## 
## Sampling from joint posterior, 8000 iterations x 2 chains 
##  
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code># Compute WAIC!

loglik_full &lt;- jags.fit1$sims.list$LogLik
loglik_red &lt;- jags.fit2$sims.list$LogLik

waic_full &lt;- waic(loglik_full)</code></pre>
<pre><code>## Warning: 
## 1 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre class="r"><code>waic_red &lt;- waic(loglik_red)

waic_full$estimates[&quot;waic&quot;,]</code></pre>
<pre><code>##   Estimate         SE 
## 2283.15099   29.81704</code></pre>
<pre class="r"><code>waic_red$estimates[&quot;waic&quot;,]</code></pre>
<pre><code>##   Estimate         SE 
## 2278.35750   29.97966</code></pre>
<pre class="r"><code>loo_compare(waic_full, waic_red)</code></pre>
<pre><code>##        elpd_diff se_diff
## model2  0.0       0.0   
## model1 -2.4       2.0</code></pre>
</div>
<div id="explicit-bayesian-model-selection" class="section level3">
<h3>Explicit Bayesian model selection</h3>
<p>Another cool, sometimes useful, but not perfect, method of Bayesian
model selection is to write the model selection directly into the JAGS
code!</p>
<pre class="r"><code># Explicit Bayesian model selection

cat(&quot;

model  {

  ### Likelihood for model 1: full

  for(i in 1:n.obs){
    expected.cones[i,1] &lt;- a1[wave[i]]*pow(DBH[i],b1[wave[i]])       # a*DBH^b
    spread.cones[i,1] &lt;- r1[wave[i]]
    p[i,1] &lt;- spread.cones[i,1] / (spread.cones[i,1] + expected.cones[i,1])
    observed.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
    predicted.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
    SE_obs[i,1] &lt;- pow(observed.cones[i,1]-expected.cones[i,1],2)
    SE_pred[i,1] &lt;- pow(predicted.cones[i,1]-expected.cones[i,1],2)
  }
  
  
  ### Priors, model 1
  for(j in 1:2){   # estimate separately for wave and non-wave
    a1[j] ~ dunif(0.001,2)
    b1[j] ~ dunif(0.5,4)
    r1[j] ~ dunif(0.5,5)
  }

  ### Likelihood for model 2: reduced

  for(i in 1:n.obs){
    expected.cones[i,2] &lt;- a2*pow(DBH[i],b2)       # a*DBH^b
    spread.cones[i,2] &lt;- r2
    p[i,2] &lt;- spread.cones[i,2] / (spread.cones[i,2] + expected.cones[i,2])
    observed.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
    predicted.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
    SE_obs[i,2] &lt;- pow(observed.cones[i,2]-expected.cones[i,2],2)
    SE_pred[i,2] &lt;- pow(predicted.cones[i,2]-expected.cones[i,2],2)
  }
  
  
  ### Priors, model 2
  a2 ~ dunif(0.001,2)
  b2 ~ dunif(0.5,4)
  r2 ~ dunif(0.5,5)

  ### Likelihood for model 3: constant a and b

  for(i in 1:n.obs){
    expected.cones[i,3] &lt;- a3*pow(DBH[i],b3)       # a*DBH^b
    spread.cones[i,3] &lt;- r3[wave[i]]
    p[i,3] &lt;- spread.cones[i,3] / (spread.cones[i,3] + expected.cones[i,3])
    observed.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
    predicted.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
    SE_obs[i,3] &lt;- pow(observed.cones[i,3]-expected.cones[i,3],2)
    SE_pred[i,3] &lt;- pow(predicted.cones[i,3]-expected.cones[i,3],2)
  }
  
  SSE_obs[1] &lt;- sum(SE_obs[,1]) 
  SSE_pred[1] &lt;- sum(SE_pred[,1])
  SSE_obs[2] &lt;- sum(SE_obs[,2]) 
  SSE_pred[2] &lt;- sum(SE_pred[,2])
  SSE_obs[3] &lt;- sum(SE_obs[,3]) 
  SSE_pred[3] &lt;- sum(SE_pred[,3])

  ### Priors, model 3
  for(j in 1:2){   # estimate separately for wave and non-wave
    r3[j] ~ dunif(0.5,5)
  }
  a3 ~ dunif(0.001,2)
  b3 ~ dunif(0.5,4)

  #####################
  ### SELECT THE BEST MODEL!!! 
  #####################

  for(i in 1:n.obs){
    observed.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])
    predicted.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])     # for posterior predictive check!
    SE2_obs[i] &lt;- pow(observed.cones2[i]-expected.cones[i,selected],2)
    SE2_pred[i] &lt;- pow(predicted.cones2[i]-expected.cones[i,selected],2)
  }
  
  SSE2_obs &lt;- sum(SE2_obs[])
  SSE2_pred &lt;- sum(SE2_pred[])


  ### Priors
  
    # model selection...
  prior[1] &lt;- 1/3
  prior[2] &lt;- 1/3     # you can put substantially more weight because fewer parameters (there are more rigorous ways to do this!!)
  prior[3] &lt;- 1/3
  selected ~ dcat(prior[])   
  
  
}
    
&quot;,file=&quot;BUGS_fir_modelselection.txt&quot;)</code></pre>
<p>Now we can use MCMC to find which model we put our beliefs in after
we account for our data!</p>
<p>Then we need to package the data for JAGS</p>
<pre class="r"><code># Package the data for JAGS

data.package3 &lt;- list(
  observed.cones = matrix(rep(fir$TOTCONES,times=3),ncol=3,byrow=F),
  observed.cones2 = fir$TOTCONES,
  n.obs = nrow(fir),
  wave = as.numeric(fir$WAVE_NON),
  #n.models = 3,
  DBH = fir$DBH
)
#data.package</code></pre>
<p>Then we can run the model!</p>
<pre class="r"><code># Run JAGS

params.to.monitor &lt;- c(&quot;a1&quot;,&quot;b1&quot;,&quot;r1&quot;,&quot;a2&quot;,&quot;b2&quot;,&quot;r2&quot;,&quot;a3&quot;,&quot;b3&quot;,&quot;r3&quot;,&quot;selected&quot;,&quot;predicted.cones2&quot;,&quot;predicted.cones&quot;,&quot;SSE_obs&quot;,&quot;SSE_pred&quot;,&quot;SSE2_obs&quot;,&quot;SSE2_pred&quot;)

jags.fit3 &lt;- jags(data=data.package3,parameters.to.save=params.to.monitor,n.adapt=1000,n.iter=5000,model.file=&quot;BUGS_fir_modelselection.txt&quot;,n.chains = 2,n.burnin = 1000,n.thin=2 )</code></pre>
<pre><code>## 
## Processing function input....... 
## 
## Done. 
##  
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 968
##    Unobserved stochastic nodes: 982
##    Total graph size: 7770
## 
## Initializing model
## 
## Adaptive phase, 1000 iterations x 2 chains 
## If no progress bar appears JAGS has decided not to adapt 
##  
## 
##  Burn-in phase, 1000 iterations x 2 chains 
##  
## 
## Sampling from joint posterior, 4000 iterations x 2 chains 
##  
## 
## Calculating statistics....... 
## 
## Done.</code></pre>
<pre class="r"><code>jagsfit3.mcmc &lt;- jags.fit3$samples   # convert to &quot;MCMC&quot; object (coda package)

BUGSlist &lt;- as.data.frame(jags.fit3$sims.list)
#summary(jagsfit.mcmc)

#plot(jagsfit.mcmc)</code></pre>
<pre class="r"><code># Visualize the model fit

#plot(jagsfit.mcmc[,&quot;selected&quot;])

plot(jagsfit3.mcmc[,&quot;a1[1]&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;a1[2]&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-2.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;a2&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-3.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;a3&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-4.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;r1[1]&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-5.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;r1[2]&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-6.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;r2&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-7.png" width="672" /></p>
<pre class="r"><code>plot(jagsfit3.mcmc[,&quot;r3[1]&quot;])</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-55-8.png" width="672" /></p>
<p>Let’s look at the model selection (that’s the whole point!!)</p>
<pre class="r"><code># Perform explicit model selection

n.iterations &lt;- length(jags.fit3$sims.list$selected)
selected &lt;- table(jags.fit3$sims.list$selected)
names(selected) &lt;- c(&quot;Full model&quot;,&quot;No wave&quot;,&quot;Fixed a&amp;b&quot;)
selected</code></pre>
<pre><code>## Full model    No wave  Fixed a&amp;b 
##       1067       1269       1664</code></pre>
<pre class="r"><code>barplot(selected/n.iterations,ylab=&quot;Degree of belief&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<div id="evaluate-model-fit" class="section level4">
<h4>Evaluate model fit</h4>
<p>Now we can look at model fit! We will use the same method we used in
lab- the posterior predictive check…</p>
<p>We can write a for loop to extract the prediction results for each
model (and for the model-averaged model):</p>
<p>First let’s just look at the model predictions vs the observed data.
First, model 1</p>
<pre class="r"><code># Goodness of fit

n.data &lt;- length(fir$DBH)

plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind &lt;- sprintf(&quot;predicted.cones[%s,1]&quot;,d)
  model1 &lt;- as.vector(jagsfit3.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col=&quot;gray&quot;,cex=0.4)
}</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<p>Next, model 2 (reduced):</p>
<pre class="r"><code># Perform posterior predictive check

plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind &lt;- sprintf(&quot;predicted.cones[%s,2]&quot;,d)
  model1 &lt;- as.vector(jagsfit3.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col=&quot;gray&quot;,cex=0.4)
}</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>And model 3 (fixed a and b):</p>
<pre class="r"><code>plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind &lt;- sprintf(&quot;predicted.cones[%s,3]&quot;,d)
  model1 &lt;- as.vector(jagsfit3.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col=&quot;gray&quot;,cex=0.4)
}</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>Clearly all the models seem to fit okay… But, it seems like there is
more prediction error than is necessary… Let’s run a posterior
predictive check!</p>
<p>For model 1:</p>
<pre class="r"><code># Posterior Predictive Checks!

plot(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[1]&quot;][[1]])~as.vector(jagsfit3.mcmc[,&quot;SSE_obs[1]&quot;][[1]]),xlab=&quot;SSE, real data&quot;,ylab=&quot;SSE, perfect data&quot;,main=&quot;Posterior Predictive Check&quot;)
abline(0,1,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r"><code>p.value=length(which(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[1]&quot;][[1]])&gt;as.vector(jagsfit3.mcmc[,&quot;SSE_obs[1]&quot;][[1]])))/length(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[1]&quot;][[1]]))
p.value </code></pre>
<pre><code>## [1] 0.997</code></pre>
<pre class="r"><code>plot(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[2]&quot;][[1]])~as.vector(jagsfit3.mcmc[,&quot;SSE_obs[2]&quot;][[1]]),xlab=&quot;SSE, real data&quot;,ylab=&quot;SSE, perfect data&quot;,main=&quot;Posterior Predictive Check&quot;)
abline(0,1,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<pre class="r"><code>p.value=length(which(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[2]&quot;][[1]])&gt;as.vector(jagsfit3.mcmc[,&quot;SSE_obs[2]&quot;][[1]])))/length(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[2]&quot;][[1]]))
p.value </code></pre>
<pre><code>## [1] 0.9975</code></pre>
<pre class="r"><code>plot(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[3]&quot;][[1]])~as.vector(jagsfit3.mcmc[,&quot;SSE_obs[3]&quot;][[1]]),xlab=&quot;SSE, real data&quot;,ylab=&quot;SSE, perfect data&quot;,main=&quot;Posterior Predictive Check&quot;)
abline(0,1,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<pre class="r"><code>p.value=length(which(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[3]&quot;][[1]])&gt;as.vector(jagsfit3.mcmc[,&quot;SSE_obs[3]&quot;][[1]])))/length(as.vector(jagsfit3.mcmc[,&quot;SSE_pred[3]&quot;][[1]]))
p.value   </code></pre>
<pre><code>## [1] 0.9965</code></pre>
<p>Interesting- the model seems to not fit very well!</p>
<p>Okay that’s it for model selection, now let’s move on to:</p>
</div>
</div>
<div id="model-averaging" class="section level3">
<h3>Model averaging</h3>
<p>The fact that model selection is such a big deal in ecology and
environmental science indicates that we are rarely certain about which
model is the best model. Even after constructing an AIC table we may be
very unsure about which model is the “true” model.</p>
<p>The AIC weights tell us in essence how much we “believe” in each
model. This is a very Bayesian interpretation, but model averaging
really is best thought of in a Bayesian context.</p>
<p>One way to do model averaging relies on AIC weights. Basically we
take the set of predictions from each model independently and weight
them by the Akaike weight. There is a literature on this and R packages
for helping (see package <a
href="https://cran.r-project.org/web/packages/AICcmodavg/AICcmodavg.pdf">‘AICcmodavg’</a>)</p>
<div id="when-should-you-use-model-averaged-parameter-estimates"
class="section level4">
<h4>When should you use model-averaged parameter estimates</h4>
<p>Really never in my opinion- can be useful for prediction but not
useful (even misleading) if you are trying to estimate effect sizes!</p>
</div>
<div id="the-bayesian-version" class="section level4">
<h4>The Bayesian version!</h4>
<p>We can use the results from the JAGS code above to easily generate
Bayesian model-averaged predictions! JAGS makes it relatively simple and
straightforward to do model averaging in a Bayesian context!</p>
<p>Look at the predictions for the model averaged model:</p>
<pre class="r"><code>plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind &lt;- sprintf(&quot;predicted.cones2[%s]&quot;,d)
  model1 &lt;- as.vector(jagsfit3.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col=&quot;gray&quot;,cex=0.4)
}</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>Note that these predictions naturally incorporate both parameter
uncertainty and structural (model selection) uncertainty!</p>
<p>We can do a posterior predictive check with the model-averaged
model!</p>
<pre class="r"><code># Posterior predictive check with model-averaged model!

plot(as.vector(jagsfit3.mcmc[,&quot;SSE2_pred&quot;][[1]])~as.vector(jagsfit3.mcmc[,&quot;SSE2_obs&quot;][[1]]),xlab=&quot;SSE, real data&quot;,ylab=&quot;SSE, perfect data&quot;,main=&quot;Posterior Predictive Check&quot;)
abline(0,1,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<pre class="r"><code>p.value=length(which(as.vector(jagsfit3.mcmc[,&quot;SSE2_pred&quot;][[1]])&gt;as.vector(jagsfit3.mcmc[,&quot;SSE2_obs&quot;][[1]])))/length(as.vector(jagsfit3.mcmc[,&quot;SSE2_pred&quot;][[1]]))
p.value </code></pre>
<pre><code>## [1] 0.9985</code></pre>
<p><a href="LECTURE9.html">–go to next lecture–</a></p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
