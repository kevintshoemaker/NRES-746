<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Anson Call, Jonathan DeBoer, Jacob Macdonald, Mikey Johnson" />

<meta name="date" content="2018-11-14" />

<title>Accounting for Spatial Autocorrelation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus2.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TimeSeries.html">Time-series analysis</a>
    </li>
    <li>
      <a href="SEM.html">Structural Equation Models</a>
    </li>
    <li>
      <a href="SpatialAutocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="BayesianNetworks.html">Bayesian Networks</a>
    </li>
    <li>
      <a href="MixedEffects.html">Mixed Effects Models</a>
    </li>
    <li>
      <a href="Ordination.html">Ordination</a>
    </li>
    <li>
      <a href="QuantileRegression.html">Quantile Regression</a>
    </li>
    <li>
      <a href="RSFs.html">Resource Selection Functions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Accounting for Spatial Autocorrelation</h1>
<h4 class="author"><em>Anson Call, Jonathan DeBoer, Jacob Macdonald, Mikey Johnson</em></h4>
<h4 class="date"><em>11/14/2018</em></h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a href="SpatialAutocorrelation.R">click here</a> for the companion R script for this lecture.</p>
<p>Also, the following data files will be used in the in-class demos- please download these to your working directory:<br />
<a href="age.data1.csv" class="uri">age.data1.csv</a><br />
<a href="wideSample.csv" class="uri">wideSample.csv</a><br />
<a href="narrowSample.csv" class="uri">narrowSample.csv</a><br />
<a href="SnowEx17_GPR_Week1_transects.txt" class="uri">SnowEx17_GPR_Week1_transects.txt</a><br />
<a href="AutoCovRaster.tif" class="uri">AutoCovRaster.tif</a></p>
<pre class="r"><code>#first things first: here are the packages you&#39;ll need to follow along:

list = c(&#39;gstat&#39;, &#39;raster&#39;, &#39;geosphere&#39;, &#39;ape&#39;, &#39;foreach&#39;, &#39;doParallel&#39;, &#39;rgdal&#39;)
#install.packages(list)</code></pre>
<blockquote>
<p>Tobler’s First Law of Geography: “everything is related to everything else, but near things are more related than distant things.”</p>
</blockquote>
<p>Introduced by Waldo R. Tobler in 1969</p>
<div id="definition" class="section level2">
<h2>Definition:</h2>
<p>Observations made at different locations may not be independent. For example, measurements made at nearby locations may be closer in value than measurements made at locations farther apart. This phenomenon is called <strong>spatial autocorrelation</strong> (SA).</p>
<p>SA is everywhere. Variables like rainfall, soil nutrient content, elevation, population density, etc. are all spatially autocorrelated by nature. This is intuitive: two points that are close together are much more likely to have a similar elevation than two points that are far apart.</p>
</div>
<div id="positive-and-negative-sa" class="section level2">
<h2>Positive and Negative SA</h2>
<p>Positive SA indicates <strong>clumping,</strong> while negative SA indicates <strong>overdispersion.</strong> Clumping <em>may</em> be a problem for those fitting linear models to spatially-explicit data (more on this later).</p>
<div class="figure">
<img src="sp_ac.png" alt="Figure from: Kirkegaard, E. https://thewinnower.com/papers/2847-some-methods-for-measuring-and-correcting-for-spatial-autocorrelation" />
<p class="caption">Figure from: Kirkegaard, E. <a href="https://thewinnower.com/papers/2847-some-methods-for-measuring-and-correcting-for-spatial-autocorrelation" class="uri">https://thewinnower.com/papers/2847-some-methods-for-measuring-and-correcting-for-spatial-autocorrelation</a></p>
</div>
<p>To further illustrate this principle, we’ll simulate a landscape and generate some randomly sampled points.</p>
<div id="simulated-landscape-example" class="section level3">
<h3>Simulated Landscape Example</h3>
<p>First, we will load packages and create an empty grid.</p>
<pre class="r"><code>library(gstat)
library(raster)</code></pre>
<pre><code>## Warning: package &#39;sp&#39; was built under R version 3.5.1</code></pre>
<pre class="r"><code>xy &lt;- expand.grid(1:100, 1:100) # create a coordinate grid to represent a real landscape.
                                # the larger the grid, the longer it will take to generate data
names(xy) &lt;- c(&#39;x&#39;,&#39;y&#39;) # name the axes of the grid</code></pre>
<p>Now, we will use the <code>gstat()</code> and <code>predict()</code> functions to fill the empty grid with simulated elevation values.</p>
<pre class="r"><code>  # Specify a model to create a spatially-autocorrelated z variable.
  # range parameter controls degree of spatial autocorrelation.
saLandscapeModel &lt;- gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(100), 
                 model=vgm(psill=100, range=100, model=&#39;Exp&#39;), nmax=20)

saLandscape &lt;- predict(saLandscapeModel, newdata = xy, nsim = 1)</code></pre>
<pre><code>## [using unconditional Gaussian simulation]</code></pre>
<pre class="r"><code>image(saLandscape, axes = FALSE, col = terrain.colors(10))</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Next, we’ll take a random sample of 30 locations and measure the elevation at each location.</p>
<pre class="r"><code>sample &lt;- xy[sample(nrow(xy), 30),] # Randomly sample pixels from the grid
samplePoints &lt;- SpatialPoints(sample) # Create a SpatialPoints object
colors &lt;- topo.colors(20)


# Extract explanatory values.
names(saLandscape) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;z&#39;) # Step 1: rename columns in envImage df for clarity
gridded(saLandscape)=~x+y # Step 2: Data frame must be converted to gridded object to create raster layer
saRaster &lt;- raster(saLandscape) # Step 3: convert gridded object to raster
elev &lt;- raster::extract(x=saRaster, y=samplePoints) # Step 4: extract values.
sample &lt;- cbind(sample, elev)
row.names(sample) &lt;- (1:30) </code></pre>
<pre class="r"><code>print(head(sample))</code></pre>
<pre><code>##    x  y     elev
## 1 17 24 106.0363
## 2 67 51 104.5889
## 3 72 88 111.3068
## 4 40 94 109.4490
## 5 94 46 102.8795
## 6 73 68 103.4749</code></pre>
<pre class="r"><code>image(saLandscape, axes = FALSE, col = terrain.colors(10))
points(sample)</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>As you can see, points that are very close to each other always have a similar elevation, while points that are distant from each other are less likely to have a similar elevation.</p>
<p>Next, we will focus on a couple of ways to test for SA: one graphical, and one statistical.</p>
</div>
</div>
<div id="the-semivariogram-a-graphical-representation-of-sa" class="section level2">
<h2>The Semivariogram: a graphical representation of SA</h2>
<p>This function will create a semivariogram.</p>
<pre class="r"><code>semivariogram &lt;- function(value,x,y){
  
  # building empty and null vectors
  dist &lt;- vector(mode=&quot;numeric&quot;, length=length(value))
  semivar &lt;- vector(mode=&quot;numeric&quot;, length=length(value))
  distance &lt;- c()
  semivariance &lt;- c()
  
  # calculating all possible
  
  for (i in 1:length(value)) {                    # these loops compare all the values with each other
    for( j in 1:length(value)) {
      dist[j] &lt;-  sqrt((x[i]-x[j])^2 + (y[i]-y[j])^2)           # measuring the distance
      semivar[j] &lt;- (value[i]-value[j])^2                       # calcualting the semivariance
    }
    distance &lt;- c(distance,dist)
    semivariance &lt;- c(semivariance,semivar)
  }
  
  plot(distance, semivariance, xlab=&quot;Distance Between Points&quot;, ylab=&quot;Squared Difference&quot;)
}</code></pre>
<pre class="r"><code>semivariogram(sample$elev,sample$x,sample$y)</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/semivariogram-1.png" width="672" /></p>
<ul>
<li><p>The Sill is where we see the values level off. This is the distance where we expect to see SA fade.</p></li>
<li><p>Unfortunately we don’t always have data that looks this clean. Here’s another example. To run this code, you will need <a href="SnowEx17_GPR_Week1_transects.txt">this data file</a></p></li>
</ul>
<pre class="r"><code>semivariogram.ll(x,lat,lon)</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<ul>
<li>One think we can consider is thining the data set to see if it makes more sense</li>
</ul>
<pre class="r"><code># building a small data set for the semivarance
df.new &lt;- transect46[seq(1, nrow(transect1), 500),]
#df.new &lt;- week1_transects[seq(1, nrow(week1_transects), 1000),] #this will take all the transects

# seprating data for the function
lat &lt;- df.new$LAT
lon &lt;- df.new$LONG
x &lt;- df.new$SNOW.DEPTH..m..assuming.velocity.of.0.234.m.ns.</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<ul>
<li>The next step might be looking at some numeric values for the day.</li>
</ul>
</div>
<div id="morans-i-a-quantitative-representation-of-sa" class="section level2">
<h2>Moran’s I: a quantitative representation of SA</h2>
<div class="figure">
<img src="Moran.png" alt="Dr. Patrick Moran - 1950" />
<p class="caption">Dr. Patrick Moran - 1950</p>
</div>
<ul>
<li>Moran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.</li>
</ul>
</div>
<div id="global-vs-local" class="section level2">
<h2>Global vs Local</h2>
<ul>
<li>Moran’s I evaluates SAC at a Global level <img src="G_L.png" alt="Global SAC evaluation" /></li>
</ul>
</div>
<div id="morans-i" class="section level2">
<h2>Moran’s I</h2>
<div class="figure">
<img src="Eq.png" alt="How Moran’s I is calculated" />
<p class="caption">How Moran’s I is calculated</p>
</div>
<ul>
<li>“N” is the number of spatial units indexed by“i” and “j”; “x” is the variable of interest; “x bar” is the mean of “x”; “w_ij” is a matrix of spatial weights with zeroes on the diagonal (see in a moment); and “W” is the sum of all “w_ij”.</li>
<li>Weights are assigned by a a variety of functions such as nearest neighbor or distance decay</li>
</ul>
</div>
<div id="calculating-morans-i" class="section level2">
<h2>Calculating Moran’s I</h2>
<ul>
<li>Install the package “ape”. Used for “Analyses of Phylogenetics and Evolution”</li>
</ul>
<pre class="r"><code>#install.packages(&quot;ape&quot;)
library(&quot;ape&quot;)
#look at package &quot;ape&quot;
#??ape</code></pre>
<pre class="r"><code>#function we will use is Moran.I()
#?Moran.I
#look at the help and see the Moran&#39;s forumla
#requires &quot;x&quot; - a numeric vector - which is our variable.
#requires &quot;weight&quot; - a matrix of weights - calculated using dist().</code></pre>
<ul>
<li>First we will calculate our distance matrix. In this example, the distance matrix is used to calculate the weights using nearest neighbor in <code>Moran.I()</code></li>
</ul>
<pre class="r"><code>#generating IDW matrix
#use dist() to compute and return the distance matrix between rows of data
#?dist
sample.distances&lt;-as.matrix(dist(cbind(sample$x,sample$y))) #distance weight matrix
sample.distances.inverse&lt;-1/sample.distances #inverse distance weight matrix
sample.distances.inverse[1:5,1:5] #however &quot;infin&quot; problem&quot;</code></pre>
<pre><code>##            1          2          3          4          5
## 1        Inf 0.01759811 0.01185030 0.01357188 0.01248732
## 2 0.01759811        Inf 0.02678358 0.01969512 0.03641785
## 3 0.01185030 0.02678358        Inf 0.03071476 0.02109123
## 4 0.01357188 0.01969512 0.03071476        Inf 0.01384091
## 5 0.01248732 0.03641785 0.02109123 0.01384091        Inf</code></pre>
<pre class="r"><code>#Remove infinity and replace with 0&#39;s - occurs because 1 divide by 0 is infinity
diag(sample.distances.inverse)&lt;-0 
sample.distances.inverse[1:5,1:5]</code></pre>
<pre><code>##            1          2          3          4          5
## 1 0.00000000 0.01759811 0.01185030 0.01357188 0.01248732
## 2 0.01759811 0.00000000 0.02678358 0.01969512 0.03641785
## 3 0.01185030 0.02678358 0.00000000 0.03071476 0.02109123
## 4 0.01357188 0.01969512 0.03071476 0.00000000 0.01384091
## 5 0.01248732 0.03641785 0.02109123 0.01384091 0.00000000</code></pre>
</div>
<div id="evaluating-morans-i" class="section level2">
<h2>Evaluating Moran’s I</h2>
<p><img src="Morans_I.png" alt="Moran’s I is on a scale from -1 to +1" /> <img src="sp_ac2.png" /></p>
</div>
<div id="morans-i-1" class="section level2">
<h2>Moran’s I</h2>
<pre class="r"><code>#Morans I
#?Moran.I
#Weights are obtained, now place desired variable to test as &quot;x&quot;
Moran.I(x=sample$elev,weight=sample.distances.inverse)</code></pre>
<pre><code>## $observed
## [1] 0.2537749
## 
## $expected
## [1] -0.03448276
## 
## $sd
## [1] 0.04254205
## 
## $p.value
## [1] 1.236966e-11</code></pre>
<pre class="r"><code>#Results!</code></pre>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<ul>
<li>Returned is a list of 4 objects: observed, expected, sd, p.value</li>
</ul>
<p>1: observed = Moran’s I</p>
<p>2: expected = the expected value under the null hypothesis</p>
<p>3: sd = standard deviation under the null hypothesis</p>
<p>4: p.value = P-value of the test of the null hypothesis against the alternative hypothesis</p>
</div>
<div id="when-sa-is-a-problem-modelling-spatially-explicit-data." class="section level2">
<h2>When SA is a problem: Modelling spatially-explicit data.</h2>
<p>When SA occurs in spatially-explicit expanatory and response variables, Type I error rates (rejecting a true null hypothesis) are inflated. To demonstrate this phenomenon, I’ve recreated a simulation from Legendre et al. 2002 - a seminal paper on the topic of SA effects on field surveys.</p>
<p>Assume we want to test whether tree height is related to soil P concentration. To test our hypothesis, we decide to collect some field data, randomly selecting trees and recording the location (xy), height, and age of each tree.</p>
<p>To break our model down further, we can think of each variable (height and age) as being determined by 3 components: a deterministic component, a stochastic (error) component, and a degree of SA.</p>
<p>From Legendre et al. 2002: <img src="LegendreFigure.JPG" alt="Figure from: Legendre 2002" /></p>
<p>With this concept in mind, let’s simulate some data. Lots of data.</p>
<p>Each simulation needs 6 landscapes:</p>
<ol style="list-style-type: decimal">
<li>Deterministic soil P structure.<br />
</li>
<li>SA in soil P structure.<br />
</li>
<li>Stochastic error in soil P variable.<br />
</li>
<li>Deterministic height structure.<br />
</li>
</ol>
<ul>
<li>This is simply the sum of the first three landscapes, multiplied by a single coefficient. If there is no relationship between soil P and tree height, the coefficient will be 0.<br />
</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>SA in height structure.<br />
</li>
<li>Stochastic error in height variable.</li>
</ol>
<p>For this simulation, we’re going to make the null hypothesis (no relationship between soil P and height) true. That means the true coefficient linking soil P and tree height will be 0. Therefore, if we fit a linear model <code>lm(height~soilP)</code> to each simulated dataset, the type I error rate (incorrectly rejecting a true null hypothesis) should be equal to <span class="math inline">\(\alpha\)</span> (=0.05).</p>
<pre class="r"><code>## Parallel Spatial Autocorrelation Simulation
library(parallel)
library(foreach)
library(doParallel)

numCores &lt;- detectCores()-1
registerDoParallel(numCores)

resultsList&lt;-foreach(i=1:100, .combine = c) %do% {     # %dopar%
  library(gstat)
  library(raster)
  library(sp)
  
  xy &lt;- expand.grid(1:100, 1:100) # create a coordinate grid to represent a real landscape.
  # the larger the grid, the longer it will take to generate data
  names(xy) &lt;- c(&#39;x&#39;,&#39;y&#39;) # name the axes
  
  # If desired, model deterministic component of explanatory variable on landscape surface
  # by altering the beta parameters in the gstat() call.
  # Currently set to 0 (no deterministic relationship between space and explanatory variable)    
  envDeterm &lt;- gstat::gstat(formula=z~1+x+y, locations=~x+y, dummy=T, beta=c(1,0,0), 
                     model=gstat::vgm(psill=0, range=1, model=&#39;Exp&#39;), nmax=1)
  
  # Model spatially-autocorrelated component of explanatory variable on landscape surface.
  # range parameter controls degree of spatial autocorrelation. Set to 1 to eliminate spatial autocorrelation.
  envSA &lt;- gstat::gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0), 
                 model=gstat::vgm(psill=.025, range=20, model=&#39;Gau&#39;), nmax=2)
  
  # Model stochastic (error) component of explanatory variable on landscape surface.
  envErr &lt;- gstat::gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0), 
                  model=gstat::vgm(psill=.025, range=1, model=&#39;Gau&#39;), nmax=1)
  
  # Simulate data from each model across the landscape surface.
  # Can visualize each surface using image() if desired
  eDe &lt;- predict(envDeterm, newdata=xy, nsim=1) 
  eSa &lt;- predict(envSA, newdata=xy, nsim=1) 
  eEr &lt;- predict(envErr, newdata=xy, nsim=1)
  # image(eDe)
  # image(eSa)
  # image(eEr)
  
  # Add each component into a single landscape surface.
  envImage &lt;- eDe+eSa+eEr
  # image(envImage)
  
  # Model spatially-autocorrelated component of response variable on landscape surface.
  # range parameter controls degree of spatial autocorrelation. Set to 1 to eliminate spatial autocorrelation.
  resSA &lt;- gstat::gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0), 
                 model=gstat::vgm(psill=.025, range=20, model=&#39;Gau&#39;), nmax=2)
  
  # Model stochastic (error) component of response variable on landscape surface.
  resErr &lt;- gstat::gstat(formula=z~1, locations=~x+y, dummy=T, beta=c(0), 
                  model=gstat::vgm(psill=.025, range=1, model=&#39;Gau&#39;), nmax=1)
  
  # Simulate data from response variable SA and error models across the landscape surface.
  # Again, can visualize each surface using image() if desired
  rSa &lt;- predict(resSA, newdata=xy, nsim=1)
  rEr &lt;- predict(resErr, newdata=xy, nsim=1)
  # image(rSA)
  # image(rEr)
  
  # Combine the explanatory variable surface with the response SA and error
  # surfaces to generate predicted response values. If there is no true
  # relationship between the explanatory and response variables, the beta1
  # parameter is set to zero.
  beta1 &lt;- 0
  resImage &lt;- (beta1*envImage$sim1) + rSa + rEr 
  
  # Create random sample points
  sample &lt;- xy[sample(nrow(xy), 100),] # Randomly sample pixels from the grid
  samplePoints &lt;- sp::SpatialPoints(sample) # Create a SpatialPoints object
  
  # Extract explanatory and response values using the sampling locations. There
  # is probably a simpler way to do this, but in this case I am creating rasters
  # from the envImage and resImage data frames, then using raster::extract to
  # extract the explanatory and response values and combine them in a new data
  # frame of simulated data.
  
  # Extract explanatory values.
  names(envImage) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;explanatory&#39;) # Step 1: rename columns in envImage df for clarity
  sp::coordinates(envImage) &lt;- ~x+y  # Step 2: Data frame must be converted to gridded object to create raster layer
  sp::gridded(envImage) &lt;- TRUE
  envRaster &lt;- raster::raster(envImage) # Step 3: convert gridded object to raster
  explanatory &lt;- raster::extract(x=envRaster, y=samplePoints) # Step 4: extract values.
  
  # Extract response values. Repeat steps 1-4.
  names(resImage) &lt;- c(&#39;x&#39;,&#39;y&#39;,&#39;response&#39;)
  sp::coordinates(resImage) &lt;- ~x+y
  sp::gridded(resImage) &lt;- TRUE
  resRaster &lt;- raster::raster(resImage)
  response &lt;- raster::extract(x=resRaster, y=samplePoints) 
  
  # Bind locations, explanatory values, and response values into a singe data
  # frame of simulated data
  sample &lt;- cbind(sample,explanatory,response) 
  
  # Create simple linear model and extract the p-value.
  model &lt;- stats::lm(response ~ explanatory)
  pvals &lt;- summary.lm(model)$coefficients[,4]
  # pvals
  i &lt;- as.numeric(pvals[2]) # Store the p-value for the beta1 parameter in the resultsList
}

# Count number of positive results (p value &gt; 0.05)
for (i in 1:length(resultsList)){
  if (resultsList[i] &gt;= 0.05){
    resultsList[i] &lt;- FALSE
  }else(resultsList[i] &lt;- TRUE)
}</code></pre>
<pre class="r"><code>typeIerror &lt;- sum(resultsList)/100
pvalue &lt;- 0.05
simulationResults &lt;- c(pvalue, typeIerror)
names(simulationResults) &lt;- c(&#39;P-value&#39;, &#39;Type-I error rate&#39;)
print(simulationResults)</code></pre>
<pre><code>##           P-value Type-I error rate 
##              0.05              0.16</code></pre>
<p>As you can see, the type I error rate is greater than <span class="math inline">\(\alpha\)</span>. If we were to eliminate SA in either soil P or tree height, this effect would go away.</p>
<p>So, maybe you used a semivariogram and Moran’s I to check for spatial autocorrelation in your explanatory and response variables, and (surprise!) you’ve found it. What can be done? In the next section, we will discuss one method for <em>correcting</em> a model to account for spatial autocorrelation.</p>
</div>
<div id="accounting-for-sa-the-autocovariate-modeling-method" class="section level2">
<h2>Accounting for SA: the Autocovariate Modeling method</h2>
<p><br> Suppose we want to find the relationship between a tree species’ height and its age. We go out and record the age, height, and location of sampled trees. To run the following sample code, you will need <a href="age.data1.csv">this data</a></p>
<pre class="r"><code>library(rgdal)
library(raster)
age.data1 &lt;- read.csv(&#39;age.data1.csv&#39;)
print(head(age.data1))</code></pre>
<pre><code>##          x        y  heights age
## 1 176.0817 176.1028 12.77577  30
## 2 165.6816 131.5045 25.88494 123
## 3 170.9068 168.1290 25.95757 135
## 4 145.6704 152.3659 22.15213  41
## 5 157.8530 151.2091 21.39883  58
## 6 173.6417 168.3585 23.84544 140</code></pre>
<pre class="r"><code>cropped &lt;- raster(&#39;AutoCovRaster.tif&#39;)
plot(cropped, axes = FALSE, col = terrain.colors(10))
points(age.data1[,1], age.data1[,2])</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>semivariogram1 &lt;- function(value,x,y){                     
  dist &lt;- vector(mode=&quot;numeric&quot;, length=length(value))    
  semivar &lt;- vector(mode=&quot;numeric&quot;, length=length(value))
  distance &lt;- c()
  semivariance &lt;- c()
  for (i in 1:length(value)) {
    for( j in 1:length(value)) {
      dist[j] &lt;-  sqrt((x[i]-x[j])^2 + (y[i]-y[j])^2)           
      semivar[j] &lt;- (value[i]-value[j])^2                       
    }
    distance &lt;- c(distance,dist)
    semivariance &lt;- c(semivariance,semivar)
  }
  plot(distance, semivariance, xlab=&quot;Distance Between Points&quot;, ylab=&quot;Squaired Difference between residuals&quot;, ylim=c(0,250))
}</code></pre>
<p>If we simply run a linear regression using the most common method, Ordinary Least Squares (OLS), we get find the following relationship:</p>
<pre class="r"><code>basic.model &lt;- lm(heights~age, data=age.data1)
basic.model$coefficients</code></pre>
<pre><code>## (Intercept)         age 
## 15.17893847  0.05334926</code></pre>
<p><br> <br> But this model is only valid if the assumptions of OLS are met. One assumption of OLS is that the residuals are independent. In other words, there must not be spatial autocorrelation in the residuals.</p>
<pre class="r"><code>semivariogram1(basic.model$residuals,age.data1$x,age.data1$y)</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Now we will account for correlation in the residuals of this model by applying the Autocovariate modeling method. This method works by computing an “autocovariate” value for each site. This value represents the degree of correlation between a given site and its neighbors. The autocovariate is then treated as an additional predictor variable in our linear model. <br> <br></p>
<p><span class="math display">\[Y=CX + \varepsilon \,\,\,\,\,\,\,\,\,\, \rightarrow \,\,\,\,\,\,\,\,\,\, Y=C_{1}X+C_{2}A +\varepsilon\]</span> Where the autocovariate value, A is defined as: <span class="math display">\[A_{i}= \sum_{j \, \epsilon \, k} W_{ij}Y_{j} \]</span> k is the neighborhood around a given site. Each site has a neighborhood containing some number, j, neighbors. <br> <br> The following functions will calculate A, but how do the functions work? <br> <br> <span class="math display">\[ \textrm{datapoint locations}\,\,\,\,\,\,\,\, \rightarrow \,\,\,\,\,\,\,\,\textrm{dist matrix}\,\,\,\,\,\,\,\, \rightarrow \,\,\,\,\,\,\,\,\ \textrm{weight matrix} \,\,\,\,\,\,\,\, \rightarrow \,\,\,\,\,\,\,\, A   \]</span> <br></p>
<pre class="r"><code>linear.cor &lt;- function(dist.matrix=distance){  # helper function for autocovariate.model()
  weight.part1 &lt;- (dist.matrix*(-1/40) + 1)    # defines how strongly neighboring points are correlated with each other as a function of distance between them
}                                              # ie. determines what the values of the weight matrix will be



# Autocovariate.model() is a function which appends an autocovariate value to each datapoint.
# The autocovariate values are then used as a second predictor variable in you linear model
# The inputs are your data and your correlation function (uses the above function, linear.cor() by default)

autocovariate.model &lt;- function(data=data1, cor.func=linear.cor){
  dist.temp &lt;- dist(data[,1:2])      # create a distance matrix from the data
  distance &lt;- as.matrix(x=dist.temp) # create a distance matrix from the data
  weight.part1 &lt;- cor.func(distance) # next few lines create a weighted distance matrix based on your correlation function
  no.negs &lt;- function(x){
    if(x&gt;=0){x&lt;-x}else{x&lt;-0}
    return(x)
  }
  weight &lt;- apply(weight.part1, c(1,2), no.negs)   # neibors beyond a given threshold should have no influance rather than a negative correlation, so any negative values are converted to 0
  weighted.heights &lt;- matrix(nrow=nrow(data), ncol=nrow(data)) # set up matrix which will be the weight matrix times the response variable
  for(i in 1:nrow(data)){
    weighted.heights[i,] &lt;- (weight[i,]*data$heights)
  }
  autocov &lt;- numeric(nrow(data))                   # calculates the autocovariate value at each site
  for(i in 1:nrow(data)){                          # autocov is the sum a given site&#39;s neighbors&#39; (weight*response variable)
    autocov[i] &lt;- sum(weighted.heights[i,])-weighted.heights[i,i]
  }
  data.autocov &lt;- cbind(data,autocov)
  return(data.autocov)
}


age.data2 &lt;- autocovariate.model(age.data1)            
head(age.data2)</code></pre>
<pre><code>##          x        y  heights age   autocov
## 1 176.0817 176.1028 12.77577  30 102.30184
## 2 165.6816 131.5045 25.88494 123  81.97293
## 3 170.9068 168.1290 25.95757 135 110.44709
## 4 145.6704 152.3659 22.15213  41 121.32546
## 5 157.8530 151.2091 21.39883  58 138.68750
## 6 173.6417 168.3585 23.84544 140 108.67815</code></pre>
<pre class="r"><code>basic.model &lt;- lm(heights~age, data=age.data2)
corrected.model &lt;- lm(heights~age+autocov, data=age.data2)</code></pre>
<pre class="r"><code>semivariogram1(basic.model$residuals,age.data2$x,age.data2$y)
title(main=&#39;basic model&#39;)</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>semivariogram1(corrected.model$residuals,age.data2$x,age.data2$y) # clearly still autocorrelation in the residuals, but it is significantly reduced
title(main=&#39;corrected model&#39;)</code></pre>
<p><img src="SpatialAutocorrelation_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<pre class="r"><code>require (&quot;ape&quot;)                # see how much correlation in the residuals is reduced acording to moran&#39;s I with a generic 1/distance weight matrix
sample.distances&lt;-as.matrix(dist(cbind(age.data2$x,age.data2$y)))   # first calculate inverse distance weights (IDWs)
sample.distances.inverse&lt;-1/sample.distances
diag(sample.distances.inverse)&lt;-0             # get rid of infinity for 0&#39;s because 1 divide by 0 is infinity</code></pre>
<p>Our method of correcting for spatial autocorrelation was not totally successful. It did not eliminate all spatial correlation in the residuals of the linear model, but it did significantly reduce it. Moran’s I shows this as well.</p>
<pre class="r"><code>Moran.I(basic.model$residuals,sample.distances.inverse, scaled=TRUE)</code></pre>
<pre><code>## $observed
## [1] 0.1915665
## 
## $expected
## [1] -0.05263158
## 
## $sd
## [1] 0.06235829
## 
## $p.value
## [1] 9.00122e-05</code></pre>
<pre class="r"><code>Moran.I(corrected.model$residuals,sample.distances.inverse, scaled=TRUE) # Moran&#39;s I also shows significant reduction</code></pre>
<pre><code>## $observed
## [1] 0.1060123
## 
## $expected
## [1] -0.05263158
## 
## $sd
## [1] 0.06245652
## 
## $p.value
## [1] 0.01108307</code></pre>
<p><br> <br> While the method isn’t perfect, it did greatly improve the accuarcy of our model.</p>
<pre class="r"><code>summary(basic.model)$r.squared</code></pre>
<pre><code>## [1] 0.2027702</code></pre>
<pre class="r"><code>summary(corrected.model)$r.squared</code></pre>
<pre><code>## [1] 0.4798902</code></pre>
<p><br> <br> Because I generated the data I know that the true value of the coeficient parameter is 0.1</p>
<pre class="r"><code>basic.model$coefficients                  </code></pre>
<pre><code>## (Intercept)         age 
## 15.17893847  0.05334926</code></pre>
<pre class="r"><code>corrected.model$coefficients</code></pre>
<pre><code>## (Intercept)         age     autocov 
##  2.49298622  0.09571946  0.10108029</code></pre>
</div>
<div id="potential-pitfalls" class="section level2">
<h2>Potential pitfalls</h2>
<ol style="list-style-type: decimal">
<li><p>Most methods (including autocovariate regression) depend on the <em>spatial weight matrix.</em> Make sure you choose appropriate weights.</p></li>
<li><p>Autocovariat regression assumes stationarity. Values need to have the same mean, standard deviation, and autocorrelation for the model to be valid.</p></li>
<li><p>False autocorrilation, mapping bias or mapping heterogeneity can cause false autocorrelation in real data</p></li>
<li><p>Autocovariate regression is just one of many ways to account for SA. We suggest exploring other methods as well.</p></li>
</ol>
<ul>
<li>Eigenvector filtering with the <code>spdep</code> package.</li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
