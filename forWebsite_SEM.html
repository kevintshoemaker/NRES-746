<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Karly Feher and Margarete Walden" />

<meta name="date" content="2016-09-29" />

<title>Structural Equation Modeling</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/htmlwidgets-0.7/htmlwidgets.js"></script>
<script src="site_libs/d3-3.3.8/d3.min.js"></script>
<script src="site_libs/dagre-0.4.0/dagre-d3.min.js"></script>
<link href="site_libs/mermaid-0.3.0/dist/mermaid.css" rel="stylesheet" />
<script src="site_libs/mermaid-0.3.0/dist/mermaid.slim.min.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="site_libs/chromatography-0.1/chromatography.js"></script>
<script src="site_libs/DiagrammeR-binding-0.8.4/DiagrammeR.js"></script>
<script src="site_libs/viz-0.3/viz.js"></script>
<script src="site_libs/grViz-binding-0.8.4/grViz.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection</a>
    </li>
    <li>
      <a href="Time Series Lab.html">Time-series mini-lab</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="GAMs.html">GAMs</a>
    </li>
    <li>
      <a href="RMarkdown_FigureDemo.html">Publication-quality figures in R</a>
    </li>
    <li>
      <a href="Bayesian Networks.pptx">Bayesian Networks</a>
    </li>
    <li>
      <a href="GraphTheory.html">Graph Theory</a>
    </li>
    <li>
      <a href="NRES746_IPMs.pptx">Integrated Population Models</a>
    </li>
    <li>
      <a href="TimeSeries_heckler.html">Time Series Analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Structural Equation Modeling</h1>
<h4 class="author"><em>Karly Feher and Margarete Walden</em></h4>
<h4 class="date"><em>September 29, 2016</em></h4>

</div>


<div id="summary" class="section level2">
<h2><a name="top"/>Summary</a></h2>
<p>Structural Equation Modeling (SEM) is a framework for representing networks of causal relations. SEM quantifies the direct and indirect relationships between variables in a visual model. This page is designed to introduce the fundamental concepts of SEM, and will work through two examples using R.</p>
</div>
<div id="contents" class="section level2">
<h2>Contents</h2>
<ul>
<li><strong><a href="#intro">Introduction: What is Structural Equation Modeling?</a></strong></li>
<li><strong><a href="#cause">A Note on Causality</a></strong></li>
<li><strong><a href="#terminology">Terminology</a></strong></li>
<li><strong><a href="#identification">Identification, or Model Requirements</a></strong></li>
<li><strong><a href="#paths">Path Coefficients</a></strong></li>
<li><strong><a href="#expath">Example #1: Path Analysis</a></strong>
<ul>
<li><a href="#introex">Introduction to our example</a></li>
<li><a href="#linear">Linear regression/multiple regression</a></li>
<li><a href="#anova">ANOVA: Does a variable contribute to observed variation in the data?</a></li>
<li><a href="#stancoef">Standardize coefficients</a></li>
<li><a href="#mediation">Evaluate mediation</a></li>
<li><a href="#fit">D-Separation: Model fit</a></li>
</ul></li>
<li><strong><a href="#exlatent">Example #2: Latent Variable Structural Model</a></strong></li>
<li><strong><a href="#future">Future Directions</a></strong></li>
<li><strong><a href="#references">References</a></strong></li>
</ul>
</div>
<div id="introduction-what-is-structural-equation-modeling" class="section level2">
<h2><a name="intro"/>Introduction: What is Structural Equation Modeling?</a></h2>
<p><strong>Structural Equation Modeling (SEM) is an approach</strong> to modeling causal relationships between variables. “Path analysis” models the relationships between measured variables only. More generally, “structural models” investigate causal effects among variables. “Measurement models”, such as confirmatory factor analysis, are used to investigate latent variables and their relationships to measured indictor variables.</p>
<p><strong>SEM is also a framework</strong> that describes the process by which causal models are hypothesized, formulated, tested, analyzed, and modified: <img src="forWebsite_SEM_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /> <strong>Why use SEM?</strong> The primary strength of SEM is its ability to quantify causal pathways, including indirect (“mediated”) effects. SEM can incorporate multiple response variables, can investigate the nature of hypothesized latent variables, and can account for repeated measures.</p>
<p><strong>Multiple regression vs. SEM</strong> Multiple regression analysis allows for only a single dependent variable, whereas SEM allows for multiple dependent variables. SEM allows for variables to correlate, whereas regression adjusts for other variables in the model. Finally, regression analysis assumes perfect measurement, whereas SEM accounts for measurement error.</p>
<p><strong>What are the limitations of SEM?</strong> Distributional assumptions for SEM depend on the method used to estimate associations. For example, maximum likelihood estimation requires that the joint distribution of the endogenous variables is multivariate normal.</p>
<p>For a very concise overview of SEM, see <a href="http://userwww.sfsu.edu/efc/classes/biol710/path/SEMwebpage.htm">Ricka Stoelting’s webpage.</a></p>
<p>For more information on assumptions in SEM, see <a href="#num3">Kline 2012</a>.</p>
<p><a href="#top">back to top</a></p>
</div>
<div id="a-note-on-causality" class="section level2">
<h2><a name="cause"/>A Note on Causality</a></h2>
<p><a href="#num5">Pearl (2012)</a> described SEM “as a causal-inference engine that takes qualitative causal assumptions, data and queries as inputs and produces quantitative causal claims, conditional on the input assumptions, together with data-fitness ratings to well-defined statistical tests.” However, for much of its history, SEM was criticized for its claim to be able to quantify causal relationships. It wasn’t until d-separation was developed <a href="#num4">(Pearl 1988)</a> that SEM began to gain broad acceptance for its ability to make causal claims. D-separation translates the mathematical expression of causal hypotheses (otherwise known as directed graphs) into probability theory. This translation is what enables the statistical evaluation of the observed data to justify or reject the hypothesized model.</p>
<p>D-separation is used to deduce probabilistic independence upon conditioning in a causal system. The following is a short example explaining how d-separation works. Later, we will use d-separation in our path analysis example in R.</p>
<p>Let us supppose we have the following directed acyclic graph showing the effect of variable <em>A</em> on <em>C</em>, mediated by <em>B</em>:</p>
<div id="htmlwidget-ebb17f4d4870ab5943d1" style="width:672px;height:200px;" class="DiagrammeR html-widget"></div>
<script type="application/json" data-for="htmlwidget-ebb17f4d4870ab5943d1">{"x":{"diagram":"graph LR;A[A] -->B;B -->C"},"evals":[],"jsHooks":[]}</script>
<p>The first step is to identify all undirected paths in the graph. There is one: (1) <em>A</em> –&gt; <em>B</em> –&gt; <em>C</em>. Next, we will determine whether there is any causal influence along this path. In order to do this, we need to consider two conditions: (1) are there any non-colliding vertices in the conditioning set? and (2) is every collider vertex along the path a member of the conditioning set or has a causal descendant that is a member of the conditioning set? (Colliding vertices are vertices that are inactive, while non-colliding vertices are active in the absence of conditioning.) Let’s write all the d-separation statements we can make from this graph:</p>
<table style="width:92%;">
<colgroup>
<col width="45%" />
<col width="45%" />
</colgroup>
<thead>
<tr class="header">
<th>D-separation Statement</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>A</em> and <em>C</em> are not unconditionally independent.</td>
<td>There is a directed path between <em>A</em> and <em>C</em> because <em>B</em> is a non-colliding vertex between them.</td>
</tr>
<tr class="even">
<td><em>A</em> and <em>C</em> are independent conditioned on <em>B</em>.</td>
<td><em>B</em> is a non-colliding vertex. When it is conditioned, it blocks the causal pathway between <em>A</em> and <em>C</em>.</td>
</tr>
</tbody>
</table>
<p>At this point, we can translate these d-separation statements to probability distributions. Remember, we can say that two random variables are independent if the joint probability density is the product of the probability densities of each variable. Similarly, we can say that two random variables are independent given another variable if the joint probability of the two given the third equals the product of the probability density of the first given the third and the probability density of the second given the third. Therefore:</p>
<p>If <span class="math inline">\(I(X,\phi,Y)\)</span> then <span class="math inline">\(P(X,Y) = P(X)*P(Y)\)</span>, where <span class="math inline">\(I(X,\phi,Y)\)</span> means that <em>X</em> and <em>Y</em> are independent conditioned on an empty set <span class="math inline">\(\phi\)</span></p>
<p>If <span class="math inline">\(I(X,Z,Y)\)</span> then <span class="math inline">\(P(X,Y|Z) = P(X|Z)*P(Y|Z)\)</span></p>
<p>Now, we can write our expected statistical relationships using our observed data. We would expect <em>A</em> and <em>C</em> to be correlated because they are not unconditionally independent. However, when we compare data with <em>B</em> held constant, we would expect no correlation between <em>A</em> and <em>C</em>. Let’s simulate this dataset in R:</p>
<pre class="r"><code>#-- Variable A: 1000 observations from a normal distribution with mean 0 and standard deviation 1.
varA &lt;- rnorm(1000, 0, 1)               
#-- Variable B: Deterministic function depends on Variable A, plus error (normally distributed with a mean of 0 and standard deviation 1).
varB &lt;- 0.8*varA + rnorm(1000, 0, 1)
#-- Variable C: Deterministic function depends on Variable B, plus error (normally distributed with a mean of 0 and standard deviation 1).
varC &lt;-  0.5*varB + rnorm(1000, 0, 1)</code></pre>
<p>We want to test the two d-separation statements we made earlier:</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>A</em> and <em>C</em> are not unconditionally independent.</strong> This means our null hypothesis will be that the Pearson correlation (<span class="math inline">\(\rho\)</span>) <span class="math inline">\(= 0\)</span>. Then:</p>
<pre class="r"><code>#-- Use the &quot;cor&quot; function to find the Pearson correlation between Variable A and Variable C
pearsonR &lt;- cor(varA, varC, method=&quot;pearson&quot;)
#-- Because our two variables are normally distributed with any relationship between them being linear (we know this because we simulated the data from normal distributions and linear relationships), we can test whether the Pearson correlation coefficient is significantly different from zero using Fisher&#39;s Z transform:
z_stat &lt;- .5*(sqrt(1000-3))*log((1+pearsonR)/(1-pearsonR))
#-- The function &quot;pnorm&quot; returns the probability that the observed test statistic is more extreme than the critical value of the standard normal distribution.
pval_manual &lt;- 2*pnorm(-abs(z_stat)) #Multiplying by two and getting the negative of the absolute value of the test statistic gives the two-tailed test.
#-- Another option: use the &quot;cor.test&quot; function to do the test directly:
p_value_function &lt;- cor.test(varA, varC, method=&quot;pearson&quot;)
pearsonR
pval_manual
p_value_function</code></pre>
<pre><code>## [1] 0.5085589
## [1] 3.707821e-70
## 
##  Pearson&#39;s product-moment correlation
## 
## data:  varA and varC
## t = 18.659, df = 998, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4611030 0.5531138
## sample estimates:
##       cor 
## 0.5085589</code></pre>
<p>The p-value is &lt; 0.05. Therefore, we reject the null hypothesis that <em>A</em> and <em>C</em> are unconditionally independent. This result is consistent with our first d-separation statement.</p></li>
<li><p><strong><em>A</em> and <em>C</em> are independent conditioned on <em>B</em>.</strong> One way to test for conditional independence is to regress each variable on the conditioning variable, then compare the residuals as we did before for unconditional independence. That would look like this:</p>
<pre class="r"><code>resid_varA &lt;- resid(lm(varA~varB)) #The &quot;resid&quot; function extracts the residuals from the linear model.
resid_varC &lt;- resid(lm(varC~varB))
p_value_resid &lt;- cor.test(resid_varA,resid_varC,method=&quot;pearson&quot;,alternative=&quot;two.sided&quot;)
p_value_resid</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  resid_varA and resid_varC
## t = -1.7505, df = 998, p-value = 0.08034
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.116918538  0.006689557
## sample estimates:
##         cor 
## -0.05532647</code></pre>
<p>The p-value is &gt; 0.05. Therefore, we accept the null hypothesis that <em>A</em> and <em>C</em> are conditionally independent given <em>B</em>. This result is consistent with our second d-separation statement. Another way to test for conditional independence is to use the function “pcor” in the ggm library to calculate the partial Pearson correlation from the sample covariance matrix directly:</p>
<pre class="r"><code>library(ggm)
#-- First create a data frame
mydata &lt;- data.frame(varA, varC, varB)
#-- Calculate the partial Pearson correlation between Variables A and C, conditioning on B:
partialP &lt;- pcor(c(&quot;varA&quot;,&quot;varC&quot;,&quot;varB&quot;),var(mydata))
#-- Test for independence (same as before - using Fisher&#39;s z-transform):
z_stat &lt;- .5*(sqrt(1000-3))*log((1+partialP)/(1-partialP))
pval &lt;- 2*pnorm(-abs(z_stat))
partialP
pval</code></pre>
<pre><code>## [1] -0.05532647
## [1] 0.08033666</code></pre>
<p>Again, the p-value is &gt; 0.05.</p></li>
</ol>
<p>As you can see, our observed data agree with all of the d-separation statements from our model. This means that we can interpet our model as demonstrating that <em>C</em> is indirectly caused by <em>A</em>, with the effects of <em>A</em> completely mediated by <em>B</em>.</p>
<p>This is a very limited introduction to d-separation and the topic of causality in SEM. For more information, we highly recommend the following: <a href="#num5">Pearl 2012</a> and <a href="#num6">Shipley 2016</a>.</p>
<p><a href="#top">back to top</a></p>
</div>
<div id="terminology" class="section level2">
<h2><a name="terminology"/>Terminology</a></h2>
<p>The general form for expressing relationships among observed variables using SEM is:</p>
<p><span class="math display">\[
y=By+\Gamma x+\zeta
\]</span> where <strong><em>y</em></strong> is a vector of endogenous variables, <strong><em>x</em></strong> is a vector of exogenous variables, <strong><em>B</em></strong> and <strong><span class="math inline">\(\Gamma\)</span></strong> are coefficient matrices, and <strong><span class="math inline">\(\zeta\)</span></strong> is a vector of error variables with non-zero elements for each non-zero element in the <strong><em>y</em></strong> vector on the left side of the equation.</p>
<p>For an example, let us consider the following model: <div id="htmlwidget-f3ed4687f82bf4a08077" style="width:672px;height:288px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-f3ed4687f82bf4a08077">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = <x<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n  \"b\" [label = <y<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = <y<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n  \"d\" [label = <y<FONT POINT-SIZE=\"8\"><SUB>3\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n\"a\"->\"b\" [label = <&gamma;<FONT POINT-SIZE=\"8\"><SUB>11\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"a\"->\"c\" [label = <&gamma;<FONT POINT-SIZE=\"8\"><SUB>21\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"b\"->\"d\" [label = <&beta;<FONT POINT-SIZE=\"8\"><SUB>31\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"c\"->\"d\" [label = <&beta;<FONT POINT-SIZE=\"8\"><SUB>32\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"a\"->\"a\" [label = <&delta;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"gray\"] \n\"b\"->\"b\" [label = <&zeta;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"gray\"] \n\"c\"->\"c\" [label = <&zeta;<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, color = \"gray\"] \n\"d\"->\"d\" [label = <&zeta;<FONT POINT-SIZE=\"8\"><SUB>3\u003c/SUB>\u003c/FONT>>, color = \"gray\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script></p>
<p>To express this model in equation form: <span class="math display">\[
\begin{align*}
\begin{bmatrix}
y_1\\
y_2\\
y_3\\
\end{bmatrix} &amp;= 
\begin{bmatrix}
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0\\
\beta_{31} &amp; \beta_{32} &amp; 0\\
\end{bmatrix}
\begin{bmatrix}
y_1\\
y_2\\
y_3\\
\end{bmatrix} + 
\begin{bmatrix}
\gamma_{11}\\
\gamma_{21}\\
0\\
\end{bmatrix} 
\begin{bmatrix}
x_1\\
\end{bmatrix} +
\begin{bmatrix}
\zeta_{1}\\
\zeta_{2}\\
\zeta_{3}\\
\end{bmatrix}
\end{align*}
\]</span> In this model, <span class="math inline">\(x_1\)</span> is an exogenous variable, and <span class="math inline">\(\delta_{1}\)</span> is its error term.</p>
<p><span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, and <span class="math inline">\(y_3\)</span> are endogenous variables, and their error terms are represented by <span class="math inline">\(\zeta\)</span>.</p>
<p>The path coefficients are shown by <span class="math inline">\(\gamma\)</span> (from exogenous to endogenous variable) and <span class="math inline">\(\beta\)</span> (from/to endogenous variables).</p>
<p>Latent variables are usually shown in circles rather than rectangles. The path coefficients from a latent variable to its indicator variable(s) are represented by <span class="math inline">\(\lambda\)</span>.</p>
<p><a href="#top">back to top</a></p>
</div>
<div id="identification-or-model-requirements" class="section level2">
<h2><a name="identification"/>Identification, or Model Requirements</a></h2>
<p>Identification refers to the ability of the model to uniquely estimate parameters. In order for a model to be identified, there must be enough knowns to identify the unknowns. For example, using algebra, if we wanted to estimate two parameters, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, we would need two equations in order to find the unique solution for the pair <span class="math inline">\((x,y)\)</span>. Applying the same principle to our example model above, our known set of parameters are the means, variances, and covariances for the observed variables. Because the unknown parameters, in this case the path coefficients, are functions of the known parameters and can be uniquely estimated, this model is identified.</p>
<p>To know whether a model is underidentified (too few knowns to estimate the unknowns) or overidentified (preferred, because this gives you degrees of freedom for the overall chi-square fit test), you can use the T-rule:</p>
<p><span class="math display">\[
\textrm{number of parameters} \leq \frac{\textrm{number of variables}(\textrm{number of variables} + 1)}{2}
\]</span></p>
<p>In the above model, we have 8 parameters (4 path coefficients + 4 error terms) and 4 variables. This gives us <span class="math inline">\(8\leq \frac{4(4+1)}{2} \Longrightarrow 8\leq \frac{20}{2} \Longrightarrow 8\leq 10\)</span>, therefore this model is overidentified with 2 degrees of freedom.</p>
<p>Certain model structures, complex models, small samples, or high multicollinearity can lead to issues with identification as well. A more thorough treatment of the issue of identification can be found on <a href="http://davidakenny.net/cm/identify_formal.htm">David Kenny’s webpage</a>.</p>
<p><a href="#top">back to top</a></p>
</div>
<div id="path-coefficients" class="section level2">
<h2><a name="paths"/>Path Coefficients</a></h2>
<p>Path coefficients are estimated from the covariance matrix, although models are usually displayed with correlation coefficients because they are easier to interpret. (Remember: a correlation coefficient is simply the covariance standardized by the standard deviations, and the correlation coefficient takes values from -1 to 1.)</p>
<p>Variables are shown with the standardized regression coefficient (<span class="math inline">\(r^2\)</span>).</p>
<p>Error terms are shown as <span class="math inline">\(1-r^2\)</span>, or, alternatively the path from the error term to the variable is shown as <span class="math inline">\(\sqrt{1-r^2}\)</span>.</p>
<p>Let’s use our previous example to show how to estimate the parameters of the model:</p>
<pre class="r"><code>cor(mydata) #-- Returns the correlation matrix (for displaying the path coefficients)
varB_r2 &lt;- summary(lm(varB~varA))$r.squared #-- Returns the regression coefficient for Variable B.
varC_r2 &lt;- summary(lm(varC~varB))$r.squared #-- Returns the regression coefficient for Variable C.
varB_r2
varC_r2
1-varB_r2 #-- Returns the value for the error term for Variable B.
1-varC_r2 #-- Returns the value for the error term for Variable C.</code></pre>
<pre><code>##           varA      varC      varB
## varA 1.0000000 0.5085589 0.8227313
## varC 0.5085589 1.0000000 0.6472725
## varB 0.8227313 0.6472725 1.0000000
## [1] 0.6768867
## [1] 0.4189617
## [1] 0.3231133
## [1] 0.5810383</code></pre>
<p>Here is the model with those estimates:</p>
<div id="htmlwidget-066bf8eb4369d594d341" style="width:672px;height:192px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-066bf8eb4369d594d341">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = \"A\", shape = \"rectangle\", color = \"black\"] \n  \"b\" [label = <B  r<FONT POINT-SIZE=\"8\"><SUP>2\u003c/SUP>\u003c/FONT>=0.582>, shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = <C  r<FONT POINT-SIZE=\"8\"><SUP>2\u003c/SUP>\u003c/FONT>=0.521>, shape = \"rectangle\", color = \"black\"] \n\"a\"->\"b\" [label = \"0.763\", color = \"black\"] \n\"b\"->\"b\" [label = \"0.418\", color = \"gray\"] \n\"b\"->\"c\" [label = \"0.722\", color = \"black\"] \n\"c\"->\"c\" [label = \"0.479\", color = \"gray\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script>
<p><a href="#top">back to top</a></p>
</div>
<div id="example-1-path-analysis" class="section level2">
<h2><a name="expath"/>Example #1: Path Analysis</a></h2>
<div id="workflow-from-regressions-to-sem" class="section level3">
<h3>Workflow: From Regressions to SEM</h3>
<ul>
<li><strong><a href="#introex">Introduction to our example</a></strong></li>
<li><strong><a href="#linear">Linear regression/multiple regression</a></strong></li>
<li><strong><a href="#anova">ANOVA: Does a variable contribute to observed variation in the data?</a></strong></li>
<li><strong><a href="#stancoef">Standardize coefficients</a></strong></li>
<li><strong><a href="#mediation">Evaluate mediation</a></strong></li>
<li><strong><a href="#fit">D-Separation: Model fit</a></strong></li>
</ul>
</div>
<div id="example-dataset" class="section level3">
<h3><a name="introex"/>Example Dataset</a></h3>
<p>Let’s look at an example from <a href="#num1">Grace and Keeley 2006</a>:</p>
<p>The goal was to understand patterns in species richness over five years following fire. Parameters were measured in 90 x 1000 <span class="math inline">\(m^2\)</span> plots. For this exercise, we’ll focus on four measured variables: distance to ocean, abiotic index, spatial heterogeneity, and species richness.</p>
<p>Our preliminary hypothesized model is:</p>
<div id="htmlwidget-27b43bc28042ed6d4ce9" style="width:672px;height:192px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-27b43bc28042ed6d4ce9">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = \"distance\", shape = \"rectangle\", color = \"black\"] \n  \"b\" [label = \"abiotic\", shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = \"hetero\", shape = \"rectangle\", color = \"black\"] \n  \"d\" [label = \"richness\", shape = \"rectangle\", color = \"black\"] \n\"a\"->\"b\" [color = \"black\"] \n\"a\"->\"c\" [color = \"black\"] \n\"b\"->\"d\" [color = \"black\"] \n\"c\"->\"d\" [color = \"black\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script>
<p>First, read in the data. The dataset “keeley_rawdata.csv” is in the “Files” folder on WebCampus.</p>
<pre class="r"><code>keeley &lt;- read.csv(&quot;./keeley_rawdata.csv&quot;,header=TRUE)
summary(keeley) #-- Look at data</code></pre>
<pre><code>##     distance          elev           abiotic           age       
##  Min.   :37.04   Min.   :  60.0   Min.   :32.59   Min.   : 3.00  
##  1st Qu.:39.46   1st Qu.: 202.5   1st Qu.:43.81   1st Qu.:15.00  
##  Median :51.77   Median : 400.0   Median :48.04   Median :25.00  
##  Mean   :49.23   Mean   : 424.7   Mean   :49.24   Mean   :25.57  
##  3rd Qu.:58.40   3rd Qu.: 630.0   3rd Qu.:54.90   3rd Qu.:35.00  
##  Max.   :60.72   Max.   :1225.0   Max.   :70.46   Max.   :60.00  
##      hetero          firesev          cover              rich      
##  Min.   :0.3842   Min.   :1.200   Min.   :0.05558   Min.   :15.00  
##  1st Qu.:0.6246   1st Qu.:3.700   1st Qu.:0.48769   1st Qu.:37.00  
##  Median :0.6843   Median :4.300   Median :0.63712   Median :50.00  
##  Mean   :0.6833   Mean   :4.565   Mean   :0.69123   Mean   :49.23  
##  3rd Qu.:0.7684   3rd Qu.:5.550   3rd Qu.:0.91468   3rd Qu.:62.00  
##  Max.   :0.8779   Max.   :9.200   Max.   :1.53541   Max.   :85.00</code></pre>
</div>
<div id="linear-regressionmultiple-regression" class="section level3">
<h3><a name="linear"/>Linear regression/multiple regression</a></h3>
<p>Our first step is to perform linear regression for each piece of our hypothesized model. These “piecewise” linear regressions tell us some information about our model variables. We can determine whether individual variables are significant in each regression by looking at the p-values for the parameter estimates. Also, we can find the <span class="math inline">\(r^2\)</span> values for our endogenous variables from the “lm” function in R.</p>
<pre class="r"><code>abioticLM &lt;- lm(abiotic~distance, data=keeley)    #-- Abiotic caused by distance
heteroLM &lt;- lm(hetero~distance, data=keeley)      #-- Heterogeneity caused by distance
richnessLM &lt;- lm(rich~abiotic+hetero,data=keeley) #-- Richness caused by abiotic and heterogeneity
summary(abioticLM)$coefficients
summary(heteroLM)$coefficients
summary(richnessLM)$coefficients
summary(abioticLM)$r.squared
summary(heteroLM)$r.squared
summary(richnessLM)$r.squared</code></pre>
<pre><code>##               Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 29.5537053 4.11762269 7.177371 2.142310e-10
## distance     0.3998271 0.08233372 4.856176 5.158424e-06
##                Estimate  Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 0.461802396 0.065045361 7.099698 3.063408e-10
## distance    0.004499205 0.001300611 3.459300 8.370677e-04
##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) -21.6224780 10.0651511 -2.148252 3.447302e-02
## abiotic       0.8135516  0.1746343  4.658602 1.136795e-05
## hetero       45.0702119 11.6796513  3.858866 2.181692e-04
## [1] 0.2113455
## [1] 0.1197074
## [1] 0.3667967</code></pre>
</div>
<div id="anova-does-a-varible-contribute-to-observed-variation-in-the-data" class="section level3">
<h3><a name="anova"/>ANOVA: Does a varible contribute to observed variation in the data?</a></h3>
<p>Remember, an ANOVA is linear regression. An alternative to looking at the summaries for each “lm” object above is to use the “aov” function to determine whether our three hypothesized linear relationships are significant:</p>
<pre class="r"><code>summary(aov(abioticLM))
summary(aov(heteroLM))
summary(aov(richnessLM))</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## distance     1   1109    1109   23.58 5.16e-06 ***
## Residuals   88   4139      47                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## distance     1 0.1405 0.14045   11.97 0.000837 ***
## Residuals   88 1.0329 0.01174                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## abiotic      1   5248    5248   35.51 5.28e-08 ***
## hetero       1   2201    2201   14.89 0.000218 ***
## Residuals   87  12859     148                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Notice that the p-values for significance are the same as above.</p>
<p>Because all variables are significant in our piecewise linear regressions, we will retain all terms in our model.</p>
</div>
<div id="standardize-coefficients" class="section level3">
<h3><a name="stancoef"/>Standardize coefficients</a></h3>
<p>Our next step is to estimate the parameters in our model. Remember, our parameters are the path coefficients and error terms. We can use the <span class="math inline">\(r^2\)</span> values from our earlier piecewise linear regressions for our endogenous variables and error terms because we are retaining all of those variables in our model. Now we need to estimate the path coefficients.</p>
<p>In our earlier explanation of path coefficients, we stated that model estimation uses the covariance matrix. However, for display and interpretation, it is usual to standardize the path coefficients by dividing the covariance by the standard deviations. There are two ways to do this in R: you can use the “cor” function to display the correlation matrix for all your variables, and then fill in the values. The other way is to use the “lm.beta” function in the package “QuantPsyc”. This will return the standardized path coefficient(s) directly for individual piecewise linear regressions.</p>
<pre class="r"><code>cor_matrix &lt;- cor(keeley[,c(1,3,5,8)])      #-- The [,] is matrix notation in which the (row,column) numbers can be specified. In this case, we&#39;re saying to use only the numbered columns of data to calculate the correlation matrix.
cor_matrix

library(QuantPsyc)
lm.beta(abioticLM)   #-- Path from &quot;distance&quot; to &quot;abiotic&quot;.
lm.beta(heteroLM)    #-- Path from &quot;distance&quot; to &quot;hetero&quot;.
lm.beta(richnessLM)  #-- &quot;abiotic&quot; = path from &quot;abiotic&quot; to &quot;richness&quot;; &quot;hetero&quot; = path from &quot;hetero&quot; to richness.</code></pre>
<pre><code>##           distance   abiotic    hetero      rich
## distance 1.0000000 0.4597233 0.3459875 0.5844775
## abiotic  0.4597233 1.0000000 0.2766418 0.5083485
## hetero   0.3459875 0.2766418 1.0000000 0.4569914
## rich     0.5844775 0.5083485 0.4569914 1.0000000
##  distance 
## 0.4597233 
##  distance 
## 0.3459875 
##   abiotic    hetero 
## 0.4135769 0.3425788</code></pre>
<p>Note that the “lm.beta” function and the “cor” function return the same values.</p>
<p>We will now proceed to fill in our estimates for our model:</p>
<div id="htmlwidget-db66e1e09c4bf16d404a" style="width:672px;height:288px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-db66e1e09c4bf16d404a">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = \"distance\", shape = \"rectangle\", color = \"black\"] \n  \"b\" [label = <abiotic  r<FONT POINT-SIZE=\"8\"><SUP>2\u003c/SUP>\u003c/FONT>=0.21>, shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = <hetero  r<FONT POINT-SIZE=\"8\"><SUP>2\u003c/SUP>\u003c/FONT>=0.12>, shape = \"rectangle\", color = \"black\"] \n  \"d\" [label = <richness  r<FONT POINT-SIZE=\"8\"><SUP>2\u003c/SUP>\u003c/FONT>=0.36>, shape = \"rectangle\", color = \"black\"] \n\"a\"->\"b\" [label = \"0.46\", color = \"black\"] \n\"a\"->\"c\" [label = \"0.35\", color = \"black\"] \n\"b\"->\"d\" [label = \"0.41\", color = \"black\"] \n\"c\"->\"d\" [label = \"0.34\", color = \"black\"] \n\"b\"->\"b\" [label = \"0.889\", color = \"grey\"] \n\"c\"->\"c\" [label = \"0.938\", color = \"grey\"] \n\"d\"->\"d\" [label = \"0.800\", color = \"grey\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="evaluate-mediation" class="section level3">
<h3><a name="stancoef"/>Evaluate mediation</a></h3>
<p>In a mediation relationship, there is a direct effect between an independent variable and a dependent variable. There are also indirect effects between an independent variable and a mediator variable, and between a mediator variable and a dependent variable.</p>
<p>The degree to which the direct effect changes as a result of including the mediating variable is referred to as the mediational effect. Testing for mediation involves running a series of regression analyses for all of the causal pathways and some method of estimating a change in direct effect.</p>
<p>In our example, we have two mediations: the effect of “distance” on “richness” is mediated by two mediator variables: “abiotic” and “hetero”. We need to investigate whether these mediations are fully explaining the effect of “distance” on “richness.” One way to determine this is to look at the residuals from the linear regression of “richness” by “abiotic” and “hetero.” The residuals will be the remaining variance in “richness” not explained by “abiotic” and “hetero.” We would expect no structure in the residuals if “abiotic” and “hetero” are fully explaining all effects. However, if we saw a relationship between the residuals and “distance”, we would suspect that there is some effect of “distance” on “richness” that is not being sufficiently explained, or mediated, by “abiotic” and “hetero.” Let’s do this in R:</p>
<pre class="r"><code>#-- First we&#39;ll extract the residuals from the linear regression of richness by abiotic and hetero.
resid_rich &lt;- resid(richnessLM)
#-- Next, we&#39;ll perform a linear regression of these residuals by distance. If this relationship is significant, it would indicate we are missing a path in our model because abiotic and hetero are incomplete mediators.
residLM &lt;- lm(resid_rich~distance,data=keeley)
summary(residLM)$coefficients   #-- The relationship is signficant (we could also test this using the &quot;aov&quot; function)
#-- To help visualize this, we&#39;ll plot the residuals against distance and show the fitted linear regression.
plot(resid_rich~distance,data=keeley)
abline(residLM,col=&quot;blue&quot;,lwd=2)</code></pre>
<p><img src="forWebsite_SEM_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) -23.2326282  6.8078379 -3.412629 0.0009741965
## distance      0.4718762  0.1361258  3.466472 0.0008176835</code></pre>
<p>As distance increases, the values of the residuals increase. Because this relationship is signficant, we conclude that we need to add a path in our model from “distance” to “richness”.</p>
<p>Now, we would need to perform a new regression: <code>lm(rich~abiotic+hetero+distance)</code> in order to estimate the new <span class="math inline">\(r^2\)</span> for “richness”. For the purposes of this example, we are going to show what would happen if you went directly to testing goodness of fit without investigating mediation. (In the end, the final model chosen will be the same by either process.)</p>
</div>
<div id="d-separation-and-goodness-of-fit" class="section level3">
<h3><a name="fit"/>D-separation and goodness of fit</a></h3>
<p>Next, we evaluate goodness of fit. Our first check is to use d-separation to determine whether the observed data adequately account for the modeled probabilities.</p>
<p>Rewording from earlier, the d-separation criterion for any pair of variables involves:</p>
<ol style="list-style-type: decimal">
<li>Controlling for common ancestors that could generate correlations between the pair.</li>
<li>Controlling for causal connections through multi-link directed pathways via parents.</li>
<li>Not controlling for common descendent variables.</li>
</ol>
<p>In the context of our model (ignoring the path from “distance” to “richness”), if we wanted to formulate a d-separation statement for the path from “distance” to “richness”:</p>
<ol style="list-style-type: decimal">
<li>There are no common ancestors.</li>
<li>We include the parents of “richness” that are part of mediating pathways: “abiotic”&quot; and “hetero”.</li>
<li>There are no common descendents.</li>
</ol>
<p>Thus, we conclude that the residuals for “distance” and “richness” are predicted to be uncorrelated.</p>
<p>Because writing out the entire set of all possible d-statements would involve overlap and would increase the possibility of Type II error, we will identify what is called the “basis set”, which is the minimum number of d=separation statements that is sufficient to predict the entire set of d-separation statements. In this case:</p>
<p><span class="math display">\[
\textrm{distance}\perp \textrm{richness} | (\textrm{abiotic, hetero})\\
\textrm{biotic}\perp \textrm{hetero} | (\textrm{distance})
\]</span> Each of these d-separated statements predicts a (conditional) probabilistic independence:</p>
<ol style="list-style-type: decimal">
<li>“Distance” is d-separated from “richness” when conditioned on “abiotic” and “hetero”.</li>
<li>“Biotic” is d-separated from “hetero” when conditioned on “distance”.</li>
</ol>
<p>The composite probability for the basis set is Fisher’s C test: <span class="math display">\[
C=-2*\sum_{i=1}^{k}Ln(p_{i})
\]</span> Where <span class="math inline">\(p_{i}=\)</span> p-values of all tests of conditional independence; <em>C</em> has a chi-square distribution with 2k degrees of freedom; <em>k</em> = number of elements of the basis set.</p>
<p>We conclude that the predicted variables are conditionally independent if <span class="math inline">\(p&gt;05\)</span>. If <span class="math inline">\(p&lt;.05\)</span>, there are likely to be missing paths (in order to account for the lack of independence).</p>
<p>We’ll use some built-in functions in R to determine the fit test:</p>
<pre class="r"><code>library(piecewiseSEM)
#-- First, build a model object using the correct syntax:
modList &lt;- list(
  lm(abiotic~distance, data=keeley),
  lm(hetero~distance,data=keeley),
  lm(rich~abiotic+hetero,data=keeley)
)
#-- Next, run the Fisher C test:
sem.fisher.c(modList,data=keeley,.progressBar=FALSE)</code></pre>
<pre><code>##   fisher.c df p.value
## 1    21.86  4       0</code></pre>
<p>Because the p-value &lt; 0.05, we know that the model is missing a pathway. Let’s suppose that pathway is a direct pathway from “distance to ocean” to “richness” (as we concluded from our evaluation of mediation). So, that means our new script will be:</p>
<pre class="r"><code>#-- First, build a model object using the correct syntax:
modList2 &lt;- list(
  lm(abiotic~distance, data=keeley),
  lm(hetero~distance,data=keeley),
  lm(rich~abiotic+hetero+distance,data=keeley) #-- note that we&#39;ve added &quot;distance&quot; as a direct cause of &quot;richness&quot;
)
#-- Next, run the Fisher C test:
sem.fisher.c(modList2,data=keeley,.progressBar=FALSE)</code></pre>
<pre><code>##   fisher.c df p.value
## 1     3.35  2   0.187</code></pre>
<p>Now, our p-value is &gt; 0.05. This means we can conclude our model is supported by our data. We can use the package semPlot to visualize our final model:</p>
<pre class="r"><code>library(lavaan)
library(semPlot)
#--First create an object that stores the relationships between the variables:
modList3 &lt;- &#39;
  abiotic~distance
  hetero~distance
  rich~abiotic+hetero+distance
&#39;
#--Next, we need to create a model object that &quot;semPaths&quot; can work with. In this case, we use the &quot;sem&quot; function in the library &quot;lavaan&quot; to create our SEM model object &quot;fullfit1&quot;.
fullfit1 &lt;- sem(modList3, data=keeley)
#--Then visualize the model:
semPaths(fullfit1,rotate=1,layout=&quot;spring&quot;,&quot;std&quot;)</code></pre>
<p><img src="forWebsite_SEM_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Note that the “sem” function in the package “lavaan” uses maximum likelihood estimation by default for estimating the model parameters. This compares the modeled covariance matrix to the observed covariance matrix. For path analysis, the ML estimates are likely to be very close to the piecewise estimates we generated earlier using Least Squares from our linear regressions.</p>
<p>Using this model, we would make the following inferences:</p>
<ol style="list-style-type: decimal">
<li>46% of the variance of “richness” is accounted for by the path model, 54% is residual variance.</li>
<li>The correlation between “distance” and “richness” is the sum of the products of the path coefficients along each path. Thus, <span class="math inline">\((0.27*0.46)+(0.38)+(0.26*0.35)=0.595\)</span>, meaning that 59.5% of the correlation between “distance” and “richness” is due to the direct and indirect effects of “distance” on “richness”.</li>
</ol>
<p>What other conclusions might we draw from this model?</p>
<p><a href="#top">back to top</a></p>
</div>
</div>
<div id="example-2-latent-variable-structural-model" class="section level2">
<h2><a name="exlatent"/>Example #2: Latent Variable Structural Model</a></h2>
<p>Latent variables are variables or constructs that cannot be measured, and so instead we measure “indicator” variables and use those to understand the latent variable. Latent variables may also be called “factors”. They are more commonly encountered in psychology (<em>e.g.</em>, “intelligence”) and sociology (<em>e.g.</em>, “socioeconomic status”). Reasons to consider latent variables in ecology can include: repeated measures (<em>e.g.</em>, point counts on three occasions for estimating “bird density”), multiple samples (<em>e.g.</em>, three observers independently counting birds for estimating “bird density”), or multiple methods (<em>e.g.</em>, point counts and automated sound recordings for estimating “bird density”).</p>
<p>A latent variable is modeled as the cause of the indicator variables. The correlation among the indicator variables is due to the effect of the latent variable. Error for each indicator variable is due to measurement error plus stray causes.</p>
<p>In SEM, latent variables are shown in circles while measured variables are shown in squares. The model for a latent variable with exogenous indicators is parameterized follows:</p>
<div id="htmlwidget-e8a63fdace468f9eb72f" style="width:672px;height:288px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-e8a63fdace468f9eb72f">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = \"&xi;\", shape = \"circle\", color = \"black\"] \n  \"b\" [label = <x<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = <x<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n\"a\"->\"b\" [label = <&lambda;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"a\"->\"c\" [label = <&lambda;<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"b\"->\"b\" [label = <&delta;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n\"c\"->\"c\" [label = <&delta;<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script>
<p>The model for a latent variable with endogenous indicators is parameterized as follows:</p>
<div id="htmlwidget-1bd868d3c09391879422" style="width:672px;height:288px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1bd868d3c09391879422">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = \"&eta;\", shape = \"circle\", color = \"black\"] \n  \"b\" [label = <y<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = <y<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, shape = \"rectangle\", color = \"black\"] \n\"a\"->\"a\" [label = \"&zeta;\", color = \"gray\"] \n\"a\"->\"b\" [label = <&lambda;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"a\"->\"c\" [label = <&lambda;<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"b\"->\"b\" [label = <&epsilon;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n\"c\"->\"c\" [label = <&epsilon;<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script>
<p>Issues of identification arise when modeling latent variables because we only know the correlation between our indicator variables but need to estimate path coefficients and the latent variable error. There are options for how to handle this situation: (1) fix the value of the latent variable error to 1 and (if &lt; 3 indicator variables) standardize the indicator measures so that they are given equal weight and the path coeffients are the same, or (2) fix one path coefficient to 1. In practical terms, the choice depends on what units you want for your latent variable. The first option (fixing the latent error variance to 1) has the effect of expressing the scale of the latent variable in units of standard deviations, meaning it acts as a standardised normal variable. The second option essentially states that a one-unit change in the latent variable results in a change in the measured variable by one unit of that measure’s scale.</p>
<p>Let’s work through an example. Suppose that we thought two of the variables in our fire model, “distance to ocean” and “abiotic”, were in fact two measurements of an underlying latent variable that we’ll call “terrain”. Our hypothesized model is as follows:</p>
<div id="htmlwidget-e5b7044df20c12180c61" style="width:672px;height:384px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-e5b7044df20c12180c61">{"x":{"diagram":"digraph {\n\ngraph [rankdir=LR]\n\n\n  \"a\" [label = \"terrain;\", shape = \"circle\", color = \"black\"] \n  \"b\" [label = \"heterogeneity\", shape = \"rectangle\", color = \"black\"] \n  \"c\" [label = \"richness\", shape = \"rectangle\", color = \"black\"] \n  \"d\" [label = \"abiotic\", shape = \"rectangle\", color = \"black\"] \n  \"e\" [label = \"distance\", shape = \"rectangle\", color = \"black\"] \n\"a\"->\"b\" [label = <&gamma;<FONT POINT-SIZE=\"8\"><SUB>11\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"a\"->\"c\" [label = <&gamma;<FONT POINT-SIZE=\"8\"><SUB>12\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"b\"->\"c\" [label = <&beta;<FONT POINT-SIZE=\"8\"><SUB>12\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"b\"->\"b\" [label = <&zeta;<FONT POINT-SIZE=\"8\"><SUB>1\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n\"c\"->\"c\" [label = <&zeta;<FONT POINT-SIZE=\"8\"><SUB>2\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n\"a\"->\"d\" [label = <&lambda;<FONT POINT-SIZE=\"8\"><SUB>x1\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"a\"->\"e\" [label = <&lambda;<FONT POINT-SIZE=\"8\"><SUB>x2\u003c/SUB>\u003c/FONT>>, color = \"black\"] \n\"d\"->\"d\" [label = <&delta;<FONT POINT-SIZE=\"8\"><SUB>x1\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n\"e\"->\"e\" [label = <&delta;<FONT POINT-SIZE=\"8\"><SUB>x2\u003c/SUB>\u003c/FONT>>, color = \"grey\"] \n\"a\"->\"a\" [label = \"1\", color = \"grey\"] \n}","config":{"engine":null,"options":null}},"evals":[],"jsHooks":[]}</script>
<p>Now let’s use R to test our model against the data:</p>
<pre class="r"><code>#-- First see if the correlations between distance and elevation are strong enough to support the idea of a latent variable:
cor(keeley[,c(1,3)]) #Correlation is fairly high (&gt;.4)
#-- Create the model relationships (#Note the &#39;=~&#39; to signify a latent variable):
modListLatent &lt;- &#39;
terrain =~ abiotic+distance 
hetero ~ terrain
rich ~ terrain + hetero
&#39;
#-- We&#39;ll use the &quot;sem&quot; function in library &quot;Lavaan&quot; to test whether our model including the latent variable is a good fit:
#-- Note: &quot;std.lv=TRUE&quot; fixes the latent variable variance to 1&quot;.
fullfitlatent1 &lt;- sem(modListLatent,data=keeley,std.lv=TRUE)
fullfitlatent1 #-- The p-value for the Fisher&#39;s C test is &gt;0.05, meaning the overall fit is good.
#-- Now let&#39;s visualize our model:
semPaths(fullfitlatent1,rotation=2,layout=&quot;tree2&quot;,&quot;std&quot;,intercepts=FALSE)</code></pre>
<p><img src="forWebsite_SEM_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre><code>##           distance   abiotic
## distance 1.0000000 0.4597233
## abiotic  0.4597233 1.0000000
## lavaan (0.5-22) converged normally after  67 iterations
## 
##   Number of observations                            90
## 
##   Estimator                                         ML
##   Minimum Function Test Statistic                0.069
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.793</code></pre>
<p>Which model is better: the one with the latent variable “terrain” or the one with only the four measured variables? Because our models are not nested, we cannot use the difference between their chi-square statistics to compare the two. Essentially, the final chosen model needs to be based on a strong hypothesized model and theoretical foundation to justify the relationships.</p>
<p>For more information about latent variables, see: <a href="http://web.pdx.edu/~newsomj/semclass/ho_latent.pdf">Jason Newsom’s webpage</a> and <a href="http://faculty.cas.usf.edu/mbrannick/regression/SEM.html">Michael Brannick’s webpage</a></p>
<p><a href="#top">back to top</a></p>
</div>
<div id="future-directions" class="section level2">
<h2><a name="future"/>Future Directions</a></h2>
<p>Structural Equation Modeling seems to have been largely avoided in ecology for much of its history. This may have been due to the misunderstandings surrounding its ability to justify causal relationships or it may have been due to a perceived lack of need for incorporating latent variables into ecological models. However, SEM has become more popular in the last decade and papers reporting SEM are becoming more common.</p>
<p>A growing area of interest is to attempt to translate SEM into predictive probabilistic networks. In other words, how can you move from a SEM describing existing data to a predictive model of phemonema?</p>
<p>For a good (but dense!) look into SEM in ecology, including recent advances, see <a href="#num2">Grace et al. 2012</a>.</p>
<p><a href="#top">back to top</a></p>
</div>
<div id="references" class="section level2">
<h2><a name="references"/>References</a></h2>
<p><a name="num1"/>Grace, J. B., and J. E. Keeley. 2006. </a>A structural equation model analysis of postfire plant diversity in California shrublands. <em>Ecological Applications</em>, 16(2), 503-514. <a href="http://www.werc.usgs.gov/OLDsitedata/seki/pdfs/k2006_grace_sem_ea.pdf">Available here.</a></p>
<p><a name="num2"/>Grace, J. B., D. R. Schoolmaster Jr., G. R. Guntenspergen, A. M. Little, B. R. Mitchell, K. M. Miller, and E. W. Schweiger. 2012. </a>Guidelines for a graph-theoretic implementation of structural equation modeling. <em>Ecosphere</em> 3(8):73. <a href="http://onlinelibrary.wiley.com/doi/10.1890/ES12-00048.1/full">Available here.</a></p>
<p><a name="num3"/>Kline, R. B. 2012. </a>“Assumptions in structural equation modeling.” <em>In</em> Hoyle, R. H. (editor) <em>Handbook of structural equation modeling</em>. New York: Guilford Press. p 111-125.<a href="http://psychology.concordia.ca/fac/kline/library/k13a.pdf">Available here.</a></p>
<p><a name="num4"/>Pearl, J. </a><em>Probabilistic Reasoning in Intelligent Systems</em>. San Mateo: Morgan Kaufmann, 1988.</p>
<p><a name="num5"/>Pearl, J. 2012. </a>“The causal foundations of structural equation modeling.” <em>In</em> Hoyle, R. H. (editor) <em>Handbook of structural equation modeling</em>. New York: Guilford Press. p 68-91. <a href="http://ftp.cs.ucla.edu/pub/stat_ser/r370.pdf">Available here.</a></p>
<p><a name="num6"/>Shipley, B. </a><em>Cause and Correlation in Biology: A User’s Guide to Path Analysis, Structural Equations and Causal Inference with R.</em> Cambridge: Cambridge University Press, 2016.</p>
<p><a href="#top">back to top</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
