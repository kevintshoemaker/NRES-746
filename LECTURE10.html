<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Kevin Shoemaker" />


<title>Machine Learning: Random Forest</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LECTURE10.html">Machine Learning</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Machine Learning: Random Forest</h1>
<h4 class="author">Kevin Shoemaker</h4>
<h4 class="date">Fall 2021</h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a href="LECTURE10.R">click here</a> for the companion R script for this lecture.</p>
<div id="machine-learning" class="section level2">
<h2>Machine learning</h2>
<p>Machine learning may seem similar to statistics, but there are some important differences.</p>
<p>In statistics, we attempt to draw inferences about a (statistical) population from a sample. In essence, we are using sample data we collected to gain <em>understanding</em> about our study system. We may also use our new understanding to make predictions and forecasts. In statistics, prediction is often secondary to building understanding of our system.</p>
<p>In machine learning, we attempt to use available data to detect generalizable predictive patterns. In machine learning we often don’t care as much about building our understanding of the system, we simply want to be able to make better predictions. Machine learning is generally not going to allow us to make inference about population parameters of interest using confidence bounds, p-values etc. However, the predictions we make using machine learning can be more accurate than the predictions we make using statistical models.</p>
<p>Machine-learning tends to impose fewer assumptions on the data than statistical models. For example, we don’t need to assume normal residuals, or even linearity!</p>
<div id="decision-trees" class="section level3">
<h3>Decision trees</h3>
<p>A decision tree is essentially a bunch of branching rules for determining an expected response from a set of predictor variables (features).</p>
<p>First, we use one of the features to divide our data such that the response variables are as similar as possible within the two resulting <em>branches</em> of the tree. For each of the resulting branches, we repeat this procedure, dividing the resulting branches again and again (recursively) until some endpoint is reaches (e.g., 3 or fewer observations remaining in the branch). Each decision point in the tree are called <em>nodes</em>. The final branches in the tree are called <em>leaves</em>.</p>
<p>Here is an example:</p>
<p><img src="Decision_Tree1.jpg" /></p>
</div>
<div id="random-forest" class="section level3">
<h3>Random forest</h3>
<p>Random forest is one of the older and still most popular machine learning methods. It is relatively easy to understand, and tends to make very good predictions. A random forest consists of a large set of decision trees!</p>
<p>The basic algorithm is as follows:</p>
<p>For each of a large number of iterations (e.g., 500) do the following:</p>
<ol style="list-style-type: decimal">
<li>Draw a small bootstrap sample from the original dataset- generally much smaller than the original dataset.<br />
</li>
<li>Using this bootstrap sample, build a <em>decision tree</em>. At each node of the decision tree, select splitting rules from a small subset of the available predictor variables (features).</li>
</ol>
<p>Once you have the full set of trees, you make predictions by using a weighted average or majority-voting rule based on all the component trees in the forest:</p>
<p><img src="Random_forest1.png" /></p>
<p>In random forest, each tree is meant to be somewhat independent from one another- in machine learning, each tree is a “weak learner”. All of the trees (the ‘committee’) will generally be a much better and more robust (generalizable) predictor than any single tree in the forest. The use of bootstrapping and random sampling of features within the random forest algorithm ensures that each tree is a weak predictor and is relatively independent from other trees in the forest. However, the collection of trees becomes a better predictor than any single tree could be!</p>
<p>Random forest is not only a good way of making predictions, it also helps us:</p>
<ol style="list-style-type: decimal">
<li><em>Identify which features are most important for prediction</em>: By keeping track of which features tend to yield the biggest improvements in prediction accuracy across all trees in the forest (generally we keep track of the ability to predict observations that were not used for fitting each tree- these are called ‘out of bag’ observations), we can produce robust indicators of variable importance.</li>
<li><em>Identify non-linear relationships</em>. By plotting out our predicted response across a range of each predictor variable, we can see if any non-linear patterns emerge!</li>
<li><em>Identify important interactions</em>. By comparing how predicted responses for one feature change across a range of another feature, we can assess the degree to which features interact to determine the expected response.</li>
</ol>
</div>
</div>
<div id="example-the-titanic-disaster" class="section level2">
<h2>Example: the titanic disaster</h2>
<p>Let’s evaluate which factors were related to surviving the titanic disaster!</p>
<p>You can load the titanic data example <a href="titanic.csv">here</a>. Alternatively you can use the ‘titanic’ package in R!</p>
<pre class="r"><code>##########
# titanic disaster example  (load data)

titanic &lt;- read.csv(&quot;titanic.csv&quot;,header=T)
head(titanic)</code></pre>
<pre><code>##   PassengerId Survived Pclass
## 1           1        0      3
## 2           2        1      1
## 3           3        1      3
## 4           4        1      1
## 5           5        0      3
## 6           6        0      3
##                                                  Name    Sex Age SibSp Parch
## 1                             Braund, Mr. Owen Harris   male  22     1     0
## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0
## 3                              Heikkinen, Miss. Laina female  26     0     0
## 4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0
## 5                            Allen, Mr. William Henry   male  35     0     0
## 6                                    Moran, Mr. James   male  NA     0     0
##             Ticket    Fare Cabin Embarked
## 1        A/5 21171  7.2500              S
## 2         PC 17599 71.2833   C85        C
## 3 STON/O2. 3101282  7.9250              S
## 4           113803 53.1000  C123        S
## 5           373450  8.0500              S
## 6           330877  8.4583              Q</code></pre>
<pre class="r"><code>library(titanic)            # alternative!</code></pre>
<pre><code>## Warning: package &#39;titanic&#39; was built under R version 4.1.1</code></pre>
<pre class="r"><code>titanic &lt;- titanic_train</code></pre>
<p>Let’s first load a package that implements a fast/efficient version of random forest</p>
<p>While we’re at it, let’s load another package for running a single decision tree</p>
<pre class="r"><code>library(ranger)    # fast implementation of random forest
library(party)     # good package for running decision tree analysis (and random forest- just slower)</code></pre>
<p>When using categorical variables, we should make sure they are encoded as factors, not as numeric. Use class(data$Resp) to check the encoding, and use as.factor(data$Resp) to encode your vector as a factor.</p>
<pre class="r"><code>titanic$Survived &lt;- as.factor(titanic$Survived)    # code response variable as a factor variable (categorical)
titanic$Sex &lt;- as.factor(titanic$Sex)</code></pre>
<p>Now let’s define the predictors and response:</p>
<pre class="r"><code>predictorNames &lt;- c(  &quot;Sex&quot;,       # nice readable names
                      &quot;Age&quot;,
                      &quot;Sibs/spouses&quot;,
                      &quot;Parents/children&quot;,
                      &quot;Fare&quot;
)

pred.names=c(  &quot;Sex&quot;,      
               &quot;Age&quot;,
               &quot;SibSp&quot;,
               &quot;Parch&quot;,
               &quot;Fare&quot;
)
# cbind(pred.names,predictorNames)

response=&quot;Survived&quot;


formula1 &lt;- as.formula(paste(response,&quot;~&quot;,paste(pred.names,collapse=&quot;+&quot;)))    # formula for the RF model</code></pre>
</div>
<div id="run-a-decision-tree" class="section level2">
<h2>Run a decision tree</h2>
<p>This is also known as a CART analysis (classification and regression tree) - a single tree, not a forest!</p>
<pre class="r"><code>TerrMamm.tr &lt;- ctree(formula=formula1, data=titanic, controls = ctree_control(mincriterion = 0.85,maxdepth = 3))

plot(TerrMamm.tr)</code></pre>
<p><img src="LECTURE10_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>But remember that a single tree is not very robust- these are very likely to over-fit to the data! Random forest gets around this issue and is much more robust than CART analysis!</p>
<p>Like most machine learning algorithms, we can “tune” the algorithm in several different ways. If this were a “real” analysis, I would try several alternative parameter tunings.</p>
<pre class="r"><code>####### Run a random forest model!

titanic2 &lt;- na.omit(titanic)   # remove missing data (ranger does not handle missing data- &#39;party&#39; implementation does...)

thismod &lt;- ranger(formula1, data=titanic2, probability=T,importance = &#39;permutation&#39; )</code></pre>
</div>
<div id="variable-importance" class="section level2">
<h2>Variable importance</h2>
<p>One thing we can easily get from a RF analysis is an index of the relative importance of each predictor variable</p>
<pre class="r"><code>    # get the importance values
varimp &lt;- importance(thismod)


lengthndx &lt;- length(varimp)
#par(mai=c(0.95,3.1,0.6,0.4))
par(mai=c(1.2,3.4,0.6,0.9))
col &lt;- rainbow(lengthndx, start = 3/6, end = 4/6)      
barplot(height=varimp[order(varimp,decreasing = FALSE)],
        horiz=T,las=1,main=&quot;Order of Importance of Predictor Variables&quot;,
        xlab=&quot;Index of overall importance&quot;,col=col,           
        names.arg=predictorNames[match(names(varimp),pred.names)][order(varimp,decreasing = FALSE)])</code></pre>
<p><img src="LECTURE10_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="univariate-predictions" class="section level2">
<h2>Univariate predictions</h2>
<p>We can also generate univariate predictive plots, also known as “partial dependence plots”. Note the use of the ‘predict’ function!</p>
<pre class="r"><code>##### Make univariate plots of the relationships- plot one relationship at a time

par(mai=c(1,1,.8,.1))
p=1
for(p in 1:length(pred.names)){
  thisvar &lt;- pred.names[p]
  
  if(is.factor(titanic2[[thisvar]])){
    nd &lt;- data.frame(x=as.factor(levels(titanic2[[thisvar]])))
  }else{
    nd &lt;- data.frame(x=seq(min(titanic2[[thisvar]]),max(titanic2[[thisvar]]),length=50))
  }
  names(nd) &lt;- thisvar
  
  othervars &lt;- setdiff(pred.names,thisvar)
  temp &lt;- sapply(othervars,function(t){ if(is.factor(titanic2[[t]])){ nd[[t]] &lt;&lt;- titanic2[[t]][1]}else{ nd[[t]] &lt;&lt;- mean(titanic2[[t]]) }} )
  #nd
  
  pred = predict(thismod,data=nd,type=&quot;response&quot;)$predictions[,2]
  
  plot(pred~nd[,1],type=&quot;l&quot;,xlab=thisvar,main=predictorNames[p])
  if(!is.factor(nd[,1])) rug(jitter(titanic2[[thisvar]]))
  
}</code></pre>
<p><img src="LECTURE10_files/figure-html/unnamed-chunk-9-1.png" width="672" /><img src="LECTURE10_files/figure-html/unnamed-chunk-9-2.png" width="672" /><img src="LECTURE10_files/figure-html/unnamed-chunk-9-3.png" width="672" /><img src="LECTURE10_files/figure-html/unnamed-chunk-9-4.png" width="672" /><img src="LECTURE10_files/figure-html/unnamed-chunk-9-5.png" width="672" /></p>
</div>
<div id="interactions" class="section level2">
<h2>Interactions</h2>
<p>Finally, we can find and plot the most important interactions!</p>
<p>In the following code block we assess the strength of all possible two-way interactions, by measuring the difference between the RF predictions across each 2-D parameter space and a fully additive model. Don’t worry if this doesn’t make sense- see me another time if you’d like me to explain the code in more detail!</p>
<pre class="r"><code>allcomb &lt;- as.data.frame(t(combn(pred.names,2)))
names(allcomb) &lt;- c(&quot;var1&quot;,&quot;var2&quot;)

allcomb$int1 &lt;- NA
allcomb$int2 &lt;- NA

p=1
for(p in 1:nrow(allcomb)){
  var1 = allcomb$var1[p]
  var2 = allcomb$var2[p]

  if(!is.factor(titanic2[[var1]])){ 
    all1= seq(min(titanic2[[var1]]),max(titanic2[[var1]]),length=10)
  }else{
    all1=as.factor(levels(titanic2[[var1]]))
  }
  if(!is.factor(titanic2[[var2]])){ 
    all2 = seq(min(titanic2[[var2]]),max(titanic2[[var2]]),length=10)
  }else{
    all2=as.factor(levels(titanic2[[var2]]))
  }

  nd &lt;- expand.grid(all1,all2)
  names(nd) &lt;- c(var1,var2)
  
  othervars &lt;- setdiff(pred.names,c(var1,var2))
  temp &lt;- sapply(othervars,function(t){ if(is.factor(titanic2[[t]])){ nd[[t]] &lt;&lt;- titanic2[[t]][1]}else{ nd[[t]] &lt;&lt;- mean(titanic2[[t]]) }}   )
  
  pred = predict(thismod,data=nd,type=&quot;response&quot;)$predictions[,2]
  
  additive_model &lt;- lm(pred~nd[[var1]]+nd[[var2]])
  
  pred_add = predict(additive_model)
  
  allcomb$int1[p] &lt;- sqrt(mean((pred-pred_add)^2))
  
  maximp &lt;- mean(varimp[c(var1,var2)])
  
  allcomb$int2[p] &lt;- allcomb$int1[p]/maximp
  
}

allcomb &lt;- allcomb[order(allcomb$int1,decreasing = T),]
allcomb</code></pre>
<pre><code>##     var1  var2       int1      int2
## 6    Age Parch 0.17363907 6.4882833
## 4    Sex  Fare 0.12447808 1.3198137
## 5    Age SibSp 0.10870998 3.4526082
## 1    Sex   Age 0.10368873 1.2532406
## 7    Age  Fare 0.09920428 1.9441792
## 3    Sex Parch 0.08853880 1.2639276
## 10 Parch  Fare 0.06685638 1.7437607
## 9  SibSp  Fare 0.05987605 1.3903732
## 2    Sex SibSp 0.04017123 0.5372287
## 8  SibSp Parch 0.03402591 1.8098532</code></pre>
<p>Next, we visualize our top interactions!</p>
<pre class="r"><code>### visualize interactions
ints.torun &lt;- 1:2
int=2
for(int in 1:length(ints.torun)){
  thisint &lt;- ints.torun[int]
  var1 = allcomb$var1[thisint]
  var2 = allcomb$var2[thisint]
 
  if(!is.factor(titanic2[[var1]])){ 
    all1= seq(min(titanic2[[var1]]),max(titanic2[[var1]]),length=10)
  }else{
    all1=as.factor(levels(titanic2[[var1]]))
  }
  if(!is.factor(titanic2[[var2]])){ 
    all2 = seq(min(titanic2[[var2]]),max(titanic2[[var2]]),length=10)
  }else{
    all2=as.factor(levels(titanic2[[var2]]))
  }
  
  nd &lt;- expand.grid(all1,all2)
  names(nd) &lt;- c(var1,var2)
  
  othervars &lt;- setdiff(pred.names,c(var1,var2))
  temp &lt;- sapply(othervars,function(t)if(is.factor(titanic2[[t]])){ nd[[t]] &lt;&lt;- titanic2[[t]][1]}else{ nd[[t]] &lt;&lt;- mean(titanic2[[t]]) }  )
  
  pred = predict(thismod,data=nd,type=&quot;response&quot;)$predictions[,2]
  
  predmat = matrix(pred,nrow=length(all1),ncol=length(all2))
  
  if(!is.factor(titanic2[[var1]])){
    persp(all1,all2,predmat,theta=25,phi=25,xlab=var1,ylab=var2,zlab=&quot;prob surv&quot;)
  }else{
    plot(predmat[1,]~all2,xlab=var2,ylab=&quot;prob surv&quot;,type=&quot;l&quot;,ylim=c(0,1),col=&quot;green&quot;,lwd=2)
    lines(all2,predmat[2,],col=&quot;blue&quot;,lwd=2)
    legend(&quot;bottomright&quot;,bty=&quot;n&quot;,lty=c(1,1),col=c(&quot;green&quot;,&quot;blue&quot;),lwd=c(2,2),legend=c(&quot;Female&quot;,&quot;Male&quot;))
  }
  
}</code></pre>
<p><img src="LECTURE10_files/figure-html/unnamed-chunk-11-1.png" width="672" /><img src="LECTURE10_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="model-performance" class="section level2">
<h2>Model performance</h2>
<p>Finally, let’s bring this home by looking at model performance!</p>
<pre class="r"><code>###################################
#################### CROSS VALIDATION CODE

n.folds = 10       # set the number of &quot;folds&quot;
foldVector = rep(c(1:n.folds),times=floor(length(titanic2$Survived)/9))[1:length(titanic2$Survived)]</code></pre>
<p>Then, we do the cross validation, looping through each fold of the data, leaving out each fold in turn for model training.</p>
<pre class="r"><code>counter = 1
CV_df &lt;- data.frame(
  CVprediction = numeric(nrow(titanic2)),      # make a data frame for storage
  realprediction = 0,
  realdata = 0
)
i=1
for(i in 1:n.folds){
  fit_ndx &lt;- which(foldVector!=i)
  validate_ndx &lt;- which(foldVector==i)
  model &lt;- ranger(formula1, data = titanic2[fit_ndx,],probability=T,importance = &#39;permutation&#39;) 
  CV_df$CVprediction[validate_ndx]  &lt;- predict(model,data=titanic2[validate_ndx,],type=&quot;response&quot;)$predictions[,2] 
  CV_df$realprediction[validate_ndx]  &lt;-  predict(thismod,data=titanic2[validate_ndx,],type=&quot;response&quot;)$predictions[,2]
  CV_df$realdata[validate_ndx] &lt;- titanic2$Survived[validate_ndx]
}

fact=TRUE
if(fact){
  CV_df$realdata=CV_df$realdata-1
}

CV_RMSE = sqrt(mean((CV_df$realdata - CV_df$CVprediction)^2))       # root mean squared error for holdout samples in 10-fold cross-validation
real_RMSE = sqrt(mean((CV_df$realdata - CV_df$realprediction)^2))  # root mean squared error for residuals from final model

# print RMSE statistics

cat(&quot;The RMSE for the model under cross-validation is: &quot;, CV_RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## The RMSE for the model under cross-validation is:  0.3705312</code></pre>
<pre class="r"><code>cat(&quot;The RMSE for the model using all data for training is: &quot;, real_RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## The RMSE for the model using all data for training is:  0.2841799</code></pre>
<p>Let’s plot out the ROC curves!</p>
<pre class="r"><code>library(ROCR)
library(rms)

par(mfrow=c(2,1))
pred &lt;- prediction(CV_df$CVprediction,CV_df$realdata)     # for holdout samples in cross-validation
perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)
auc &lt;- performance(pred,&quot;auc&quot;)
plot(perf, main=&quot;Cross-validation&quot;)
text(.9,.1,paste(&quot;AUC = &quot;,round(auc@y.values[[1]],2),sep=&quot;&quot;))

pred &lt;- prediction(CV_df$realprediction,CV_df$realdata)     # for final model
perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)
auc &lt;- performance(pred,&quot;auc&quot;)
plot(perf, main=&quot;All data&quot;)
text(.9,.1,paste(&quot;AUC = &quot;,round(auc@y.values[[1]],2),sep=&quot;&quot;))</code></pre>
<p><img src="LECTURE10_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Finally, we can use the same pseudo-R-squared metric we learned above as an alternative metric of performance</p>
<pre class="r"><code>CV_df$CVprediction[which(CV_df$CVprediction==1)] &lt;- 0.9999       # ensure that all predictions are not exactly 0 or 1
CV_df$CVprediction[which(CV_df$CVprediction==0)] &lt;- 0.0001
CV_df$realprediction[which(CV_df$realprediction==1)] &lt;- 0.9999
CV_df$realprediction[which(CV_df$realprediction==0)] &lt;- 0.0001

fit_deviance_CV &lt;- mean(-2*(dbinom(CV_df$realdata,1,CV_df$CVprediction,log=T)-dbinom(CV_df$realdata,1,CV_df$realdata,log=T)))
fit_deviance_real &lt;- mean(-2*(dbinom(CV_df$realdata,1,CV_df$realprediction,log=T)-dbinom(CV_df$realdata,1,CV_df$realdata,log=T)))
null_deviance &lt;- mean(-2*(dbinom(CV_df$realdata,1,mean(CV_df$realdata),log=T)-dbinom(CV_df$realdata,1,CV_df$realdata,log=T)))
deviance_explained_CV &lt;- (null_deviance-fit_deviance_CV)/null_deviance   # based on holdout samples
deviance_explained_real &lt;- (null_deviance-fit_deviance_real)/null_deviance   # based on full model...

# print RMSE statistics

cat(&quot;The McFadden R2 for the model under cross-validation is: &quot;, deviance_explained_CV, &quot;\n&quot;)</code></pre>
<pre><code>## The McFadden R2 for the model under cross-validation is:  0.3530064</code></pre>
<pre class="r"><code>cat(&quot;The McFadden R2 for the model using all data for training is: &quot;, deviance_explained_real, &quot;\n&quot;)</code></pre>
<pre><code>## The McFadden R2 for the model using all data for training is:  0.5827352</code></pre>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
