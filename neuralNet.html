<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Keane Flynn" />


<title>Neural Networks and their Applications</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
    <li>
      <a href="GIT-tutorial.html">Demo: version control in Git</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LECTURE10.html">Machine Learning</a>
    </li>
    <li>
      <a href="Occupancy.html">Occupancy Models</a>
    </li>
    <li>
      <a href="TimeSeries_all.html">Time Series</a>
    </li>
    <li>
      <a href="SCR.html">Spatial Capture-Recapture</a>
    </li>
    <li>
      <a href="sppm.html">Spatial Point Process Models</a>
    </li>
    <li>
      <a href="SEM.RMarkdown.html">Structural Equation Models</a>
    </li>
    <li>
      <a href="spatial_autocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="MEM2.html">Mixed Effects Models</a>
    </li>
    <li>
      <a href="MANOVA.html">MANOVA</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Neural Networks and their Applications</h1>
<h4 class="author">Keane Flynn</h4>
<h4 class="date">12/1/2021</h4>

</div>


<p>If you’d like to follow along in R, you can download the script here: <a href="neuralNet.R">neuralNet.R</a></p>
<p>All of the materials for this lecture/demo are also available on <a href="https://github.com/keaneflynn/NeuralNetworks">GitHub</a></p>
<div id="neural-network-introduction" class="section level1">
<h1>Neural Network Introduction</h1>
<p>“A neural network is the second best way to solve any problem. The best way is to actually understand the problem.” - <em>Unknown</em></p>
<div class="figure">
<img src="media/neuralNet.gif" style="width:80.0%" alt="" />
<p class="caption"><em>The above example shows a very simplistic convolutional neural network used to detect numbers from an image.<br />
Sidenote: you can create CNNs in r, however they are not fun to make in r.</em></p>
</div>
<div id="what-are-they" class="section level3">
<h3>What are they?</h3>
<p>A neural network is a parallel, distributed information processing structure consisting of processing elements (nodes) interconnected together with unidirectional signal channels. Each node has a single output connection which branches into as many collateral connections as desired. The node’s output signal can be of any mathematical value (can later be converted to non-numerical values). All of the processing that goes on within each node must depend only upon the values of the received input signals arriving at the node via incoming connections and upon values stored in the node’s local memory (Hecht-Nielsen 1992).</p>
</div>
<div id="theory-behind-them" class="section level3">
<h3>Theory behind them</h3>
<p>Neural networks are a subset of machine learning that are an amalgamation of modern computer science and cognitive psychology. The data processing and analysis is based on the same process that your brains’ neurons undergo to process information. Your neurons require a certain amount of activation energy in order to fire and send information beyond the neuron and along to the axon, synapses, to other neurons and so on, an input requirement known as action potential.</p>
</div>
<div id="history-of-neural-networks" class="section level3">
<h3>History of neural networks</h3>
<ul>
<li>1943: A couple of old-timey neurophysiologists named McCulloch &amp; Pitts developed the first neural network with electrical circuitry to try to recreate the connections and processing of information within the brain</li>
<li>1949: Donald Webb proposed that neural pathways become stronger with more frequent use</li>
<li>1958: Frank Rosenblatt publishes a paper with the <strong>Perceptron</strong> concept as an answer for fly’s fight or flight response to stimuli (this was a huge breakthrough)</li>
</ul>
<div class="figure">
<img src="media/perceptron.jpeg" style="width:40.0%" alt="" />
<p class="caption"><em>Basic principle of Perceptron model.</em></p>
</div>
<ul>
<li>1959: First application of neural networks from Stanford lab used to filter out noise in phone lines (still used to this day)</li>
<li>1969: Start of the AI “dark ages” where the Perceptron idea was killed by MIT (wrongly) followed by the cold war which made people overly-terrified of technology</li>
<li>1986: People got over this intellectual blockade and resumed research in AI and more specifically neural networks. This was catalyzed by the re-discovery of an older theory called <strong>Backpropogation</strong> which made neural networks much more applicable for larger, more diverse datasets.</li>
<li>2006: The development of the deep neural network (DNN), most common models created to date.</li>
</ul>
</div>
<div id="what-led-to-the-development-and-use-of-them" class="section level3">
<h3>What led to the development and use of them?</h3>
<p>Originally, scientists were simply interested if they could recreate how the human brain works; they really didn’t have any desire to make this concept much more than a concept. After WWII and Alan Turing’s creation of what would become the modern day computer, scientists that were able to get their hands on this technology could turn it loose on whatever they could imagine including early neural networks. For the next few decades, the theory of artificial intelligence and neural networks developed faster than technology could to support it until modern computer processors and parallel computing caught up with the all of the theory (Moore’s law). Now the opposite seems to be true and computing power is allowing for a lot more applications of neural networks and the sky is the limit for real-world applications.</p>
<p><img src="media/pastry.png" style="width:60.0%" /></p>
</div>
</div>
<div id="components-and-applications" class="section level1">
<h1>Components and applications</h1>
<p><img src="media/meme.png" style="width:60.0%" /></p>
<p>Neural networks are currently the fastest growing research topic (Google scholar) as well as one of the most well-funded research areas. Most of <em>Nature’s</em> most cited papers are based on neural network research (Lek et al. 2016). They are capable of filling in analytical gaps where other statistical methods simply fall short. While they process information the same way our brains do, they are capable of finding patters that we simply are not capable of.</p>
<div id="main-types-of-neural-networks" class="section level3">
<h3>Main types of neural networks</h3>
<ol style="list-style-type: decimal">
<li>Artificial neural network (ANN)</li>
<li>Convolutional neural network (CNN)</li>
<li>Recurrent neural network (RNN)</li>
</ol>
</div>
<div id="components-and-function-of-neural-networks" class="section level3">
<h3>Components and function of neural networks</h3>
<p>Neural networks vary A LOT in structure, but most of them have relatively similar components and building blocks.</p>
<div class="figure">
<img src="media/neuralNetDiagram.png" style="width:70.0%" alt="" />
<p class="caption"><em>Structure of a simple deep learning model</em></p>
</div>
<div id="nodes" class="section level4">
<h4>Nodes</h4>
<p>A node is simply a container for a value with one or more weighted input connections, with the exception of the input layer which has yet to be passed beyond the first input layer.</p>
</div>
<div id="input-layer" class="section level4">
<h4>Input layer</h4>
<p>The input layer consist of whatever data you are choosing to help predict your desired result. These come in many varieties and number of input layers, but each input node always represents a single floating point value. For the network we will be generating in R, it requires all of our values be between 0 and 1 (requires scaling)</p>
</div>
<div id="hidden-layers" class="section level4">
<h4>Hidden layer(s)</h4>
<p>Hidden layer(s) consist of a series of nodes that are used to take in a weighted sum from a combination of nodes from the previous layer and pass those values along to the following layer. This is the black box area of neural networks; they can be very extensive and have many different kinds of layers performing different computational tasks. This is also where the term deep learning comes from, any ANN with multiple hidden layers is considered a deep learning model.</p>
</div>
<div id="output-layer" class="section level4">
<h4>Output layer</h4>
<p>This is our result. For classification-type models (i.e. what do object does the given input describe?), it will generate a value ranging from 0-1. In an ideal world, it would either be 0 or 1 each time, but it usually varies and whichever value it is closest to is the generated prediction. For numerical outputs, we have to rescale to get usable output variables.</p>
</div>
<div id="weights" class="section level4">
<h4>Weights</h4>
<p>A value that will change an input value to a node from the previous layer depending upon its significance. Weights are multiplied by the input value at your node.</p>
</div>
<div id="bias-value" class="section level4">
<h4>Bias value</h4>
<p>Bias value is added to the above multiplication of the weight and input at the node. You can think of this bias value as the equivalent of a y-intercept in a linear model.</p>
</div>
</div>
</div>
<div id="assumptions-and-steps-for-building-one" class="section level1">
<h1>Assumptions and steps for building one</h1>
<div id="assumptions" class="section level3">
<h3>Assumptions</h3>
<ul>
<li>There really are none. You can pass any data into a neural network, it just needs to be structured first depending upon the model syntax; in our case all values need to be between 0-1.</li>
<li>They do tend to perform better when your desired output layer of your training dataset follows a uniform distribution.</li>
<li>Relationships are not always linear (they seldom are), the more input data and input nodes you throw into a model the less often it is</li>
</ul>
</div>
<div id="data-structure" class="section level3">
<h3>Data structure</h3>
<ul>
<li>Every model framework is slightly different (no way we are making our own)</li>
<li>One consistent rule is that the data needs to be numeric
<ul>
<li>Easy to work around</li>
</ul></li>
<li>Understand your neural network application and create a training dataset with proper resolution</li>
<li>To properly train and test your model, it is recommended to take your data source and break it up 80/20 for training and testing data</li>
</ul>
</div>
<div id="training-the-model" class="section level3">
<h3>Training the model</h3>
<ul>
<li>Here we use a technique called <strong>Backpropogation</strong> (or something similar) to develop our model weights and bias values.</li>
<li>Backpropogation works with something commonly referred to as a cost or loss function in combination with another concept called <strong>gradient descent</strong>
<ul>
<li>Similar to how MCMC works to make these random guesses or jumps for posterior values</li>
</ul></li>
<li>Your model will start at your output layer with the ground-truthed training data you have provided and work backwards toward the input layer developing the weights and biases along the way</li>
<li>Each “step” in your backpropogation, the algorithm will alter the weights and biases to reflect an individual data point (i.e. increase weights for a certain true value and decrease all others)</li>
<li>I could spend hours talking about this so I’ll provide a couple links to YouTube videos on this process that are very helpful</li>
</ul>
</div>
<div id="forward-pass-or-inference" class="section level3">
<h3>Forward pass or inference</h3>
<p>Once we have successfully trained a properly fitted model, we then have to implement it. In many data science cases this is relatively straightforward, however if being implemented in real-time this can be a very difficult and critical step to retrieve useful data. This process is usually referred to as your forward pass or <strong>inference</strong> step. You may see models referred to as “forward feeding” and this simply means that data is moving from your input to your output layer.</p>
</div>
</div>
<div id="issues-with-neural-networks" class="section level1">
<h1>Issues with neural networks</h1>
<p>While neural networks are incredibly powerful and useful tools, they come with their caveats.</p>
<div id="overfitting" class="section level3">
<h3>Overfitting</h3>
<p>This is one area of similarity that neural networks share with other models. Overfitting is a very common issue encountered while training a neural network, however it is rather easily avoidable if properly monitored and input parameters are adjusted accordingly. - Don’t train model for excessive amount of steps/epochs - Play around with your learning rate: simply models don’t require a slow learning rate - If all else fails, try different backpropogation methods</p>
<div class="figure">
<img src="media/overfittingDiagram.png" alt="" />
<p class="caption"><em>Useful diagram depicting overfitting of various models</em></p>
</div>
</div>
<div id="a-lot-of-unknowns" class="section level3">
<h3>A lot of unknowns</h3>
<ul>
<li>With very simply models, it can be relatively easy to decipher what the model is thinking.</li>
<li>Once you work up to larger models all hope is lost of trying to figure out what the computer is learning.</li>
<li>Currently not a very accepted method of analysis in a lot of fields</li>
</ul>
</div>
<div id="collecting-sufficient-training-data-can-be-arduous" class="section level3">
<h3>Collecting sufficient training data can be arduous</h3>
<ul>
<li>Depending upon what datasets you work with, collecting and formatting training data can be very tedious (i.e. image annotation for CNNs)</li>
<li>Sometimes you are simply given bad data and this is very problematic</li>
</ul>
</div>
</div>
<div id="lets-do-some-coding" class="section level1">
<h1>Let’s do some coding</h1>
<p>Let’s start with a simple example from the iris dataset (sorry)</p>
<div id="load-packages" class="section level3">
<h3>Load packages</h3>
<pre class="r"><code>library(dplyr)
library(neuralnet)
library(stringr)
library(LaplacesDemon)</code></pre>
</div>
<div id="load-and-visualize-data" class="section level3">
<h3>Load and visualize data</h3>
<pre class="r"><code>head(iris)</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa</code></pre>
<pre class="r"><code>summary(iris$Species) #this is our output layer, or what we are predicting</code></pre>
<pre><code>##     setosa versicolor  virginica 
##         50         50         50</code></pre>
<p>Here we see that our output layer represents a perfectly uniform distribution so this is as idea as we can get as far as training is concerned.</p>
</div>
<div id="format-the-data" class="section level3">
<h3>Format the data</h3>
<pre class="r"><code>cleanedIris &lt;- model.matrix(~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + Species,data=iris)
cleanedIris &lt;- cleanedIris/max(cleanedIris)
head(cleanedIris)</code></pre>
<pre><code>##   (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1   0.1265823    0.6455696   0.4430380    0.1772152  0.02531646
## 2   0.1265823    0.6202532   0.3797468    0.1772152  0.02531646
## 3   0.1265823    0.5949367   0.4050633    0.1645570  0.02531646
## 4   0.1265823    0.5822785   0.3924051    0.1898734  0.02531646
## 5   0.1265823    0.6329114   0.4556962    0.1772152  0.02531646
## 6   0.1265823    0.6835443   0.4936709    0.2151899  0.05063291
##   Speciesversicolor Speciesvirginica
## 1                 0                0
## 2                 0                0
## 3                 0                0
## 4                 0                0
## 5                 0                0
## 6                 0                0</code></pre>
<p>This dataframe snippet shows the format necessary for the neuralnet() function in R. To properly process the data, all of the values need to be between 0 and 1.</p>
</div>
<div id="visualize-the-neuralnet" class="section level3">
<h3>Visualize the neuralnet</h3>
<pre class="r"><code>irisNN &lt;- neuralnet(Speciesversicolor+Speciesvirginica~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
                    cleanedIris, hidden=1,algorithm=&quot;rprop+&quot;,
                    learningrate=0.01, linear.output=F)
plot(irisNN)</code></pre>
<p><img src="media/irisNet.png" /></p>
<p>While the iris dataset is painful to look at, it is very useful for generating a strong correlation and creating a simple, interpretable model. For now, lets not worry about the forward pass portion. So let’s walk through this a bit.</p>
<div class="figure">
<img src="media/perceptronModel.png" style="width:60.0%" alt="" />
<p class="caption"><em>Formula for the basic perceptron model</em></p>
</div>
</div>
<div id="lets-play-around-with-some-different-data" class="section level2">
<h2>Let’s play around with some different data</h2>
<p><img src="media/Nevada-Humane-Society-Logo.png" /></p>
<p>Thanks to some friends that work over at the Nevada Humane Society, I was able to get my hands on their largescale dataset (n&gt;10000) for animal adoptions. This provided a good opportunity to create a neural network to predict residency times for incoming animals to the shelter, useful for understanding resource allocation for shelter pets. So lets look at some data.</p>
<p><a href="nhseData.csv">Click here to download the data for this example</a></p>
<div id="importing-data" class="section level3">
<h3>Importing data</h3>
<pre class="r"><code>df &lt;- read.csv(&#39;nhseData.csv&#39;)
df &lt;- df[!duplicated(df[,c(&quot;Name&quot;, &quot;Species&quot;)]),] 
head(df)</code></pre>
<pre><code>##    Created.Date    Name Species              Primary.Breed    Sex Age..Months.
## 1    03/13/2020    Luna     Dog        Pinscher, Miniature Female          146
## 4    01/30/2020   BUDDY     Dog Terrier, Yorkshire, Yorkie   Male          122
## 5    03/05/2020  BUDDAH     Cat          Domestic Longhair Female           74
## 7    02/05/2020   SASSY     Dog                 Rottweiler Female          106
## 8    01/27/2020 Biscuit     Dog      Terrier, Jack Russell Female          188
## 10   02/07/2020 Frankie     Dog                     Beagle   Male          183
##                 Age.Group Primary.Color Days.in.Custody Days.on.Site
## 1        Senior (8+years)         Black               1            1
## 4        Senior (8+years)         Brown               1            1
## 5  Adult Cat (5-10 years)         White               3            3
## 7        Senior (8+years)         Black               1            1
## 8        Senior (8+years)         White               1            1
## 10       Senior (8+years)         Brown               1            1
##    Current.Weight Attributes
## 1         9.4 lbs  Angel Pet
## 4                           
## 5       11.38 lbs           
## 7                           
## 8        25.4 lbs           
## 10         31 lbs</code></pre>
</div>
<div id="specific-data-point" class="section level3">
<h3>Specific data point</h3>
<pre class="r"><code>df %&gt;% filter(Name==&quot;Donut&quot;) #This is my pup</code></pre>
<pre><code>##   Created.Date  Name Species              Primary.Breed  Sex Age..Months.
## 1   07/15/2021 Donut     Dog Terrier, American Pit Bull Male            7
##                      Age.Group Primary.Color Days.in.Custody Days.on.Site
## 1 Adult Dog (5 months-8 years)         Black             108            8
##   Current.Weight
## 1       56.5 lbs
##                                                                                      Attributes
## 1 D2D Required,Heart Murmur,Kids under 6 meet first,Unavailable - waiting for medical procedure</code></pre>
</div>
<div id="regrouping-data" class="section level3">
<h3>Regrouping data</h3>
<pre class="r"><code>df$Age.Group &lt;- str_remove_all(df$Age.Group,&quot; \\(.*?\\)&quot;)
df$Age.Group &lt;- str_replace_all(df$Age.Group,&quot;Juvenile|Kitten|Puppy|Unweaned&quot;,&quot;juvenile&quot;)
df$Age.Group &lt;- str_replace_all(df$Age.Group,&quot;Adult Cat|Adult Dog|Adult|Young adult&quot;,&quot;adult&quot;)
df$Age.Group &lt;- str_replace_all(df$Age.Group,&quot;Senior&quot;,&quot;senior&quot;)

df$Current.Weight &lt;- as.numeric(str_extract_all(df$Current.Weight,&quot;\\d{0,3}.\\d{1,2}&quot;))

df$Sex &lt;- str_replace_all(df$Sex,&quot;Male&quot;,&quot;male&quot;)
df$Sex &lt;- str_replace_all(df$Sex,&quot;Female&quot;,&quot;female&quot;)
df$Sex &lt;- str_replace_all(df$Sex,&quot;Unknown&quot;,&quot;unknown&quot;)

df$Species &lt;- str_replace_all(df$Species,&quot;Bird, Unspecified|Chicken, Domestic|Conure, Unspecified|Parakeet, Common|Parakeet, Unspecified&quot;,&quot;bird&quot;)
df$Species &lt;- str_replace_all(df$Species,&quot;Lizard, Unspecified|Snake, Python Unspecified|Tortoise, Unspecified|Turtle, Red-Eared Slider|Turtle, Unspecified&quot;,&quot;reptile&quot;)
df$Species &lt;- str_replace_all(df$Species,&quot;Chinchilla|Ferret|Guinea Pig|Hamster, Dwarf|Hamster, Unspecified|Hedgehog|Mouse, Little Pocket|Mouse, Unspecified|Rabbit, Domestic|Rat, Unspecified|Sugar Glider&quot;,&quot;small_mammal&quot;)
df$Species &lt;- str_replace_all(df$Species,&quot;Dog&quot;,&quot;dog&quot;)
df$Species &lt;- str_replace_all(df$Species,&quot;Cat&quot;,&quot;cat&quot;)</code></pre>
</div>
<div id="final-dataframe-cleansing" class="section level3">
<h3>Final dataframe cleansing</h3>
<pre class="r"><code>df &lt;- dplyr::rename(df, c(species = Species,
                          breed = Primary.Breed,
                          sex = Sex,
                          age = Age..Months.,
                          age_group = Age.Group,
                          weight_lbs = Current.Weight,
                          custody_period = Days.in.Custody)) %&gt;%
  select(species,sex,age_group,age,weight_lbs,custody_period) %&gt;% 
  na.omit()
rownames(df) &lt;- 1:nrow(df)
df_check &lt;- df[-c(2860,755,5856,4709,5189),] #removing some outliers that are annoying me
head(df_check)</code></pre>
<pre><code>##   species    sex age_group age weight_lbs custody_period
## 1     dog female    senior 146       9.40              1
## 2     cat female     adult  74      11.38              3
## 3     dog female    senior 188      25.40              1
## 4     dog   male    senior 183      31.00              1
## 5     cat female     adult  99       8.40              1
## 6     dog female     adult  75      70.80             28</code></pre>
<p>Here we have a cleaner, more interpretable dataframe, where we can see the basis of our input layer and our output layer. Depending upon the dataset, adding more input layers can improve model accuracy but the hidden layers tend to have more relevance in model performance.</p>
</div>
<div id="formatting-the-dataframe-for-the-neuralnet-package" class="section level3">
<h3>Formatting the dataframe for the neuralnet() package</h3>
<pre class="r"><code>formatted_df &lt;- model.matrix(~ species + sex + age_group + age + weight_lbs + custody_period,data=df_check)
maxVal &lt;- max(formatted_df) #variable stored to unscale our dataframe, important for later
formatted_df &lt;- formatted_df/maxVal
head(formatted_df)</code></pre>
<pre><code>##   (Intercept)  speciesdog speciesreptile speciessmall_mammal     sexmale
## 1 0.001636661 0.001636661              0                   0 0.000000000
## 2 0.001636661 0.000000000              0                   0 0.000000000
## 3 0.001636661 0.001636661              0                   0 0.000000000
## 4 0.001636661 0.001636661              0                   0 0.001636661
## 5 0.001636661 0.000000000              0                   0 0.000000000
## 6 0.001636661 0.001636661              0                   0 0.000000000
##   sexunknown age_groupjuvenile age_groupsenior       age weight_lbs
## 1          0                 0     0.001636661 0.2389525 0.01538462
## 2          0                 0     0.000000000 0.1211129 0.01862520
## 3          0                 0     0.001636661 0.3076923 0.04157119
## 4          0                 0     0.001636661 0.2995090 0.05073650
## 5          0                 0     0.000000000 0.1620295 0.01374795
## 6          0                 0     0.000000000 0.1227496 0.11587561
##   custody_period
## 1    0.001636661
## 2    0.004909984
## 3    0.001636661
## 4    0.001636661
## 5    0.001636661
## 6    0.045826514</code></pre>
</div>
<div id="breaking-up-into-training-and-testing-data" class="section level3">
<h3>Breaking up into training and testing data</h3>
<p>Recall that we need 80% of our data for training our model and 20% for testing, so lets go ahead and break it up accordingly and randomly to avoid accidentally fitting our model to specific attributes.</p>
<pre class="r"><code>set.seed(70) #sample pseudo-randomly for replication sake
sampleSize &lt;- round(nrow(formatted_df)*0.8) #split up dataset 80/20
rowIndex &lt;- sample(seq(nrow(formatted_df)),size=sampleSize)

training_data &lt;- formatted_df[rowIndex,] #what will be passed into the model training function
testing_data &lt;-  formatted_df[-rowIndex,]
groundtruth_data &lt;- testing_data[,11]*maxVal</code></pre>
</div>
<div id="train-and-visualize-our-new-network-for-pet-adoptions" class="section level3">
<h3>Train and visualize our new network for pet adoptions</h3>
<pre class="r"><code>nn &lt;- neuralnet(custody_period~speciesdog+speciesreptile+speciessmall_mammal+sexmale+sexunknown+age_groupjuvenile+age_groupsenior+age+weight_lbs,
                training_data,
                hidden=c(5,2), learningrate=0.01,  
                linear.output=T)
nn$result.matrix</code></pre>
<pre><code>##                                          [,1]
## error                             7.405961625
## reached.threshold                 0.009361145
## steps                           788.000000000
## Intercept.to.1layhid1            -0.669454459
## speciesdog.to.1layhid1           48.286292198
## speciesreptile.to.1layhid1       77.818053946
## speciessmall_mammal.to.1layhid1  77.082353239
## sexmale.to.1layhid1             -13.343925495
## sexunknown.to.1layhid1           77.342705951
## age_groupjuvenile.to.1layhid1    49.993137322
## age_groupsenior.to.1layhid1     -32.138882110
## age.to.1layhid1                  -0.502998393
## weight_lbs.to.1layhid1           10.725454157
## Intercept.to.1layhid2            -0.940134230
## speciesdog.to.1layhid2           43.253697994
## speciesreptile.to.1layhid2      -78.266402185
## speciessmall_mammal.to.1layhid2 -78.241516751
## sexmale.to.1layhid2              -2.911871820
## sexunknown.to.1layhid2          -78.275226887
## age_groupjuvenile.to.1layhid2   -45.115216766
## age_groupsenior.to.1layhid2      70.709006746
## age.to.1layhid2                  -4.319523604
## weight_lbs.to.1layhid2          -20.925963243
## Intercept.to.1layhid3             0.225179631
## speciesdog.to.1layhid3          -41.433207557
## speciesreptile.to.1layhid3       74.390181721
## speciessmall_mammal.to.1layhid3  76.193061968
## sexmale.to.1layhid3              67.297149620
## sexunknown.to.1layhid3           77.244538993
## age_groupjuvenile.to.1layhid3    51.166889065
## age_groupsenior.to.1layhid3     -45.499212530
## age.to.1layhid3                  -0.357394975
## weight_lbs.to.1layhid3           15.193308277
## Intercept.to.1layhid4             0.254675689
## speciesdog.to.1layhid4           48.309648565
## speciesreptile.to.1layhid4      -76.188546178
## speciessmall_mammal.to.1layhid4 -78.904338577
## sexmale.to.1layhid4             -29.761526120
## sexunknown.to.1layhid4          -78.316649093
## age_groupjuvenile.to.1layhid4   -48.347582963
## age_groupsenior.to.1layhid4     -36.571978203
## age.to.1layhid4                 -18.571791871
## weight_lbs.to.1layhid4          -45.109170342
## Intercept.to.1layhid5            -0.027144794
## speciesdog.to.1layhid5           76.049750218
## speciesreptile.to.1layhid5       75.400192866
## speciessmall_mammal.to.1layhid5  59.580922621
## sexmale.to.1layhid5             -29.341353767
## sexunknown.to.1layhid5           77.066608078
## age_groupjuvenile.to.1layhid5    54.864635323
## age_groupsenior.to.1layhid5      26.074362613
## age.to.1layhid5                  -3.945598044
## weight_lbs.to.1layhid5           -4.216351590
## Intercept.to.2layhid1            -1.935453082
## 1layhid1.to.2layhid1             -1.527753827
## 1layhid2.to.2layhid1             -1.967246059
## 1layhid3.to.2layhid1              1.062721854
## 1layhid4.to.2layhid1             -7.652037630
## 1layhid5.to.2layhid1             -4.095752889
## Intercept.to.2layhid2            -2.093905032
## 1layhid1.to.2layhid2             -2.953768412
## 1layhid2.to.2layhid2              6.414406312
## 1layhid3.to.2layhid2             -1.252861574
## 1layhid4.to.2layhid2              1.452407020
## 1layhid5.to.2layhid2             -0.700316380
## Intercept.to.custody_period      -0.017285357
## 2layhid1.to.custody_period        1.299287416
## 2layhid2.to.custody_period        0.568964559</code></pre>
<pre class="r"><code>plot(nn)</code></pre>
<p><img src="media/NHSEnet1.png" /></p>
<p>Here we get a few interesting metrics from the network training. The primary one we are interested in is the error value, we ideally want this to be 0 however that won’t happen. But we can take steps to minimize it.</p>
</div>
<div id="lets-test-our-model-against-our-training-dataset" class="section level3">
<h3>Let’s test our model against our training dataset</h3>
<pre class="r"><code>computedNN &lt;- compute(nn,testing_data)$net.result #this is the forward pass or inference phase of the network
head(computedNN)</code></pre>
<pre><code>##          [,1]
## 3  0.06147906
## 4  0.06536534
## 5  0.03580975
## 13 0.02267256
## 22 0.02414354
## 25 0.04306530</code></pre>
<pre class="r"><code>predicted_values &lt;- computedNN * maxVal #Here we are taking the output from our neuralnet and unscaling it to make the outputs usable
head(predicted_values)</code></pre>
<pre><code>##        [,1]
## 3  37.56370
## 4  39.93822
## 5  21.87976
## 13 13.85293
## 22 14.75170
## 25 26.31290</code></pre>
<pre class="r"><code>plot(groundtruth_data, predicted_values, col=&#39;red&#39;, pch=1, 
     xlim = c(0,200), ylim = c(0,200), cex=0.75, 
     ylab = &quot;predicted days in shelter&quot;, xlab = &quot;actual days in shelter&quot;,
     main = &quot;NHSE Custody Period Prediction&quot;)
abline(a=0,b=1) #This b=1 line shows what the data would look like in an idea scenario</code></pre>
<p><img src="neuralNet_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>sum((groundtruth_data - predicted_values)^2)/length(groundtruth_data) #Sum of squared errors normalized by length of the dataset</code></pre>
<pre><code>## [1] 816.1444</code></pre>
<p>So this doesn’t look terrible, but what looks wrong here? (hint: think back to one of the few “assumptions” of neural network training)</p>
</div>
<div id="testing-training-input-data" class="section level3">
<h3>Testing training input data</h3>
<p>Hopefully someone answered my question so I’m not quietly standing up here wishing I had done this with a group.</p>
<pre class="r"><code>hist(df$custody_period,breaks=50)</code></pre>
<p><img src="neuralNet_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>This is incredibly not uniform. So lets try to fix that a little bit. Let’s pick up at a checkpoint dataframe</p>
<pre class="r"><code>df_check &lt;- df_check %&gt;% filter(custody_period &lt;= 40)
hist(df_check$custody_period,breaks=50)</code></pre>
<p><img src="neuralNet_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Not perfect, but nothing in life ever is, so good enough.</p>
</div>
<div id="lets-cycle-through-the-model-training-process-one-more-time" class="section level3">
<h3>Let’s cycle through the model training process one more time</h3>
<pre class="r"><code>formatted_df &lt;- model.matrix(~ species + sex + age_group + age + weight_lbs + custody_period,data=df_check)
maxVal &lt;- max(formatted_df) #variable stored to unscale our dataframe, important for later
formatted_df &lt;- formatted_df/maxVal
head(formatted_df)</code></pre>
<pre><code>##   (Intercept)  speciesdog speciesreptile speciessmall_mammal     sexmale
## 1 0.003831418 0.003831418              0                   0 0.000000000
## 2 0.003831418 0.000000000              0                   0 0.000000000
## 3 0.003831418 0.003831418              0                   0 0.000000000
## 4 0.003831418 0.003831418              0                   0 0.003831418
## 5 0.003831418 0.000000000              0                   0 0.000000000
## 6 0.003831418 0.003831418              0                   0 0.000000000
##   sexunknown age_groupjuvenile age_groupsenior       age weight_lbs
## 1          0                 0     0.003831418 0.5593870 0.03601533
## 2          0                 0     0.000000000 0.2835249 0.04360153
## 3          0                 0     0.003831418 0.7203065 0.09731801
## 4          0                 0     0.003831418 0.7011494 0.11877395
## 5          0                 0     0.000000000 0.3793103 0.03218391
## 6          0                 0     0.000000000 0.2873563 0.27126437
##   custody_period
## 1    0.003831418
## 2    0.011494253
## 3    0.003831418
## 4    0.003831418
## 5    0.003831418
## 6    0.107279693</code></pre>
<pre class="r"><code>set.seed(70) #sample pseudo-randomly for replication sake
sampleSize &lt;- round(nrow(formatted_df)*0.8) #split up dataset 80/20
rowIndex &lt;- sample(seq(nrow(formatted_df)),size=sampleSize)

training_data &lt;- formatted_df[rowIndex,] #what will be passed into the model training function
testing_data &lt;-  formatted_df[-rowIndex,]
groundtruth_data &lt;- testing_data[,11]*maxVal</code></pre>
<pre class="r"><code>nn &lt;- neuralnet(custody_period~speciesdog+speciesreptile+speciessmall_mammal+sexmale+sexunknown+age_groupjuvenile+age_groupsenior+age+weight_lbs,
                training_data,
                hidden=c(5,2), learningrate=0.01,  
                linear.output=T)
#nn$result.matrix #new neural network weights and biases
plot(nn)</code></pre>
<p><img src="media/NHSEnet2.png" /></p>
<pre class="r"><code>computedNN &lt;- compute(nn,testing_data)$net.result #this is the forward pass or inference phase of the network
predicted_values &lt;- computedNN * maxVal #Here we are taking the output from our neuralnet and unscaling it to make the outputs usable
head(predicted_values)</code></pre>
<pre><code>##         [,1]
## 3   7.750194
## 4   8.379116
## 7   7.182494
## 13  8.847443
## 20 13.411501
## 22  9.489674</code></pre>
<p>Now that is a healthy improvement on error, so lets give it a run.</p>
<pre class="r"><code>plot(groundtruth_data, predicted_values, col=&#39;red&#39;, pch=1, 
     xlim = c(0,45), ylim = c(0,45), cex=0.75, 
     ylab = &quot;predicted days in shelter&quot;, xlab = &quot;actual days in shelter&quot;,
     main = &quot;NHSE Custody Period Prediction&quot;)
abline(a=0,b=1) #This b=1 line shows what the data would look like in an idea scenario</code></pre>
<p><img src="neuralNet_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>sum((groundtruth_data - predicted_values)^2)/length(groundtruth_data) #Sum of squared errors normalized by length of the dataset</code></pre>
<pre><code>## [1] 96.8648</code></pre>
<p>This numerical value shows just about an order of magnitude improvement from our last model fitting the data. The visuals of this graph are much better, but we can still see there is a cap on prediction value. We can still see that there is a much higher density of data on the lower end of the days in shelter values.</p>
<p>So what can we do to resolve this?</p>
<p>Instead of chopping data on the tail end like we did, we can thin the data across the range of the training dataset to better achieve this uniform distribution.</p>
<p>This is also a very simple model, we could have included many more variables as inputs to improve model accuracy. For example, my dog would be predicted to be out of the shelter relatively quickly based on the model criteria, however the model didn’t take into account he has a heart condition.</p>
<p>Additionally, we could have included hidden layers with more dimensionality to try and find more subtle patterns in our data, but I don’t think this would work that well without adding in additional input values (could potentially lead to overfitting).</p>
</div>
<div id="lets-create-a-mock-dataset-for-predicting-some-theoretical-incoming-pets" class="section level3">
<h3>Let’s create a mock dataset for predicting some theoretical incoming pets</h3>
<p>Lets make a mockup of three animals: a 3 month old, male puppy roughly german shepherd sized, a 10 month old chinchilla with unknown sex, and a 1 year old female cat that weighs 8 lbs. Let’s see what it says!</p>
<pre class="r"><code>speciesdog = c(1,0,0) 
speciesreptile = c(0,0,0)
speciessmall_mammal = c(0,1,0)
sexmale = c(1,0,0)
sexunknown = c(0,1,0)
age_groupjuvenile = c(1,0,1)
age_groupsenior = c(0,1,0)
age = c(3,10,12)
weight_lbs = c(15,1,8)
testPred &lt;- data.frame(speciesdog,speciesreptile,speciessmall_mammal,
                       sexmale,sexunknown,age_groupjuvenile,age_groupsenior,
                       age,weight_lbs)
testPred &lt;- testPred/maxVal
head(testPred) </code></pre>
<pre><code>##    speciesdog speciesreptile speciessmall_mammal     sexmale  sexunknown
## 1 0.003831418              0         0.000000000 0.003831418 0.000000000
## 2 0.000000000              0         0.003831418 0.000000000 0.003831418
## 3 0.000000000              0         0.000000000 0.000000000 0.000000000
##   age_groupjuvenile age_groupsenior        age  weight_lbs
## 1       0.003831418     0.000000000 0.01149425 0.057471264
## 2       0.000000000     0.003831418 0.03831418 0.003831418
## 3       0.003831418     0.000000000 0.04597701 0.030651341</code></pre>
<pre class="r"><code>testRunNN &lt;- round(compute(nn, testPred)$net.result * maxVal) #run the neural net prediction and rescaling it for usable results
testRunNN</code></pre>
<pre><code>##      [,1]
## [1,]    2
## [2,]   12
## [3,]    7</code></pre>
<p>Here we see the prediction: the puppy would be in the shelter for 2 days, the chinchilla for 11, and the cat for 7 days. Based on casually looking through some of the training data this seems like a reasonable response.</p>
<p>Success!</p>
</div>
</div>
</div>
<div id="cool-applications-in-ecology" class="section level1">
<h1>Cool applications in ecology</h1>
<p>Like I mentioned earlier, these models only get better the more data that is collected. As ecological research scales with modern remote sensing and large scale data collection, neural networks are going to become increasingly relevant in both data collection as well as interpretation.</p>
<ul>
<li>Predicting trout spawning habitat (Lek et al. 1996)</li>
<li>Invertebrate speciation under magnification</li>
<li>Processing trail camera data and population monitoring (Snapshot Serengeti ?)</li>
<li>Passively tracking migratory animals <a href="https://github.com/keaneflynn/LOTIC">Salmon migration tracking</a></li>
</ul>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<ul>
<li>Not implicitly statistics, but a very math dense process</li>
<li>Neural networks are a fast emerging field of environmental science</li>
<li>As data availability scales, they will become more prominent</li>
<li>Can be an incredibly effective real time tool given the correct hardware</li>
<li>Limitations in understanding correlations and mechanisms, especially on larger neural nets</li>
</ul>
<div id="additional-sources-of-information" class="section level3">
<h3>Additional sources of information</h3>
<div id="general-neural-network" class="section level4">
<h4>General neural network</h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=oV3ZY6tJiA0">Neural Network Information</a> (this, however, is on images)</li>
<li><a href="https://www.ibm.com/cloud/learn/neural-networks">IBM on Neural Networks</a></li>
<li><a href="https://www.edx.org/learn/neural-network">Free courses on Neural Networks</a> (probably python)</li>
</ul>
</div>
<div id="backpropogation" class="section level4">
<h4>Backpropogation</h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">Backpropogation Overview</a></li>
<li><a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">Backpropogation Calculus</a></li>
</ul>
</div>
</div>
<div id="citations" class="section level3">
<h3>Citations</h3>
<ol style="list-style-type: decimal">
<li>Hecht-Nielsen, R. (1992). Theory of the backpropagation neural network. In Neural networks for perception (pp. 65-93). Academic Press.</li>
<li>Lek, S., Delacoste, M., Baran, P., Dimopoulos, I., Lauga, J., &amp; Aulagnier, S. (1996). Application of neural networks to modelling nonlinear relationships in ecology. Ecological modelling, 90(1), 39-52.</li>
<li>Lek, M., Karczewski, K. J., Minikel, E. V., Samocha, K. E., Banks, E., Fennell, T., … &amp; MacArthur, D. G. (2016). Analysis of protein-coding genetic variation in 60,706 humans. Nature, 536(7616), 285-291.</li>
<li>Humane Society Of The United States. The Humane Society of the United States . United States, 2001.</li>
</ol>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
