<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-18" />

<title>Likelihood!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746, Fall 2016</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Course Schedule</a>
</li>
<li>
  <a href="labschedule.html">Lab Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The virtual ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
  </ul>
</li>
<li>
  <a href="Syllabus.pdf">Syllabus</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Likelihood!</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 18, 2016</em></h4>

</div>


<p>In the last class, we talked about simulating data from models. This is often called <em>forward</em> modeling- that is, we take a model and use it to predict emergent patterns. The process flow goes something like this:</p>
<p><span class="math inline">\(Model \rightarrow Data\)</span></p>
<p>In this class, we will talk about <em>inference</em> using the method of <strong>maximum likelihood</strong>. Inference is in some ways the opposite process (sometimes called <em>inverse</em> modeling). We are using the data to say something about the model.</p>
<p><span class="math inline">\(Data \rightarrow Model\)</span></p>
<p>In many ways, these two processes – <em>simulation</em> and <em>inference</em> – are inter-related. Simulation can help us to make better inferences, and inference can help us to construct better simulation models!</p>
<p>Let’s see how simulation can help us to make inference. This leads directly into the core idea of maximum likelihood!</p>
<div id="using-data-simulation-to-make-inferences" class="section level2">
<h2>Using data simulation to make inferences</h2>
<p>Let’s use the “mtcars” data for this example: (note: some code borrowed from <a href="http://stats.stackexchange.com/questions/142443/simple-non-linear-regression-problem">here</a>)</p>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Looks nonlinear, with relatively constant variance across parameter space. So let’s see if we can build a model that could possibly produce these data!!</p>
<p>Since this looks a little like an exponential decline, let’s first build a function that can generate data that follows that deterministic function:</p>
<p><span class="math inline">\(mpg = N\left \{ intercept\cdot e^{slope\cdot displacement} ,Variance\right \}\)</span></p>
<p>Or, if we package the parameters simply as params a, b, and c:</p>
<p><span class="math inline">\(mpg = N\left \{ a\cdot e^{b\cdot displacement} ,c\right \}\)</span></p>
<pre class="r"><code>Deterministic_component &lt;- function(xvals,a,b){
  yexp &lt;- a*exp(b*xvals)
  return(yexp)
}

DataGenerator_exp &lt;- function(xvals,params){
  yexp &lt;- Deterministic_component(xvals,params$a,params$b)  # get signal
  yvals &lt;- rnorm(length(yexp),yexp,sqrt(params$c))     # add noise
  return(yvals)
}</code></pre>
<p>Let’s test this function to see if it does what we want:</p>
<pre class="r"><code>xvals=mtcars$disp    # xvals same as data (this is a &quot;fixed effect&quot;, so there is no random component here- we can&#39;t really &quot;sample&quot; x values)
params &lt;- list()  
params$a=30
params$b=-0.005
params$c=1

yvals &lt;- DataGenerator_exp(xvals,params)

plot(yvals~xvals)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Okay, looks reasonable. Now, let’s write a function to generate multiple replicate datasets from a particular model and plot out the barplots across measured parameter space.</p>
<pre class="r"><code>PlotRangeOfPlausibleData &lt;- function(xvals,params,reps){ 
  samplesize &lt;- length(xvals)
  results &lt;- array(0,dim=c(samplesize,reps))   # storage array for results
  for(i in 1:reps){
    yvals &lt;- DataGenerator_exp(xvals,params)
    results[,i] &lt;- yvals
  }
      # now make a boxplot of the results
  boxplot(lapply(1:nrow(results), function(i) results[i,]),at=xvals, xaxt=&quot;n&quot;,main=&quot;Plausible data under this model&quot;,ylab=&quot;mpg&quot;,xlab=&quot;Displacement&quot;,boxwex=6)
  cleanseq &lt;- (seq(0,max(round(xvals/100)),length=(max(round(xvals/100)))+1))*100
  axis(1,at=cleanseq,labels = cleanseq)    # label the x axis properly
  
}</code></pre>
<p>Let’s try it out!</p>
<pre class="r"><code>reps &lt;- 1000    # number of replicate datasets to generate

PlotRangeOfPlausibleData(xvals,params,reps)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Now we can overlay the data and see how well we did!</p>
<pre class="r"><code>real_yvals &lt;- mtcars$mpg
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Okay, not very good yet. Let’s see if we can improve this by changing the parameters. Let’s increase the intercept and reduce the slope:</p>
<pre class="r"><code>params$a=40       # was 30
params$b=-0.001   # was 0.005

    
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col=&quot;green&quot;)    # overlay the real data</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Oops- we overshot!! Let’s find something in the middle!</p>
<pre class="r"><code>params$a=33       # was 40
params$b=-0.002   # was 0.001

    
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col=&quot;green&quot;)    # overlay the real data</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Much better! This model could plausibly generate most of these data!</p>
<p>So, we have used simulation, along with trial and error, to infer parameter values for our model!</p>
</div>
<div id="computing-data-likelihood" class="section level2">
<h2>Computing data <em>likelihood</em></h2>
<p>First of all, what does “data likelihood” really mean? Formally,</p>
<p><span class="math inline">\(\pounds (Model|obs.data) \equiv Prob(obs.data|Model)\)</span></p>
<div id="definition-in-plain-english" class="section level3">
<h3>Definition, in plain English!</h3>
<p>The likelihood of a set of parameters $$ given some observed data is equal to the <em>probability</em> of observing these data given those particular parameter values. In this way, likelihood is a quantitative measure of <em>goodness-of-fit</em>. Higher likelihoods correspond to a higher probability of the model producing the observed data.</p>
</div>
<div id="worked-example" class="section level3">
<h3>Worked example</h3>
<p>Let’s go through an example! For simplicity, let’s stick with the cars example for now.</p>
<p>For simplicity, let’s consider only the first observation:</p>
<pre class="r"><code>obs.data &lt;- mtcars[1,c(&quot;mpg&quot;,&quot;disp&quot;)]
obs.data</code></pre>
<pre><code>##           mpg disp
## Mazda RX4  21  160</code></pre>
<p>Remember, we are considering the following data generating model:</p>
<p><span class="math inline">\(mpg = N\left \{ a\cdot e^{b\cdot displacement} ,c\right \}\)</span></p>
<p>Let’s assume for a second that the parameters we selected in our trial-and-error exercise above are the true parameters. What is the expected value of our observation under this data generating model.</p>
<pre class="r"><code>############
# &quot;best fit&quot; parameters from above
############

params$a=33       # was 40
params$b=-0.002   # was 0.001
params$c=1

params</code></pre>
<pre><code>## $a
## [1] 33
## 
## $b
## [1] -0.002
## 
## $c
## [1] 1</code></pre>
<pre class="r"><code>expected_val &lt;- Deterministic_component(obs.data$disp,params$a,params$b)
expected_val</code></pre>
<pre><code>## [1] 23.96292</code></pre>
<p>Okay we now know our expected (mean) value for mpg for a car with displacement of 160 cubic inches. We also know the observed mpg for a car with a displacement of 160 cubic inches: it was 21 mpgs. Since the model also specifies the variance (1) we can compute the probability of observing a car with 21 mpgs under our model.</p>
<pre class="r"><code>mean = expected_val   # 23.96
stdev = sqrt(params$c)

curve(dnorm(x,mean,stdev),10,30,xlab=&quot;mpg&quot;,ylab=&quot;probability density&quot;)   # probability density
abline(v=obs.data$mpg,col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Now it is straightforward to compute the likelihood. We just need to know the probability density where the red line (observed data) intersects with the normal density curve above:</p>
<pre class="r"><code>likelihood = dnorm(obs.data$mpg,mean,stdev)
likelihood</code></pre>
<pre><code>## [1] 0.004949936</code></pre>
<p>Now let’s consider a second observation as well!</p>
<pre class="r"><code>obs.data &lt;- mtcars[c(1,3),c(&quot;mpg&quot;,&quot;disp&quot;)]
obs.data</code></pre>
<pre><code>##             mpg disp
## Mazda RX4  21.0  160
## Datsun 710 22.8  108</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))  # set up graphics!

for(i in 1:nrow(obs.data)){
  curve(dnorm(x,Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c)),10,30,xlab=&quot;mpg&quot;,ylab=&quot;probability density&quot;)   # probability density
  abline(v=obs.data$mpg[i],col=&quot;red&quot;,lwd=2)
}</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>What is the likelihood of observing both of these data points???</p>
<p><span class="math inline">\(Prob(obs.data_{1}|Model])\cdot Prob(obs.data_{2}|Model])\)</span></p>
<pre class="r"><code>Likelihood &lt;- dnorm(obs.data$mpg[1],Deterministic_component(obs.data$disp[1],params$a,params$b),sqrt(params$c)) *
              dnorm(obs.data$mpg[2],Deterministic_component(obs.data$disp[2],params$a,params$b),sqrt(params$c))  
Likelihood</code></pre>
<pre><code>## [1] 1.505202e-06</code></pre>
<p>Let’s consider four observations:</p>
<pre class="r"><code>obs.data &lt;- mtcars[c(1,3,4,5),c(&quot;mpg&quot;,&quot;disp&quot;)]
obs.data</code></pre>
<pre><code>##                    mpg disp
## Mazda RX4         21.0  160
## Datsun 710        22.8  108
## Hornet 4 Drive    21.4  258
## Hornet Sportabout 18.7  360</code></pre>
<pre class="r"><code>par(mfrow=c(2,2))  # set up graphics!

for(i in 1:nrow(obs.data)){
  curve(dnorm(x,Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c)),10,30,xlab=&quot;mpg&quot;,ylab=&quot;probability density&quot;)   # probability density
  abline(v=obs.data$mpg[i],col=&quot;red&quot;,lwd=2)
}</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>What is the combined likelihood of all of these four data points, assuming each observation is independent…</p>
<pre class="r"><code>Likelihood &lt;- 1     # initialize the likelihood
for(i in 1:nrow(obs.data)){
  Likelihood &lt;- Likelihood * dnorm(obs.data$mpg[i],Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c))
}
Likelihood</code></pre>
<pre><code>## [1] 1.738001e-09</code></pre>
<p>Alternatively, we can use the “prod” function in R:</p>
<pre class="r"><code>Likelihood &lt;- prod(dnorm(obs.data$mpg,Deterministic_component(obs.data$disp,params$a,params$b),sqrt(params$c)))
Likelihood</code></pre>
<pre><code>## [1] 1.738001e-09</code></pre>
<p>Okay, so it should be fairly obvious how we might get the likelihood of the entire dataset. Assuming independence of observations of course!</p>
<pre class="r"><code>full.data &lt;- mtcars[,c(&quot;mpg&quot;,&quot;disp&quot;)]
Likelihood &lt;- prod(dnorm(full.data$mpg,Deterministic_component(full.data$disp,params$a,params$b),sqrt(params$c)))
Likelihood</code></pre>
<pre><code>## [1] 4.654279e-84</code></pre>
<p>You may notice that that’s a pretty small number. When you multiply lots of really small numbers together, we get much smaller numbers. This can be very undesirable, especially when you run into computational errors (e.g., arithmetic overflow). For this reason, and because we generally like sums rather than products, we generally log-transform likelihoods. As you recall,</p>
<p><span class="math inline">\(log(a\cdot b\cdot c)=log(a)+log(b)+log(c)\)</span></p>
<p>Log transformations just make it easier to work with likelihoods!</p>
<p>In fact, the probability distribution functions in R make it extra easy for us to work with log likelihoods, using the ‘log=TRUE’ option!</p>
<pre class="r"><code>Log.Likelihood &lt;- sum(dnorm(full.data$mpg,Deterministic_component(full.data$disp,params$a,params$b),sqrt(params$c),log=TRUE)) 
Log.Likelihood  </code></pre>
<pre><code>## [1] -191.8794</code></pre>
<pre class="r"><code>exp(Log.Likelihood)   # we can convert back to likelihood if we want...</code></pre>
<pre><code>## [1] 4.654279e-84</code></pre>
</div>
</div>
<div id="maximum-likelihood-estimation-mle" class="section level2">
<h2>Maximum Likelihood Estimation (MLE)</h2>
<p>Maximum likelihood estimation is a general, flexible and generally robust method for drawing inference about models from data.</p>
<p>The basic idea is simple: given a data generating model, search for the set of parameters that maximizes the data likelihood!</p>
<p>The steps of a typical MLE analysis are as follows:</p>
<ol style="list-style-type: decimal">
<li>Build a likelihood function</li>
<li>Use numerical optimization algorithms to find the parameters that maximize the likelihood function</li>
<li>Use the shape of the likelihood function to make inference about parameter uncertainty (e.g., confidence intervals)</li>
</ol>
<p>Okay, so what is a <em>likelihood function</em>? We basically already have this for the cars example: it is just a function that produces a joint likelhood value, given (1) a <strong>dataset</strong> and (2) a <strong>data generating function</strong>.</p>
<pre class="r"><code>LogLikFunction &lt;- function(params,df,yvar,xvar){
  LogLik &lt;- sum(dnorm(df[,yvar],Deterministic_component(df[,xvar],params[&#39;a&#39;],params[&#39;b&#39;]),sqrt(params[&#39;c&#39;]),log=TRUE))
  return(LogLik)
}
LogLikFunction(unlist(params),df=mtcars,yvar=&quot;mpg&quot;,xvar=&quot;disp&quot;)</code></pre>
<pre><code>## [1] -191.8794</code></pre>
<p>Now that we have a likelihood function, we need to search parameter space for the parameter set that maximizes the log likelhood. Luckily, there are lots of <em>computational algorithms</em> that can do this. We will look at this in detail in the next lecture. For now, we just need to know that they exist, and that they can be harnessed using the ‘optim’ function in R.</p>
<p>Let’s find the MLE for the three parameters in the cars example!!</p>
<pre class="r"><code>MLE &lt;- optim(fn=LogLikFunction,par=unlist(params),df=mtcars,yvar=&quot;mpg&quot;,xvar=&quot;disp&quot;,control=list(fnscale=-1))  # note, the control param is set so that &quot;optim&quot; maximizes rather than minimizes the Log-likelihood. </code></pre>
<p>Now, we can get the MLEs for the three parameters:</p>
<pre class="r"><code>MLE$par</code></pre>
<pre><code>##            a            b            c 
## 33.077114768 -0.002338554  8.166392492</code></pre>
<p>We can also get the log likelihood for the best model</p>
<pre class="r"><code>MLE$value</code></pre>
<pre><code>## [1] -79.0089</code></pre>
<p>Let’s look at goodness-of-fit for the best model!</p>
<pre class="r"><code>bestParams &lt;- as.list(MLE$par)

xvals=mtcars$disp
yvals &lt;- mtcars$mpg
PlotRangeOfPlausibleData(xvals,bestParams,1000)
points(xvals,yvals,pch=20,cex=3,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Now this isn’t such a bad looking model!</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
