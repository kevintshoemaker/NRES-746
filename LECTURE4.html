<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-18" />

<title>Likelihood!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Model Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
    <li>
      <a href="uta_simulated_data.csv">Uta data</a>
    </li>
    <li>
      <a href="tide_ALL_navd_HH.csv">tide data?</a>
    </li>
    <li>
      <a href="TestDataset2.csv">test dataset for SDM?</a>
    </li>
    <li>
      <a href="PRISM_tmean_189511_201610.csv">Reed Frog Func Resp</a>
    </li>
    <li>
      <a href="PRISM_ppt_1895-2015Mo2.csv">Reed Frog Func Resp</a>
    </li>
    <li>
      <a href="final_winter_modeldata2.csv">winter model data</a>
    </li>
    <li>
      <a href="keeley_rawdata.csv">Keeley data</a>
    </li>
    <li>
      <a href="Nest_basic_ALL.csv">nest basic data</a>
    </li>
    <li>
      <a href="Cap_data_for SA.csv">cap data for SA</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="SEM_Minilab_v2.html">SEM mini-lab</a>
    </li>
    <li>
      <a href="GAMs.html">GAMs</a>
    </li>
    <li>
      <a href="RMarkdown_FigureDemo.html">Publication-quality figures in R</a>
    </li>
    <li>
      <a href="Bayesian Networks.pptx">Bayesian Networks</a>
    </li>
    <li>
      <a href="Bayes_Network_Markdown_Final.html">Bayesian Networks mini-lab</a>
    </li>
    <li>
      <a href="GraphTheory.html">Graph Theory</a>
    </li>
    <li>
      <a href="NRES746_IPMs.pptx">Integrated Population Models</a>
    </li>
    <li>
      <a href="TimeSeries_heckler.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="Time_Series_Lab.html">Time-series mini-lab</a>
    </li>
    <li>
      <a href="Spatial_Autocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="SA_minilab.html">Spatial Autocorrelation mini-lab</a>
    </li>
    <li>
      <a href="SDM_pres.html">Species Distribution Modeling</a>
    </li>
    <li>
      <a href="IPM_miniLab.html">IPM mini-lab</a>
    </li>
    <li>
      <a href="Final_MiniLabScript.html">RSF mini-lab</a>
    </li>
    <li>
      <a href="MixedModelMinilab.html">Mixed-effects model mini-lab</a>
    </li>
    <li>
      <a href="Minilab_Ordination.html">Ordination mini-lab</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Likelihood!</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 18, 2016</em></h4>

</div>


<p>In the last class, we talked about simulating data from models. This is often called <em>forward</em> modeling- that is, we take a model and use it to predict emergent patterns. The process flow goes something like this:</p>
<p><span class="math inline">\(Model \rightarrow Data\)</span></p>
<p>In this class, we will talk about <em>inference</em> using the method of <strong>maximum likelihood</strong>. Inference is in some ways the opposite process (sometimes called <em>inverse</em> modeling). We are using the data to say something about the model.</p>
<p><span class="math inline">\(Data \rightarrow Model\)</span></p>
<p>In many ways, these two processes – <em>simulation</em> and <em>inference</em> – are inter-related. Simulation can help us to make better inferences, and inference can help us to construct better simulation models!</p>
<p>Let’s see how simulation can help us to make inference. This leads directly into the core idea of maximum likelihood!</p>
<div id="using-data-simulation-to-make-inferences" class="section level2">
<h2>Using data simulation to make inferences</h2>
<p>Let’s use the “mtcars” data for this example: (note: some code borrowed from <a href="http://stats.stackexchange.com/questions/142443/simple-non-linear-regression-problem">here</a>)</p>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Looks nonlinear, with relatively constant variance across parameter space. So let’s see if we can build a model that could possibly produce these data!!</p>
<p>Since this looks a little like an exponential decline, let’s first build a function that can generate data that follows that deterministic function:</p>
<p><span class="math inline">\(mpg = N\left \{ intercept\cdot e^{slope\cdot displacement} ,Variance\right \}\)</span></p>
<p>Or, if we package the parameters simply as params a, b, and c:</p>
<p><span class="math inline">\(mpg = N\left \{ a\cdot e^{b\cdot displacement} ,c\right \}\)</span></p>
<pre class="r"><code>Deterministic_component &lt;- function(xvals,a,b){
  yexp &lt;- a*exp(b*xvals)        # deterministic exponential decay
  return(yexp)
}

DataGenerator_exp &lt;- function(xvals,params){
  yexp &lt;- Deterministic_component(xvals,params$a,params$b)  # get signal
  yvals &lt;- rnorm(length(yexp),yexp,sqrt(params$c))     # add noise
  return(yvals)
}</code></pre>
<p>Let’s test this function to see if it does what we want:</p>
<pre class="r"><code>xvals=mtcars$disp    # xvals same as data (this is a &quot;fixed effect&quot;, so there is no random component here- we can&#39;t really &quot;sample&quot; x values)
params &lt;- list()  
params$a=30
params$b=-0.005   # = 1/200
params$c=1

yvals &lt;- DataGenerator_exp(xvals,params)

plot(yvals~xvals)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Okay, looks reasonable. Now, let’s write a function to generate multiple replicate datasets from a particular model and make boxplots describing the plausible data produced by the model across measured parameter space.</p>
<pre class="r"><code>PlotRangeOfPlausibleData &lt;- function(xvals,params,reps){ 
  samplesize &lt;- length(xvals)
  results &lt;- array(0,dim=c(samplesize,reps))   # storage array for results
  for(i in 1:reps){
    yvals &lt;- DataGenerator_exp(xvals,params)
    results[,i] &lt;- yvals
  }
      # now make a boxplot of the results
  boxplot(lapply(1:nrow(results), function(i) results[i,]),at=xvals, xaxt=&quot;n&quot;,main=&quot;Plausible data under this model&quot;,ylab=&quot;mpg&quot;,xlab=&quot;Displacement&quot;,boxwex=6)
  cleanseq &lt;- (seq(0,max(round(xvals/100)),length=(max(round(xvals/100)))+1))*100
  axis(1,at=cleanseq,labels = cleanseq)    # label the x axis properly
  
}</code></pre>
<p>Let’s try it out!</p>
<pre class="r"><code>reps &lt;- 1000    # number of replicate datasets to generate

PlotRangeOfPlausibleData(xvals,params,reps)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Now we can overlay the data and see how well we did!</p>
<pre class="r"><code>real_yvals &lt;- mtcars$mpg
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Okay, not very good yet. Let’s see if we can improve this by changing the parameters. Let’s increase the intercept and reduce the slope:</p>
<pre class="r"><code>params$a=40       # was 30
params$b=-0.001   # was 0.005

    
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col=&quot;green&quot;)    # overlay the real data</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Oops- we overshot!! Let’s find something in the middle!</p>
<pre class="r"><code>params$a=33       # was 40
params$b=-0.002   # was 0.001

    
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col=&quot;green&quot;)    # overlay the real data</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Much better! This model could plausibly generate most of these data!</p>
<p>So, we have used simulation, along with trial and error, to infer parameter values for our model!</p>
</div>
<div id="computing-data-likelihood" class="section level2">
<h2>Computing data <em>likelihood</em></h2>
<p>First of all, what does “data likelihood” really mean? Formally,</p>
<p><span class="math inline">\(\mathcal{L} (Model|obs.data) \equiv Prob(obs.data|Model)\)</span></p>
<div id="definition-in-plain-english" class="section level3">
<h3>Definition, in plain English!</h3>
<p>The likelihood of a set of parameters $$ given some observed data is equal to the <em>probability</em> of observing these data given those particular parameter values. In this way, likelihood is a quantitative measure of <em>goodness-of-fit</em>. Higher likelihoods correspond to a higher probability of the model producing the observed data.</p>
</div>
<div id="worked-example" class="section level3">
<h3>Worked example</h3>
<p>Let’s go through an example! For simplicity, let’s stick with the cars example for now.</p>
<p>For simplicity, let’s consider only the first observation:</p>
<pre class="r"><code>obs.data &lt;- mtcars[1,c(&quot;mpg&quot;,&quot;disp&quot;)]
obs.data</code></pre>
<pre><code>##           mpg disp
## Mazda RX4  21  160</code></pre>
<p>Remember, we are considering the following data generating model:</p>
<p><span class="math inline">\(mpg = N\left \{ a\cdot e^{b\cdot displacement} ,c\right \}\)</span></p>
<p>Let’s assume for a second that the parameters we selected in our trial-and-error exercise above are the true parameters. What is the expected value of our observation under this data generating model.</p>
<pre class="r"><code>############
# &quot;best fit&quot; parameters from above
############

params$a=33       # was 40
params$b=-0.002   # was 0.001
params$c=1

params</code></pre>
<pre><code>## $a
## [1] 33
## 
## $b
## [1] -0.002
## 
## $c
## [1] 1</code></pre>
<pre class="r"><code>expected_val &lt;- Deterministic_component(obs.data$disp,params$a,params$b)
expected_val</code></pre>
<pre><code>## [1] 23.96292</code></pre>
<p>Okay we now know our expected (mean) value for mpg for a car with displacement of 160 cubic inches. We also know the observed mpg for a car with a displacement of 160 cubic inches: it was 21 mpgs. Since the model also specifies the variance (1) we can compute the probability of observing a car with 21 mpgs under our model.</p>
<pre class="r"><code>mean = expected_val   # 23.96
stdev = sqrt(params$c)

curve(dnorm(x,mean,stdev),10,30,xlab=&quot;mpg&quot;,ylab=&quot;probability density&quot;)   # probability density
abline(v=obs.data$mpg,col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Now it is straightforward to compute the likelihood. We just need to know the probability density where the red line (observed data) intersects with the normal density curve above:</p>
<pre class="r"><code>likelihood = dnorm(obs.data$mpg,mean,stdev)
likelihood</code></pre>
<pre><code>## [1] 0.004949936</code></pre>
<p>Now let’s consider a second observation as well!</p>
<pre class="r"><code>obs.data &lt;- mtcars[c(1,3),c(&quot;mpg&quot;,&quot;disp&quot;)]
obs.data</code></pre>
<pre><code>##             mpg disp
## Mazda RX4  21.0  160
## Datsun 710 22.8  108</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))  # set up graphics!

for(i in 1:nrow(obs.data)){
  curve(dnorm(x,Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c)),10,30,xlab=&quot;mpg&quot;,ylab=&quot;probability density&quot;)   # probability density
  abline(v=obs.data$mpg[i],col=&quot;red&quot;,lwd=2)
}</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>What is the likelihood of observing both of these data points???</p>
<p><span class="math inline">\(Prob(obs.data_{1}|Model])\cdot Prob(obs.data_{2}|Model])\)</span></p>
<pre class="r"><code>Likelihood &lt;- dnorm(obs.data$mpg[1],Deterministic_component(obs.data$disp[1],params$a,params$b),sqrt(params$c)) *
              dnorm(obs.data$mpg[2],Deterministic_component(obs.data$disp[2],params$a,params$b),sqrt(params$c))  
Likelihood</code></pre>
<pre><code>## [1] 1.505202e-06</code></pre>
<p>Let’s consider four observations:</p>
<pre class="r"><code>obs.data &lt;- mtcars[c(1,3,4,5),c(&quot;mpg&quot;,&quot;disp&quot;)]
obs.data</code></pre>
<pre><code>##                    mpg disp
## Mazda RX4         21.0  160
## Datsun 710        22.8  108
## Hornet 4 Drive    21.4  258
## Hornet Sportabout 18.7  360</code></pre>
<pre class="r"><code>par(mfrow=c(2,2))  # set up graphics!

for(i in 1:nrow(obs.data)){
  curve(dnorm(x,Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c)),10,30,xlab=&quot;mpg&quot;,ylab=&quot;probability density&quot;)   # probability density
  abline(v=obs.data$mpg[i],col=&quot;red&quot;,lwd=2)
}</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>What is the combined likelihood of all of these four data points, assuming each observation is independent…</p>
<pre class="r"><code>Likelihood &lt;- 1     # initialize the likelihood
for(i in 1:nrow(obs.data)){
  Likelihood &lt;- Likelihood * dnorm(obs.data$mpg[i],Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c))
}
Likelihood</code></pre>
<pre><code>## [1] 1.738001e-09</code></pre>
<p>Alternatively, we can use the “prod” function in R:</p>
<pre class="r"><code>Likelihood &lt;- prod(dnorm(obs.data$mpg,Deterministic_component(obs.data$disp,params$a,params$b),sqrt(params$c)))
Likelihood</code></pre>
<pre><code>## [1] 1.738001e-09</code></pre>
<p>Okay, so it should be fairly obvious how we might get the likelihood of the entire dataset. Assuming independence of observations of course!</p>
<pre class="r"><code>full.data &lt;- mtcars[,c(&quot;mpg&quot;,&quot;disp&quot;)]
Likelihood &lt;- prod(dnorm(full.data$mpg,Deterministic_component(full.data$disp,params$a,params$b),sqrt(params$c)))
Likelihood</code></pre>
<pre><code>## [1] 4.654279e-84</code></pre>
<p>You may notice that that’s a pretty small number. When you multiply lots of really small numbers together, we get much smaller numbers. This can be very undesirable, especially when you run into computational errors (e.g., arithmetic overflow). For this reason, and because we generally like sums rather than products, we generally log-transform likelihoods. As you recall,</p>
<p><span class="math inline">\(log(a\cdot b\cdot c)=log(a)+log(b)+log(c)\)</span></p>
<p>Log transformations just make it easier to work with likelihoods!</p>
<p>In fact, the probability distribution functions in R make it extra easy for us to work with log likelihoods, using the ‘log=TRUE’ option!</p>
<pre class="r"><code>Log.Likelihood &lt;- sum(dnorm(full.data$mpg,Deterministic_component(full.data$disp,params$a,params$b),sqrt(params$c),log=TRUE)) 
Log.Likelihood  </code></pre>
<pre><code>## [1] -191.8794</code></pre>
<pre class="r"><code>exp(Log.Likelihood)   # we can convert back to likelihood if we want...</code></pre>
<pre><code>## [1] 4.654279e-84</code></pre>
</div>
</div>
<div id="maximum-likelihood-estimation-mle" class="section level2">
<h2>Maximum Likelihood Estimation (MLE)</h2>
<p>Maximum likelihood estimation is a general, flexible and generally robust method for drawing inference about models from data.</p>
<p>The basic idea is simple: given a data generating model, search for the set of parameters that maximizes the data likelihood!</p>
<p>The steps of a typical MLE analysis are as follows:</p>
<ol style="list-style-type: decimal">
<li>Build a likelihood function</li>
<li>Use numerical optimization algorithms to find the parameters that maximize the likelihood function</li>
<li>Use the shape of the likelihood function to make inference about parameter uncertainty (e.g., confidence intervals)</li>
</ol>
<p>Okay, so what is a <em>likelihood function</em>? We basically already have this for the cars example: it is just a function that produces a joint likelhood value, given (1) a <strong>dataset</strong> and (2) a <strong>data generating function</strong>.</p>
<pre class="r"><code>LogLikFunction &lt;- function(params,df,yvar,xvar){
  LogLik &lt;- sum(dnorm(df[,yvar],Deterministic_component(df[,xvar],params[&#39;a&#39;],params[&#39;b&#39;]),sqrt(params[&#39;c&#39;]),log=TRUE))
  return(LogLik)
}
LogLikFunction(unlist(params),df=mtcars,yvar=&quot;mpg&quot;,xvar=&quot;disp&quot;)</code></pre>
<pre><code>## [1] -191.8794</code></pre>
<p>Now that we have a likelihood function, we need to search parameter space for the parameter set that maximizes the log likelhood. Luckily, there are lots of <em>computational algorithms</em> that can do this. We will look at this in detail in the next lecture. For now, we just need to know that they exist, and that they can be harnessed using the ‘optim’ function in R.</p>
<p>Let’s find the MLE for the three parameters in the cars example!!</p>
<pre class="r"><code>MLE &lt;- optim(fn=LogLikFunction,par=unlist(params),df=mtcars,yvar=&quot;mpg&quot;,xvar=&quot;disp&quot;,control=list(fnscale=-1))  # note, the control param is set so that &quot;optim&quot; maximizes rather than minimizes the Log-likelihood. </code></pre>
<p>Now, we can get the MLEs for the three parameters:</p>
<pre class="r"><code>MLE$par</code></pre>
<pre><code>##            a            b            c 
## 33.077114768 -0.002338554  8.166392492</code></pre>
<p>We can also get the log likelihood for the best model</p>
<pre class="r"><code>MLE$value</code></pre>
<pre><code>## [1] -79.0089</code></pre>
<p>Let’s look at goodness-of-fit for the best model!</p>
<pre class="r"><code>bestParams &lt;- as.list(MLE$par)

xvals=mtcars$disp
yvals &lt;- mtcars$mpg
PlotRangeOfPlausibleData(xvals,bestParams,1000)
points(xvals,yvals,pch=20,cex=3,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Now this isn’t such a bad looking model!</p>
<p>Using this same strategy, we can fit countless models to data. We just performed a standard non-linear regression. But we could just as well have fit any number of alternative model formulations. Maximum Likelihood is a powerful and flexible framework. We will have plenty of more opportunities to play with likeihood-based frameworks, both in a frequentist and a Bayesian context.</p>
<div id="aside-generating-initial-values" class="section level3">
<h3>Aside: generating initial values</h3>
<p>Note that the ‘optim’ function requires specification of initial values for every free parameter in your model. In general, it is not critical that the initial values fit the data very well. However, we need to put serious thought into our initial values. Using values that are too far away from the best-fit parameter estimates can cause our optimization algorithms to fail! The strategy I recommend for setting initial values is:</p>
<ul>
<li>If possible, use general understanding of the meaning of the parameters to “eyeball” an approximate value for each parameter</li>
<li>Write a data-generation function (sometimes just the deterministic component might be enough) and plot out this function against the observed data. If the fit is “close-ish” then you should be able to use these values for your initial values. If not, tweak the parameters until you find something that is “close-ish”.</li>
</ul>
</div>
</div>
<div id="estimating-parameter-uncertainty" class="section level2">
<h2>Estimating parameter uncertainty</h2>
<p>We have now identified the parameter values that maximize the likelihood function. This is now our “best” estimate of the parameter value. But we usually want to know something about how certain we are about our estimate, often in the form of confidence intervals.</p>
<p>Fortunately, likelihood theory offers us a strategy for estimating confidence intervals around our parameter estimates. The idea is that the <em>shape</em> of the likelihood function tells us something about the range of parameter values under which the true parameter value may potentially fall.</p>
<p>For instance, let’s plot out the shape of the likelihood function across a <strong>slice</strong> of parameter space. In this case, let’s look at the “decay rate” term (the ‘b’ parameter). We can vary that parameter over a range of possible values and see how the likelihood changes over that parameter space. Let’s hold the other parameters at their maximum likelihood values:</p>
<pre class="r"><code>upperval &lt;- -1/1000
lowerval &lt;- -1/200
allvals &lt;- seq(lowerval,upperval,length=1000)
likelihood_slice &lt;- numeric(1000)   # set up storage vector! 
newParams &lt;- bestParams 
for(i in c(1:length(allvals))){
  newParams$b &lt;- allvals[i]
  likelihood_slice[i] &lt;- exp(LogLikFunction(unlist(newParams),mtcars,&quot;mpg&quot;,&quot;disp&quot;))    # get the data likelihood across slice of parameter space
}

plot(allvals,likelihood_slice,type=&quot;l&quot;,main=&quot;Likelihood Slice&quot;,xlab=&quot;Parameter Slice for \&#39;b\&#39;&quot;,ylab=&quot;Likelihood&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Again, we generally want to work with log-likelihoods. And here we have an even better reason to use log-likelihoods. This reason: the “rule of 2”, and (more formally) the <strong>likelihood ratio test</strong>.</p>
<p>For now, suffice it to say that all parameter values within 2 log-likelihood units of the best-fit parameter value are <em>plausible</em>. Therefore, a <em>reasonable</em> confidence interval (approximate 95% conf int) can be obtained by idenfifying the range of parameter values for which the log-likelihood is within two of the maximum likelihood…</p>
<p>Let’s plot out the log-likelihood slice:</p>
<pre class="r"><code>upperval &lt;- -1/1000
lowerval &lt;- -1/200
allvals &lt;- seq(lowerval,upperval,length=1000)
loglikelihood_slice &lt;- numeric(1000)   # set up storage vector! 
newParams &lt;- bestParams 
for(i in c(1:length(allvals))){
  newParams$b &lt;- allvals[i]
  loglikelihood_slice[i] &lt;- LogLikFunction(unlist(newParams),mtcars,&quot;mpg&quot;,&quot;disp&quot;)    # get the data likelihood across slice of parameter space
}

plot(allvals,loglikelihood_slice,type=&quot;l&quot;,main=&quot;Log Likelihood Slice&quot;,xlab=&quot;Parameter Slice for \&#39;b\&#39;&quot;,ylab=&quot;Log-Likelihood&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Let’s “zoom in” to a smaller slice of parameter space so we can more clearly see the “plausible” values:</p>
<pre class="r"><code>upperval &lt;- -1/550
lowerval &lt;- -1/350
allvals &lt;- seq(lowerval,upperval,length=1000)
loglikelihood_slice &lt;- numeric(1000)   # set up storage vector! 
newParams &lt;- bestParams 
for(i in c(1:length(allvals))){
  newParams$b &lt;- allvals[i]
  loglikelihood_slice[i] &lt;- LogLikFunction(unlist(newParams),mtcars,&quot;mpg&quot;,&quot;disp&quot;)    # get the data likelihood across slice of parameter space
}

plot(allvals,loglikelihood_slice,type=&quot;l&quot;,main=&quot;Log Likelihood Slice&quot;,xlab=&quot;Parameter Slice for \&#39;b\&#39;&quot;,ylab=&quot;Log-Likelihood&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>What region of this space falls within 2 log likelihood units of the best value?</p>
<pre class="r"><code>bestVal &lt;- bestParams$b
bestVal</code></pre>
<pre><code>## [1] -0.002338554</code></pre>
<pre class="r"><code>plot(allvals,loglikelihood_slice,type=&quot;l&quot;,main=&quot;Log Likelihood Slice&quot;,xlab=&quot;Parameter Slice for \&#39;b\&#39;&quot;,ylab=&quot;Log-Likelihood&quot;)
abline(v=bestVal,lwd=3,col=&quot;blue&quot;)
abline(h=(MLE$value-2))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>So what is our 95% confidence interval in this case?? (remember this is <em>very approximate</em>!)</p>
<pre class="r"><code>reasonable_parameter_values &lt;- allvals[loglikelihood_slice&gt;=(MLE$value-2)]
min(reasonable_parameter_values)</code></pre>
<pre><code>## [1] -0.002595063</code></pre>
<pre class="r"><code>max(reasonable_parameter_values)</code></pre>
<pre><code>## [1] -0.002100022</code></pre>
<pre class="r"><code>plot(allvals,loglikelihood_slice,type=&quot;l&quot;,main=&quot;Log Likelihood slice&quot;,xlab=&quot;Parameter Slice for \&#39;b\&#39;&quot;,ylab=&quot;Log-Likelihood&quot;)
abline(v=bestVal,lwd=3,col=&quot;blue&quot;)
abline(h=(MLE$value-2),lty=2)
abline(v=min(reasonable_parameter_values),lwd=1,col=&quot;blue&quot;)
abline(v=max(reasonable_parameter_values),lwd=1,col=&quot;blue&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div id="key-point" class="section level4">
<h4>Key point!</h4>
<p>From the Bolker book:</p>
<ul>
<li>The geometry of the likelihood surface – where it peaks and how the distribution falls off around the peak – contains essentially all the information you need to estimate parameters and confidence intervals.</li>
</ul>
</div>
<div id="estimating-parameter-uncertainty-in-multiple-dimensions-the-likelihood-profile" class="section level3">
<h3>Estimating parameter uncertainty in multiple dimensions: the Likelihood Profile</h3>
<p>If we have more than one <em>free parameter</em> in our model, it seems strange to fix any parameter at a particular value, as we did in computing the “likelihood slice” above. It makes more sense to allow all the parameters to vary. For the purpose of visualization, let’s assume for now that we only have two free parameters in our model: ‘a’ and ‘b’. We will assume for now that the variance parameter is known for certain.</p>
<p>Let’s try to visualize the likelihood surface in two dimensions!</p>
<pre class="r"><code>upperval_b &lt;- -1/800
lowerval_b &lt;- -1/300

upperval_a &lt;- 50
lowerval_a &lt;- 5

allvals_a &lt;- seq(lowerval_a,upperval_a,length=500)
allvals_b &lt;- seq(lowerval_b,upperval_b,length=500)

loglikelihood_surface &lt;- matrix(0,nrow=500,ncol=500)   # set up storage matrix! 

newParams &lt;- bestParams 
for(i in 1:length(allvals_a)){  # loop through possible a params
  newParams$a &lt;- allvals_a[i]
  for(j in 1:length(allvals_b)){    # loop through possible b params
    newParams$b &lt;- allvals_b[j]
    loglikelihood_surface[i,j] &lt;- LogLikFunction(unlist(newParams),mtcars,&quot;mpg&quot;,&quot;disp&quot;)    # get the data likelihood across slice of parameter space
  }
}

image(x=allvals_b,y=allvals_a,z=loglikelihood_surface,zlim=c(-100,-75),col=topo.colors(12))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Now let’s add a contour line to indicate the <em>95% bivariate confidence region</em></p>
<pre class="r"><code>conf95 &lt;- qchisq(0.95,2)/2
image(x=allvals_a,y=allvals_b,z=loglikelihood_surface,zlim=c(-100,-75),col=topo.colors(12))
contour(x=allvals_a,y=allvals_b,z=loglikelihood_surface,levels=(MLE$value-conf95),add=TRUE,lwd=3,col=gray(0.3))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>So, what is the “<strong>profile likelihood</strong>” confidence interval for the a parameter?</p>
<p>By “profile likelihood” confidence interval, we mean this: we have a parameter of interest, and “nuisance parameter(s)”. For every value of the parameter of interest, we find the highest likelihood value across all possible values of all the other parameters.</p>
<pre class="r"><code>              ### A parameter
profile_A &lt;- apply(loglikelihood_surface,1,max)
reasonable_parameter_values_A &lt;- allvals_a[profile_A &gt;=(MLE$value-conf95)]
min(reasonable_parameter_values_A)</code></pre>
<pre><code>## [1] 29.25852</code></pre>
<pre class="r"><code>max(reasonable_parameter_values_A)</code></pre>
<pre><code>## [1] 37.28457</code></pre>
<pre class="r"><code>plot(allvals_a,profile_A,type=&quot;l&quot;,main=&quot;Log Likelihood profile&quot;,xlab=&quot;Parameter Slice for \&#39;a\&#39;&quot;,ylab=&quot;Log-Likelihood&quot;)
abline(v=MLE$par[&quot;a&quot;],lwd=3,col=&quot;blue&quot;)
abline(v=min(reasonable_parameter_values_A),lwd=1,col=&quot;blue&quot;)
abline(v=max(reasonable_parameter_values_A),lwd=1,col=&quot;blue&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>And the b parameter?</p>
<pre class="r"><code>profile_B &lt;- apply(loglikelihood_surface,2,max)
reasonable_parameter_values_B &lt;- allvals_b[profile_B &gt;=(MLE$value-conf95)]
min(reasonable_parameter_values_B)</code></pre>
<pre><code>## [1] -0.002982632</code></pre>
<pre class="r"><code>max(reasonable_parameter_values_B)</code></pre>
<pre><code>## [1] -0.001742652</code></pre>
<pre class="r"><code>plot(allvals_b,profile_B,type=&quot;l&quot;,main=&quot;Log Likelihood profile&quot;,xlab=&quot;Parameter Slice for \&#39;b\&#39;&quot;,ylab=&quot;Log-Likelihood&quot;)
abline(v=MLE$par[&quot;b&quot;],lwd=3,col=&quot;blue&quot;)
abline(v=min(reasonable_parameter_values_B),lwd=1,col=&quot;blue&quot;)
abline(v=max(reasonable_parameter_values_B),lwd=1,col=&quot;blue&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>So,what happens if we compare the profile likelihood confidence interval with the “slice” method we used earlier??</p>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p><strong>Why is there a difference??</strong></p>
<p>In R and other software packages you may have worked with, you will have the option to estimate the confidence interval using the ‘profile likelihood’ method. Now you know what that means!</p>
<div id="short-exercise-1" class="section level4">
<h4>Short exercise 1</h4>
<blockquote>
<p>Develop a function that returns the data likelihood (<em>likelihood function</em>) for the following scenario: you visit three known-occupied wetland sites ten times and for each site you record the number of times a particular frog species is detected within a 5 minute period. Assuming that all sites are occupied continously, compute the likelihood of these data: [3,2 and 6 detections for sites 1, 2, and 3 respectively] for a given detection probability <span class="math inline">\(p\)</span>. Assume that all sites have the same (unknown) detection probability. Using this likelihood function, answer the following questions:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>What is the maximum likelihood estimate for the p parameter?</li>
<li>Using the “rule of 2”, determine the approximate 95% confidence interval for the p parameter.</li>
</ol>
</div>
</div>
<div id="the-likelihood-ratio-test-lrt" class="section level3">
<h3>The likelihood ratio test (LRT)</h3>
<p>Q: In what ways is the likelihood surface likely to change if you were to collect more data?</p>
<p>Steeper gradients (slopes) in the likelihood surface near the maximum likelihood estimate correspond to narrower confidence intervals. But how can we determine the cutoff for where the edge of the confidence interval is?</p>
<div id="definition-likelihood-ratio" class="section level4">
<h4>Definition: Likelihood Ratio</h4>
<p>The likelihood Ratio is defined as:</p>
<p><span class="math inline">\(\frac{\mathcal{L} _{r}}{\widehat{\mathcal{L}}}\)</span></p>
<p>Where $\widehat{ $ is the likelihood at the global maximum likelihood estimate and <span class="math inline">\(\mathcal{L} _{r}\)</span> is the Likelihood of a model for which some parameters have been “fixed” (determined <em>a priori</em> and therefore no longer “free” to be estimated.</p>
</div>
<div id="definition-deviance" class="section level4">
<h4>Definition: Deviance</h4>
<p>Twice the negative log of the likelihood ratio, <span class="math inline">\(-2*ln(\frac{\mathcal{L} _{r}}{\widehat{\mathcal{L}}})\)</span>, is also known as the <em>deviance</em>!</p>
</div>
<div id="distribution-of-the-likelihood-ratio-frequentist" class="section level4">
<h4>Distribution of the Likelihood Ratio (frequentist!)</h4>
<p>For some reason (which we will not dwell on) the deviance under the null hypothesis (the full model adds no useful information with respect to the restricted model) is approximately $^2 $ (“chi-squared”) distributed with <em>r</em> degrees of freedom, where <em>r</em> is the number of dimensions by which the full model has been reduced.</p>
<p><em>r</em> is equal to the number of free parameters that have been eliminated with respect to the full model. That is, if our full model has three free parameters and we fix the value of two of these parameters, <em>r</em> is equal to 2.</p>
<p>We can use this to estimate the statistical significance of any difference in likelihood between a full model and a restricted model.</p>
<p>Here is a visualization of the chi-squared distribution with 2 degrees of freedom:</p>
<pre class="r"><code>curve(dchisq(x,2),0,10,ylab=&quot;probability density&quot;,xlab=&quot;x&quot;, main=&quot;Chi-Squared distribution, df=2&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>What is the 95% quantile of this distribution?</p>
<pre class="r"><code>curve(dchisq(x,2),0,10,ylab=&quot;probability density&quot;,xlab=&quot;x&quot;, main=&quot;Chi-Squared distribution, df=2&quot;)
abline(v=qchisq(0.95,2),col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>What about 1 df?</p>
<pre class="r"><code>curve(dchisq(x,1),0,5,ylab=&quot;probability density&quot;,xlab=&quot;x&quot;, main=&quot;Chi-Squared distribution, df=1&quot;)
abline(v=qchisq(0.95,1),col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>Okay, so now we know that the deviance, the quantity defined as <span class="math inline">\(-2*ln(\frac{\mathcal{L} _{r}}{\widehat{\mathcal{L}}})\)</span>, should be distributed according to the above probability distribution under the null hypothesis of no difference between the restricted and full model. How does this relate to confidence intervals?</p>
<p>Remember this is a frequentist test. So we are imagining multiple alternative universes where we are collecting data and determining a maximum likelihood estimate. Even though the data generating process is the same each time, each dataset we collect will yield a slightly different MLE. Now imagine we <em>fix</em> the value of one of our parameters at the <strong>true</strong> parameter value and collect thousands of datasets, each time maximizing the likelihood with respect to all the other parameters. The <em>deviance</em> between the restricted model and the full model should be chi-squared distributed!</p>
<p>That is, 95% of the time, the deviance should be below 3.84</p>
<p>So, if we want to determine a range in parameter space can plausibly contain the true parameter value, we can select the range of parameter space for which the deviance is less than or equal to 3.84.</p>
<p>$ Deviance - 3.84$<br />
<span class="math inline">\(-2*ln(\frac{\mathcal{L} _{r}}{\widehat{\mathcal{L}}}) \leq - 3.84\)</span><br />
<span class="math inline">\(-2*[ln(\mathcal{L} _{r}) - ln{\widehat{\mathcal{L}}})] \leq - 3.84\)</span><br />
<span class="math inline">\([ln(\mathcal{L} _{r}) - ln{\widehat{\mathcal{L}}})] \leq 1.92\)</span></p>
<p>So, now what do you think about the “rule of 2”? Is it close enough???</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
