---
title: "Model Selection"
author: "NRES 746"
date: "November 9, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


**Model selection** or **model comparison** is a very common problem in ecology- that is, we often have multiple competing hypotheses about how our data were generated. 

If we can describe our data generating process explicitly as a set of deterministic and stochastic componenets (likelihood function), then we should be able to use Likelihood-based methods (e.g., LRT, AIC, BIC, Bayesian model selection) to infer which data generating model(s) could most plausibly have generated our observed data. 

## Principle of Parsimony

We will discuss several alternative approaches to model selection in ecology. However, all approaches follow a basic principle- that -- all things equal, we should prefer the simpler model over any more complex alternative. This is known as the principle of parsimony.

### Example data: Balsam fir data from NY

Bolker uses a study of balsam fir in New York to illustrate model selection. Perhaps it's time to move on from Myxomatosis!

Let's load up the data first

```{r}
library(emdbook)
data(FirDBHFec)
fir <- na.omit(FirDBHFec[,c("TOTCONES","DBH","WAVE_NON")])
fir$TOTCONES <- round(fir$TOTCONES)
head(fir)
```

We can examine the fecundity (total cones) as a function of the tree size (DBH):

```{r}
plot(fir$TOTCONES ~ fir$DBH)
```

One additional point of complexity in this data set- some trees were sampled from areas that have undergone periodic wave-like die-offs. Other trees were sampled from areas that have not undergone die-offs. 

```{r}
ndx <- fir$WAVE_NON=="w"   # logical vector indicating which observations were from "wave" sites
plot(fir$TOTCONES[ndx] ~ fir$DBH[ndx],xlab="DBH",ylab="Tot Cones")
points(fir$DBH[!ndx],fir$TOTCONES[!ndx],pch=4,col="red")
legend("topleft",pch=c(1,4),col=c("black","red"),legend=c("Wave","Non-wave"),bty="n")
```

Let's assume (following Bolker) that fecundity increases as a power-law relationship with DBH:

$\mu = a\cdot DBH^{b}$

Let's also assume that the fecundity follows a negative binomial distribution:

$Y = NegBin(\mu,k)$

We can model each of these parameters (*a*, *mu*, and *k*) separately for trees from wave and nonwave populations.

We can also run simpler models in which these parameters are modeled as the same for both populations.

Then we can ask the question: **which model is the "best model"?**

#### FULL MODEL

Here is a likelihood function for the *full model* -- that is, the most complex model (6-dimensional likelihood surface):

```{r}
NegBinomLik_full <- function(params){
  wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
  a <- c(params[1],params[2])[wave.code]     # a parameters
  b <- c(params[3],params[4])[wave.code]      # b parameter (not a function of wave/nonwave)
  k <- c(params[5],params[6])[wave.code]       # dispersion parameters
  expcones <- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params <- c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1)

NegBinomLik_full(params)
```


We can fit the full model using "optim" (using a quasi-newton optimization routine), just like we have done before:

```{r}

MLE_full <- optim(fn=NegBinomLik_full,par=c(a.n=1,a.w=1,b.n=1,b.w=1,k.n=1,k.w=1),method="L-BFGS-B")

MLE_full$par

MLE_full$value

```


#### REDUCED MODELS

Let's run a simpler model now. This time, let's model the b parameter as equal for wave and nonwave population:


```{r}
NegBinomLik_constb <- function(params){
  wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
  a <- c(params[1],params[2])[wave.code]      # a parameters
  b <- params[3]                              # b parameter (not a function of wave/nonwave)
  k <- c(params[4],params[5])[wave.code]      # dispersion parameters
  expcones <- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params <- c(a.n=1,a.w=1,b=1,k.n=1,k.w=1)

NegBinomLik_constb(params)
```


And we can fit the full model using "optim":

```{r}

MLE_constb <- optim(fn=NegBinomLik_constb,par=c(a.n=1,a.w=1,b=1,k.n=1,k.w=1),method="L-BFGS-B")

MLE_constb$par

MLE_constb$value

```


Let's compute the *deviances* of the two models. Recall that deviance is defined as $-2*log(likelihood)$

```{r}
deviance_full <- 2*MLE_full$value

deviance_constb <- 2*MLE_constb$value

deviance_full
deviance_constb
```

Note here that the deviance of the full model is lower than the deviance from the reduced model. This should always be the case- if not, something is wrong. That is, the degree to which the fitted model matches the data should *always* improve when more parameters are added! This is where the principle of parsimony comes into play! 
What if we wanted to test which model was better supported by the data. One way is to use our old friend, the Likelihood Ratio Test (LRT)!

### Likelihood-ratio test

We have encountered the LRT once before, in the context of generating confidence intervals from likelihood surfaces (at and near the MLE). The same principle applies for model selection.  The LRT tests whether the extra goodness-of-fit is worth the extra complexity of the additional parameters.

As you recall, the likelihood ratio is defined (obviously) as:

$\frac{Likelihood_{reduced}}{Likelihood_{full}}$

Because the raw likelihood ratio under the null hypothesis does not have a known distribution, we first convert the likelihood ratio:

$-2ln(\frac{Likelihood_{reduced}}{Likelihood_{full}})$

Which can also be written as:

$-2ln(Likelidhood_{reduced}) - -2ln(Likelihood_{full})$

or 

$ Deviance_{reduced} - Deviance_{full}$  

When expressed as a difference of deviances, the likelihood ratio (asymptotically) should be chi-square distributed with df=number of fixed dimensions
 
The LRT can be used for two-way model comparison as long as one model is nested within the other (full model vs reduced model). If the models are not nested then the LRT doesn't really make sense.

```{r}
Deviance.dif <- deviance_constb - deviance_full 
Deviance.dif

Chisq.crit <- qchisq(0.95,1)
Chisq.crit

Deviance.dif>=Chisq.crit   # perform the LRT

1-pchisq(Deviance.dif,1)   # p-value
```

```{r}
curve(dchisq(x,df=1),0,5)
abline(v=Deviance.dif,col="red",lwd=4)
```

Clearly, the deviance gain is not worth the extra complexity in this case (this deviance gain could easily be produced by random chance!) Therefore, we favor the reduced model. 

What about if we try a different reduced model. This time, we decide to fix the a, b, and k parameters, so the "wave" factor is not considered.

```{r}
NegBinomLik_nowave <- function(params){
  a <- params[1]      # a parameters
  b <- params[2]      # b parameter (not a function of wave/nonwave)
  k <- params[3]      # dispersion parameters
  expcones <- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params <- c(a=1,b=1,k=1)

NegBinomLik_nowave(params)
```


And we use "optim" to locate the maximum likelihood estimate:

```{r}

MLE_nowave <- optim(fn=NegBinomLik_nowave,par=params,method="L-BFGS-B")

MLE_nowave$par

MLE_nowave$value

```


Now we can perform a LRT to see which model is better!

```{r}
deviance_full <- 2*MLE_full$value

deviance_nowave <- 2*MLE_nowave$value

Deviance.dif <- deviance_nowave - deviance_full 
Deviance.dif

Chisq.crit <- qchisq(0.95,df=3)   # now three additional params in the more complex model!
Chisq.crit

Deviance.dif>=Chisq.crit

1-pchisq(Deviance.dif,df=3)   # p-value

```


```{r}
curve(dchisq(x,df=3),0,15)
abline(v=Deviance.dif,col="red",lwd=4)
```


Again, the difference in deviance does not justify the additional parameters. This difference in deviance between the full and restricted model could be produced easily by random chance. 

Remember *this is a frequentist test*. The null hypothesis is that there is no difference between the restricted model and the more complex model. So we are imagining multiple alternative universes where we are collecting data and determining a maximum likelihood estimate. Even though the data generating process is the same each time, each dataset we collect will yield a slightly different MLE. Now imagine we *fix* the value of one or more of our parameters at the **true** parameter value and collect thousands of datasets, each time maximizing the likelihood with respect to all the other parameters. The *deviance* between the restricted model and the full model should be chi-squared distributed with df = number of dimensions that were "fixed"!

As you can imagine, there are a lot of pairwise comparisons that could be generated, even in this simple example. For instance, there are 15 pairwise comparisons that could be produced from even this simple example. What about more complex models? Clearly this can get a bit unwieldy!

In addition, not all models we wish to compare will necessarily be nested. For example, consider the model selection exercise we were performing in lab- comparing the M-M fit to the Ricker fit...

### Information-theoretic metrics

Information-theoretic metrics for model comparison, like AIC, provide a way to get around the issues with LRT. These metrics allow us to make tables for comparing multiple models at once. However, these metrics *have no frequentist interpretation*. 
 
Metrics like AIC represent (theoretically) the distance between some particular model and the "true" model. Information-theoretic metrics are composed of a likelihood component (e.g., -2Ln(L)) and a *penalty term*. For AIC, the likelihood component is the *deviance* ($-2*logL$) and the penalty term is twice the number of parameters. 

#### Akaike Information Criterion (AIC) 

AIC is computed using the following equation:  

  $AIC = -2L + 2k$

AIC is the most commonly used information criterion. 

L is the log-likelhood at the MLE

k is the number of parameters in the model

As with all information-theoretic metrics, we look for the model associated with the minimum AIC. This is the "best model"! So simple!

For small sample sizes, Burnham and Anderson (2002) recommend that a finite-size correction should be used:

  $AIC_c = AIC + \frac{2k(k+1)}{n-k-1}$
 

A *rule of thumb* is that models within 2 AIC units of the best model are "reasonable".

However, some statisticians caution that models within 7 AIC units of the best model can be useful and may warrant further consideration!


#### Schwarz information criterion (BIC)

Another common I-T metric is the Schwarz, or *Bayesian* information criterion. The penalty term for BIC is (log n)*k.   

$BIC = -2logL + (log(n))\cdot k$

In general, BIC is more conservative than AIC- that is, more likely to select the simpler model (since the penalty term is generally greater)


#### AIC in action

Let's return to the fir fecundity model, and use AIC to select among a set of models. Let's first fit a couple more candidate models...

This time, we decide to fix the *a*, and *k* parameters, so the "wave" factor is only considered for the *b* parameter.

```{r}
NegBinomLik_constak <- function(params){
  wave.code <- as.numeric(fir$WAVE_NON)      # convert to ones and twos
  a <- params[1]                             # a parameters
  b <- c(params[2],params[3])[wave.code]                              # b parameter (not a function of wave/nonwave)
  k <- params[4]                               # dispersion parameters
  expcones <- a*fir$DBH^b
  -sum(dnbinom(fir$TOTCONES,mu=expcones,size=k,log=TRUE))
}

params <- c(a=1,b.n=1,b.w=1,k=1)  

NegBinomLik_constak(params)
```


And we can fit the full model using "optim":

```{r}

MLE_constak <- optim(fn=NegBinomLik_constak,par=params)

MLE_constak$par

MLE_constak$value

deviance_constak <- 2*MLE_constak$value

```


Finally, let's fit a model with no "wave" effect, but where we assume the error is Poisson distributed...

```{r}
PoisLik_nowave <- function(params){
  a <- params[1]      # a parameters
  b <- params[2]      # b parameter (not a function of wave/nonwave)
  expcones <- a*fir$DBH^b
  -sum(dpois(fir$TOTCONES,lambda=expcones,log=TRUE))
}

params <- c(a=1,b=1)

PoisLik_nowave(params)

MLE_pois <- optim(fn=PoisLik_nowave,par=params)

MLE_pois$par

MLE_pois$value

deviance_pois <- 2*MLE_pois$value
```

**Note** we could not compare the Poisson model to the Negative Binomial model using LRT- one is not nested within the other!

Now we can compare the five models we have run so far using AIC

```{r}
AIC_constak <- deviance_constak + 2*4
AIC_full <- deviance_full + 2*6
AIC_constb <- deviance_constb + 2*5
AIC_nowave <- deviance_nowave + 2*3
AIC_pois <- deviance_pois + 2*2

AICtable <- data.frame(
  Model = c("Full","Constant b","Constant a and k","All constant","Poisson"),
  AIC = c(AIC_full,AIC_constb,AIC_constak,AIC_nowave,AIC_pois),
  Deviance = c(deviance_full,deviance_constb,deviance_constak,deviance_nowave,deviance_pois),
  params = c(6,5,4,3,2),
  stringsAsFactors = F
)

AICtable$DeltaAIC <- AICtable$AIC-AICtable$AIC[which.min(AICtable$AIC)]

AICtable$Weights <- round(exp(-0.5*AICtable$DeltaAIC) / sum(exp(-0.5*AICtable$DeltaAIC)),3)

AICtable$AICc <- AICtable$AIC + ((2*AICtable$params)*(AICtable$params+1))/(nrow(fir)-AICtable$params-1)

AICtable[order(AICtable$AIC),c(1,7,2,5,6,4,3)]
```

This AIC table shows us that the simplest model is best! Despite the fact that the deviance is lowest for the full model! (principle of parsimony at work) 

### Bayes Factor

Can we do model selection in a Bayesian framework? The answer is yes! Unfortunately it is not usually as straightforward as using I-T metrics...

Note that BIC is no more Bayesian than AIC. Bayesians generally do not use BIC for model selection... One metric that *is* used by Bayesians for model selection is the *Bayes Factor*. The Bayes factor is defined as the ratio of *marginal likelihoods*. 

Recall that our I-T metrics, as well as likelihood ratio tests, used the value of the likelihood surface at the MLE. That is, we are only taking into account a single point on the likelihood surface to represent what our data have to say about our model.

Bayesians prefer to take into account the entire likelihood surface rather than just a single point. The *marginal likelihood* represents the mean of the likelihood across parameter space, averaged over the prior distribution. 

$\overline{\mathcal{L}} = \int \mathcal{L}(x)\cdot Prior(x) dx$

The marginal likelihood represents the average probability of the data across parameter space, or the *average quality of fit of a given model to the data*. The ratio of marginal likelihoods is known as the **Bayes factor**

$\overline{\mathcal{L}}_1 / \overline{\mathcal{L}}_2$ 

This is interpreted as *the odds in favor of model 1 over model 2*

This simple formula naturally accounts for over-parameterization. Simpler models will generally have higher marginal likelihoods than more complex models. We have already seen why this might be. More complex models will always have a higher likelihood at the MLE, but generally will have much lower likelihoods in other parts of parameter space. A higher marginal likelihood means that a model fits the data better even after taking all of parameter space into account.

Interestingly, 2*logarithm of the Bayes factor (putting it on the deviance scale) is comparable to AIC (with a fairly strong prior) and BIC (with a fairly weak prior).

In practice, computing marginal likelihoods can be tricky, involving multidimensional integration!


#### Bayes Factor Example

A simple binomial distribution example can illustrate Bayes factors quite nicely

Imagine we conduct a tadpole experiment where we are interested in estimating the mortality rate of some treatment, on the basis of the number of dead tadpoles observed out of 10 in a tank. We are interested in comparing a simple model where *p* is fixed at 0.5 with a more complex model where *p* is a *free parameter*.  

First let's look at the simple model. 

```{r}
probs1 <- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab="probability")
```

**Q** What is the *marginal likelihood* under the simple model for an observation of 2 mortalities? How about 3 mortalities?

**A** It is exactly `r dbinom(2,10,0.5)`. That is, there is no marginalizing to do since there is no free parameter to marginalize over. The likelihood is the likelihood is ... well you get it!

Now let's consider a 'complex' model, where *p* is a free parameter (one additional free parameter relative to the simple model). First, let's assume that *p* is assigned a uniform $beta(1,1)$ prior across parameter space:

```{r}
curve(dbeta(x,1,1))
```

What is the marginal likelihood of observing any particular number of mortalities from 0 to 10? For example, what is the average probability of observing a 2 exactly, across all possible values of p??? 

Intuitively, because all values of *p* are equally likely, all possible observations should also be equally likely! That is, neither the likelihood function nor the prior distribution favors any particular observation (0 to 10) over any other.   

We can do this mathematically...

For two observed mortalities, the marginal likelihood is: 

```{r}

# ?integrate
binom2 <- function(x) dbinom(x=2,size=10,prob=x)
marginal_likelihood <- integrate(f=binom2,0,1)$value    # use "integrate" function
marginal_likelihood

```

For three observed mortalities, the marginal likelihood (probability of observing a "3" across all possible values of p) is:

```{r}
binom3 <- function(x) dbinom(x=3,size=10,prob=x)
marginal_likelihood <- integrate(f=binom3,0,1)$value    # use "integrate" function
marginal_likelihood
```

Basically, the way our model is written, no particular observation is favored over any other:

```{r}
lots=100000
hist(rbinom(lots,10,prob=rbeta(lots,1,1)),freq = F,xlab="potential observations")   # no particular observation is favored
```

Therefore, since there are 11 possibilities, the marginal likelihood of any particular data observation under the model with p as a free parameter should be 1/11 = `r 1/11`.

Here is a visualization of the marginal likelihood across all possible data observations:

```{r}
probs2 <- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))
```

Now let's overlay the marginal likelihoods for the simpler model:

```{r}

probs2 <- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))

probs1 <- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab="probability",add=T,col="red",density=20)
```

Assuming we observed 2 mortalities, what is the Bayes Factor? Which model is better?

```{r}

probs2 <- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))

probs1 <- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab="probability",add=T,col="red",density=20)

abline(v=3,col="green",lwd=4 )
```


```{r}
BayesFactor = (1/11)/dbinom(2,10,0.5)
BayesFactor
```

What if the data instead were 3 mortalities? Which model is the best model?

```{r}

probs2 <- rep(1/11,times=11)          
names(probs2) = 0:10
barplot(probs2,ylab="probability",ylim=c(0,1))

probs1 <- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab="probability",add=T,col="red",density=20)

abline(v=4.3,col="green",lwd=4 )
```


```{r}
BayesFactor = dbinom(3,10,0.5)/(1/11)
BayesFactor
```

*The simpler model wins*!!! That would never happen if we compared maximum likelihood estimates- the deviance of the (fitted) more highly-parameterized model will always be lower! That's why we have to penalize or *regularize* more highly-parameterized models.

We can visualize this! First of all, what is the maximum likelihood estimate for p under the model with 3 mortality observations?

```{r}

# probs2 <- rep(1/11,times=11)          
# names(probs2) = 0:10
# barplot(probs2,ylab="probability",ylim=c(0,1))

probs1 <- dbinom(0:10,10,0.5)          
names(probs1) = 0:10
barplot(probs1,ylab="probability",col="red",density=20,ylim=c(0,1))

probs3 <- dbinom(0:10,10,0.3)          
names(probs3) = 0:10
barplot(probs3,ylab="probability",add=T,col="green",density=10,angle = -25)

abline(v=4.3,col="green",lwd=4 )
```

So clearly the likelihood ratio favors the more complex model (fitted) vs the simple, 0-parameter model!

What does the likelihood ratio test say?

```{r}
Likelihood_simple <- dbinom(3,10,0.5)
Likelihood_complex <- dbinom(3,10,0.3)
Likelihood_simple
Likelihood_complex
-2*log(Likelihood_simple)--2*log(Likelihood_complex)

qchisq(0.95,1)


```

What about AIC?

```{r}

AIC_simple <- -2*log(Likelihood_simple) + 2*0
AIC_complex <-  -2*log(Likelihood_complex) + 2*1

AIC_simple
AIC_complex    


```

What about AICc?

$AIC_c = AIC + \frac{2k(k+1)}{n-k-1}$


```{r}

AICc_simple <- -2*log(Likelihood_simple) + 0 + 0
AICc_complex <-  -2*log(Likelihood_complex) + 1 + ((2*2)/(3-1-1))

AICc_simple
AICc_complex    

```


What about BIC?

```{r}

BIC_simple <- -2*log(Likelihood_simple) + 0
BIC_complex <-  -2*log(Likelihood_complex) + log(3)*1

BIC_simple
BIC_complex    


```

All these methods give the same basic answer- the simple model is better, even though the complex model fits better!

**Q** Does it make sense that the principle of parsimony is naturally incorporated in the Bayes factor? That is, we don't need to impose a penalty term- it is there already!!!


### Deviance Information Criterion (DIC)

DIC is computed by default in JAGS and WinBUGS. And it is analogous to other I-T metrics like AIC (and therefore easy to interpret and use)!

... but it is generally not reliable and I would not recommend using it in most cases, especially for complex hierarchical models... 


Let's run an example anyway...


First, let's write a BUGS model for the fir data

```{r}
cat("

model  {

### Likelihood

  for(i in 1:n.obs){
    expected.cones[i] <- a[wave[i]]*pow(DBH[i],b[wave[i]])   # power function: a*DBH^b
    p[i] <- r[wave[i]] / (r[wave[i]] + expected.cones[i])
    observed.cones[i] ~ dnegbin(p[i],r[wave[i]])
  }
  
  
  ### Priors
  for(j in 1:2){   # estimate separately for wave and non-wave
    a[j] ~ dunif(0.001,2)
    b[j] ~ dunif(0.5,4)
    r[j] ~ dunif(0.5,5)
  }
  
}
    
",file="BUGS_fir.txt")

```

Then we need to package the data for JAGS

```{r}
data.package <- list(
  observed.cones = fir$TOTCONES,
  n.obs = nrow(fir),
  wave = as.numeric(fir$WAVE_NON),
  DBH = fir$DBH
)
#data.package
```


Now we make a function for generating initial values:

```{r}
init.generator <- function(){ list(
  a = runif(2, 0.2,0.5),
  b = runif(2, 2,3),
  r = runif(2, 1,2)
  
  )
}
init.generator()
```

Then we can run the model!

```{r}
library(R2jags)    # load packages
library(coda)
library(lattice)

params.to.monitor <- c("a","b","r")

jags.fit <- jags(data=data.package,inits=init.generator,parameters.to.save=params.to.monitor,n.iter=10000,model.file="BUGS_fir.txt",n.chains = 2,n.burnin = 2000,n.thin=5 )

jagsfit.mcmc <- as.mcmc(jags.fit)   # convert to "MCMC" object (coda package)

summary(jagsfit.mcmc)

plot(jagsfit.mcmc)


```


```{r}
densityplot(jagsfit.mcmc)
```


First of all, is there any evidence that the dispersion of cone data from wave sites is different than that of the non-wave sites?


```{r}
hist(jags.fit$BUGSoutput$sims.list$r[,1],main="dispersion param",ylab="Prob Density",xlab="dispersion param",freq = F,ylim=c(0,2),xlim=c(0.5,2.5))
hist(jags.fit$BUGSoutput$sims.list$r[,2],density=20,col="green",add=T,freq=F)
legend("topright",col=c("green","white"),density=c(20,0),legend=c("wave","nonwave"),bty="n")

```



What is the DIC for this model?

```{r}
DIC_full <- jags.fit$BUGSoutput$DIC
DIC_full

```



Now let's build the reduced model and compare DIC values!


```{r}
cat("

model  {

### Likelihood

  for(i in 1:n.obs){
    expected.cones[i] <- a*pow(DBH[i],b)   # a*DBH^b
    p[i] <- r / (r + expected.cones[i])
    observed.cones[i] ~ dnegbin(p[i],r)
  }
  
  
  ### Priors
  
  a ~ dunif(0.001,2)
  b ~ dunif(0.5,4)
  r ~ dunif(0.5,5)

  
}
    
",file="BUGS_fir_reduced.txt")

```

Then we need to package the data for JAGS

```{r}
data.package <- list(
  observed.cones = fir$TOTCONES,
  n.obs = nrow(fir),
  #wave = as.numeric(fir$WAVE_NON),
  DBH = fir$DBH
)
#data.package
```


Now we make a function for generating initial values:

```{r}
init.generator <- function(){ list(
  a = runif(1, 0.2,0.5),
  b = runif(1, 2,3),
  r = runif(1, 1,2)
  
  )
}
init.generator()
```

Then we can run the model!

```{r}

params.to.monitor <- c("a","b","r")

jags.fit <- jags(data=data.package,inits=init.generator,parameters.to.save=params.to.monitor,n.iter=10000,model.file="BUGS_fir_reduced.txt",n.chains = 2,n.burnin = 2000,n.thin=5 )

jagsfit.mcmc <- as.mcmc(jags.fit)   # convert to "MCMC" object (coda package)

summary(jagsfit.mcmc)

plot(jagsfit.mcmc[,"a"])
plot(jagsfit.mcmc[,"b"])
plot(jagsfit.mcmc[,"r"])


```


```{r}
densityplot(jagsfit.mcmc)
```


What is the DIC for this model?

```{r}
DIC_reduced <- jags.fit$BUGSoutput$DIC

DIC_reduced
DIC_full

```

Is there a good reason to prefer the full model now? 

What would happen if I re-ran the model? Would the DIC be the same?

What would happen if I changed the priors? Would the DIC be the same?

What about AIC? Is the AIC the same every time?


### Explicit Bayesian model selection

Another cool, sometimes useful, but not perfect, method of Bayesian model selection is to write the model selection directly into the JAGS code!


```{r}
cat("

model  {

  ### Likelihood for model 1: full

  for(i in 1:n.obs){
    expected.cones[i,1] <- a1[wave[i]]*pow(DBH[i],b1[wave[i]])       # a*DBH^b
    spread.cones[i,1] <- r1[wave[i]]
    p[i,1] <- spread.cones[i,1] / (spread.cones[i,1] + expected.cones[i,1])
    observed.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
    predicted.cones[i,1] ~ dnegbin(p[i,1],spread.cones[i,1])
    SE_obs[i,1] <- pow(observed.cones[i,1]-expected.cones[i,1],2)
    SE_pred[i,1] <- pow(predicted.cones[i,1]-expected.cones[i,1],2)
  }
  
  
  ### Priors, model 1
  for(j in 1:2){   # estimate separately for wave and non-wave
    a1[j] ~ dunif(0.001,2)
    b1[j] ~ dunif(0.5,4)
    r1[j] ~ dunif(0.5,5)
  }

  ### Likelihood for model 2: reduced

  for(i in 1:n.obs){
    expected.cones[i,2] <- a2*pow(DBH[i],b2)       # a*DBH^b
    spread.cones[i,2] <- r2
    p[i,2] <- spread.cones[i,2] / (spread.cones[i,2] + expected.cones[i,2])
    observed.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
    predicted.cones[i,2] ~ dnegbin(p[i,2],spread.cones[i,2])
    SE_obs[i,2] <- pow(observed.cones[i,2]-expected.cones[i,2],2)
    SE_pred[i,2] <- pow(predicted.cones[i,2]-expected.cones[i,2],2)
  }
  
  
  ### Priors, model 2
  a2 ~ dunif(0.001,2)
  b2 ~ dunif(0.5,4)
  r2 ~ dunif(0.5,5)

  ### Likelihood for model 3: constant a and b

  for(i in 1:n.obs){
    expected.cones[i,3] <- a3*pow(DBH[i],b3)       # a*DBH^b
    spread.cones[i,3] <- r3[wave[i]]
    p[i,3] <- spread.cones[i,3] / (spread.cones[i,3] + expected.cones[i,3])
    observed.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
    predicted.cones[i,3] ~ dnegbin(p[i,3],spread.cones[i,3])
    SE_obs[i,3] <- pow(observed.cones[i,3]-expected.cones[i,3],2)
    SE_pred[i,3] <- pow(predicted.cones[i,3]-expected.cones[i,3],2)
  }
  
  SSE_obs[1] <- sum(SE_obs[,1]) 
  SSE_pred[1] <- sum(SE_pred[,1])
  SSE_obs[2] <- sum(SE_obs[,2]) 
  SSE_pred[2] <- sum(SE_pred[,2])
  SSE_obs[3] <- sum(SE_obs[,3]) 
  SSE_pred[3] <- sum(SE_pred[,3])

  ### Priors, model 3
  for(j in 1:2){   # estimate separately for wave and non-wave
    r3[j] ~ dunif(0.5,5)
  }
  a3 ~ dunif(0.001,2)
  b3 ~ dunif(0.5,4)

  #####################
  ### SELECT THE BEST MODEL!!! 
  #####################

  for(i in 1:n.obs){
    observed.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])
    predicted.cones2[i] ~ dnegbin(p[i,selected],spread.cones[i,selected])     # for posterior predictive check!
    SE2_obs[i] <- pow(observed.cones2[i]-expected.cones[i,selected],2)
    SE2_pred[i] <- pow(predicted.cones2[i]-expected.cones[i,selected],2)
  }
  
  SSE2_obs <- sum(SE2_obs[])
  SSE2_pred <- sum(SE2_pred[])


  ### Priors
  
    # model selection...
  prior[1] <- 1/3
  prior[2] <- 1/3     # put substantially more weight because fewer parameters (there are more rigorous ways to do this!!)
  prior[3] <- 1/3
  selected ~ dcat(prior[])   
  
  
}
    
",file="BUGS_fir_modelselection.txt")

```


Now we can use MCMC to find which model we put our beliefs in after we account for our data!

Then we need to package the data for JAGS

```{r}
data.package <- list(
  observed.cones = matrix(rep(fir$TOTCONES,times=3),ncol=3,byrow=F),
  observed.cones2 = fir$TOTCONES,
  n.obs = nrow(fir),
  wave = as.numeric(fir$WAVE_NON),
  #n.models = 3,
  DBH = fir$DBH
)
#data.package
```

Then we can run the model!

```{r}

params.to.monitor <- c("a1","b1","r1","a2","b2","r2","a3","b3","r3","selected","predicted.cones2","predicted.cones","SSE_obs","SSE_pred","SSE2_obs","SSE2_pred")

jags.fit <- jags(data=data.package,parameters.to.save=params.to.monitor,n.iter=5000,model.file="BUGS_fir_modelselection.txt",n.chains = 2,n.burnin = 1000,n.thin=2 )

jagsfit.mcmc <- as.mcmc(jags.fit)   # convert to "MCMC" object (coda package)

BUGSlist <- as.data.frame(jags.fit$BUGSoutput$sims.list)
#summary(jagsfit.mcmc)

#plot(jagsfit.mcmc)


```


```{r}
#plot(jagsfit.mcmc[,"selected"])

plot(jagsfit.mcmc[,"a1[1]"])
plot(jagsfit.mcmc[,"a1[2]"])
plot(jagsfit.mcmc[,"a2"])
plot(jagsfit.mcmc[,"a3"])

plot(jagsfit.mcmc[,"r1[1]"])
plot(jagsfit.mcmc[,"r1[2]"])
plot(jagsfit.mcmc[,"r2"])
plot(jagsfit.mcmc[,"r3[1]"])


```


Let's look at the model selection (that's the whole point!!)

```{r}
n.iterations <- length(jags.fit$BUGSoutput$sims.list$selected)
selected <- table(jags.fit$BUGSoutput$sims.list$selected)
names(selected) <- c("Full model","No wave","Fixed a&b")
selected

barplot(selected/n.iterations,ylab="Degree of belief")

```


#### Evaluate model fit

Now we can look at model fit! We will use the same method we used in lab- the posterior predictive check... 

We can write a for loop to extract the prediction results for each model (and for the model-averaged model):

First let's just look at the model predictions vs the observed data. First, model 1

```{r}

n.data <- length(fir$DBH)

plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind <- sprintf("predicted.cones[%s,1]",d)
  model1 <- as.vector(jagsfit.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col="gray",cex=0.4)
}

```


First let's just look at the model predictions vs the observed data. Next, model 2 (reduced)

```{r}

plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind <- sprintf("predicted.cones[%s,2]",d)
  model1 <- as.vector(jagsfit.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col="gray",cex=0.4)
}

```

And model 3 (fixed a and b):

```{r}
plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind <- sprintf("predicted.cones[%s,3]",d)
  model1 <- as.vector(jagsfit.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col="gray",cex=0.4)
}
```


Clearly all the models seem to fit okay... But, it seems like there is more prediction error than is necessary... Let's run a posterior predictive check!

For model 1:

```{r}
plot(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[1]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[1]"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE_pred[1]"][[1]]))
p.value 
```

```{r}
plot(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[2]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[2]"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE_pred[2]"][[1]]))
p.value 
```

```{r}
plot(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])~as.vector(jagsfit.mcmc[,"SSE_obs[3]"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]])>as.vector(jagsfit.mcmc[,"SSE_obs[3]"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE_pred[3]"][[1]]))
p.value                                                                                                                    
```


Interesting- the model seems to not fit very well! 


Okay that's it for model selection, now let's move on to:

### Model averaging

The fact that model selection is such a big deal in Ecology indicates that we are rarely certain about which model is the best model. Even after constructing an AIC table we may be very unsure about which model is the "true" model. 

The AIC weights tell us in essence how much we "believe" in each model. This is a very Bayesian interpretation, but model averaging really is best thought of in a Bayesian context. 

One way to do model averaging relies on AIC weights. Basically we take the set of predictions from each model independently and weight them by the Akaike weight. There is a literature on this and R packages for helping (see package ['AICcmodavg'](https://cran.r-project.org/web/packages/AICcmodavg/AICcmodavg.pdf))

#### When should you use model-averaged parameter estimates

NEVER!

#### The Bayesian version!

We can use the results from the JAGS code above to easily generate Bayesian model-averaged predictions! JAGS makes it relatively simple and straightforward to do model averaging in a Bayesian context!

Look at the predictions for the model averaged model: 


```{r}

plot(fir$TOTCONES~fir$DBH,ylim=c(0,900),cex=2)

for(d in 1:n.data){
  tofind <- sprintf("predicted.cones2[%s]",d)
  model1 <- as.vector(jagsfit.mcmc[,tofind])
  points(rep(fir$DBH[d],times=100),sample(model1[[1]],100),pch=20,col="gray",cex=0.4)
}

```

Note that these predictions naturally incorporate both parameter uncertainty and structural (model selection) uncertainty!


We can do a posterior predictive check with the model-averaged model!

```{r}
plot(as.vector(jagsfit.mcmc[,"SSE2_pred"][[1]])~as.vector(jagsfit.mcmc[,"SSE2_obs"][[1]]),xlab="SSE, real data",ylab="SSE, perfect data",main="Posterior Predictive Check")
abline(0,1,col="red")
p.value=length(which(as.vector(jagsfit.mcmc[,"SSE2_pred"][[1]])>as.vector(jagsfit.mcmc[,"SSE2_obs"][[1]])))/length(as.vector(jagsfit.mcmc[,"SSE2_pred"][[1]]))
p.value 
```


```{r echo=FALSE, eval=FALSE}
plot(jagsfit.mcmc[,"predicted.cones[1,1]"])  # e.g., first observation from model 1


```









































