<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />


<title>Model Performance Evaluation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Model Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian inference</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
    <li>
      <a href="uta_simulated_data.csv">Uta data</a>
    </li>
    <li>
      <a href="tide_ALL_navd_HH.csv">tide data?</a>
    </li>
    <li>
      <a href="TestDataset2.csv">Ordination minilab: testdataset2</a>
    </li>
    <li>
      <a href="TestHypotheses.csv">Ordination minilab: testhypotheses</a>
    </li>
    <li>
      <a href="PRISM_ppt_1895-2015Mo2.csv">PRISM data for time series lab</a>
    </li>
    <li>
      <a href="final_winter_modeldata2.csv">winter deer data for RSF lab</a>
    </li>
    <li>
      <a href="keeley_rawdata.csv">Keeley data</a>
    </li>
    <li>
      <a href="Nest_basic_ALL.csv">nest basic data</a>
    </li>
    <li>
      <a href="Cap_data_for SA minilab.csv">cap data for SA</a>
    </li>
    <li>
      <a href="JoTrPresence02202008_dryad.txt">Joshua tree data for SDM lab</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="forWebsite_SEM.html">SEMs</a>
    </li>
    <li>
      <a href="SEM_Minilab_v2.html">SEM mini-lab</a>
    </li>
    <li>
      <a href="GAMs.html">GAMs</a>
    </li>
    <li>
      <a href="RMarkdown_FigureDemo.html">Publication-quality figures in R</a>
    </li>
    <li>
      <a href="Bayesian Networks.pptx">Bayesian Networks</a>
    </li>
    <li>
      <a href="Bayes_Network_Markdown_Final.html">Bayesian Networks mini-lab</a>
    </li>
    <li>
      <a href="GraphTheory.html">Graph Theory</a>
    </li>
    <li>
      <a href="NRES746_IPMs.pptx">Integrated Population Models</a>
    </li>
    <li>
      <a href="TimeSeries_heckler.html">Time Series Analysis</a>
    </li>
    <li>
      <a href="Time_Series_Lab.html">Time-series mini-lab</a>
    </li>
    <li>
      <a href="Spatial_Autocorrelation.html">Spatial Autocorrelation</a>
    </li>
    <li>
      <a href="SA_minilab.html">Spatial Autocorrelation mini-lab</a>
    </li>
    <li>
      <a href="SDM_pres.html">Species Distribution Modeling</a>
    </li>
    <li>
      <a href="IPM_miniLab.html">IPM mini-lab</a>
    </li>
    <li>
      <a href="Final_MiniLabScript.html">RSF mini-lab</a>
    </li>
    <li>
      <a href="MixedModelMinilab.html">Mixed-effects model mini-lab</a>
    </li>
    <li>
      <a href="Minilab_Ordination.html">Ordination mini-lab</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Model Performance Evaluation</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>November 9, 2016</em></h4>

</div>


<p>In this course so far, we have constructed data-generating models and fitted these models to observed data using likelihood-based methods (ML and Bayesian inference). We also have explored a range of methods to account for <em>structural uncertainty</em> (which of a set of candidate models could plausibly have generated our observed data).</p>
<p>But even after we have fitted a model to data, even after we have compared a suite of plausible models and selected the best one, are we really sure that the model is <em>good</em>?</p>
<p>What does it even mean to say that the model is good?</p>
<p>Usually, we mean one or more of the following:</p>
<ol style="list-style-type: decimal">
<li><strong>Goodness-of-fit</strong>: The data could easily/reasonably have been generated under the best-fit model.</li>
<li><strong>Predictive ability</strong>: The model performs well at predicting observed responses that were not used to train the model.</li>
<li><strong>Generality</strong>: The model performs well at extrapolating responses for data that are qualitatively different from those in the training set.</li>
</ol>
<div id="goodness-of-fit" class="section level2">
<h2>Goodness-of-fit</h2>
<p>We have already looked at a variety of methods for evaluating goodness-of-fit. In general, you can use data simulation (virtual ecology) to evaluate whether your fitted model is capable of generating the observed data.</p>
<p>For example, we can overlay the observed data on a cloud of points representing the range of data sets possibly produced under the fitted model. Or, we can use a “plug-in” prediction bounds as a substitute for the cloud of data sets possibly produced under the model.</p>
<p>Let’s return to the Myxomatosis dataset!</p>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)  #Data set from grade 1 of myxo data
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>Now let’s use ML to fit the Ricker model with a gamma error distribution.</p>
<pre class="r"><code>Ricker &lt;- function(a,b,predvar) a*predvar*exp(-b*predvar)
  
NegLogLik_func &lt;- function(params,data){
  expected &lt;- Ricker(params[1],params[2],data$day)
  -sum(dgamma(data$titer,shape=params[3],scale=expected/params[3],log = T))
}

init.params &lt;- c(a=1,b=0.2,shape=50)
NegLogLik_func(init.params,data=Myx)</code></pre>
<pre><code>## [1] 2336.22</code></pre>
<pre class="r"><code>MaxLik &lt;- optim(par=init.params, fn=NegLogLik_func, data=Myx)

MaxLik</code></pre>
<pre><code>## $par
##          a          b      shape 
##  3.5614933  0.1713346 90.6790545 
## 
## $value
## [1] 29.50917
## 
## $counts
## function gradient 
##      202       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>How might we evaluate goodness-of-fit in this case??</p>
<div id="plug-in-prediction-bounds" class="section level3">
<h3>Plug-in prediction bounds</h3>
<p>The simplest way is just to plot the expected value along with “plug-in” bounds around that prediction, to represent the range of data likely to be produced under the model. We have done this before!</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15))
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;green&quot;)

upper &lt;- qgamma(0.975,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
lower &lt;- qgamma(0.025,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])

points(1:10,upper,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)
points(1:10,lower,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This gives us a simple and useful way to visualize goodness-of-fit.</p>
</div>
<div id="simulated-datasets" class="section level3">
<h3>Simulated datasets!</h3>
<p>Alternatively, we could generate simulated data sets under the best-fit model:</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15),type=&quot;n&quot;)
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;green&quot;)

uniquedays &lt;- sort(unique(Myx$day))
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],uniquedays)
simdata &lt;- array(0,dim=c(1000,length(uniquedays)))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(length(uniquedays),shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}

upper &lt;- apply(simdata,2,function(t) quantile(t,0.975))
lower &lt;- apply(simdata,2,function(t) quantile(t,0.025))

points(uniquedays,upper,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)
points(uniquedays,lower,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)

boxplot(x=as.list(as.data.frame(simdata)),at=uniquedays,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,col=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Using simulated data, we can go further- we can compare the simulated data vs the observed data more quantitatively. For example, we can compute the root mean squared error (RMSE) for the simulated datasets and compare that with the root mean squared error for the observed data.</p>
<pre class="r"><code>expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],Myx$day)
simdata &lt;- array(0,dim=c(1000,length(Myx$day)))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(length(Myx$day),shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}
 
rmse_observed &lt;- sqrt(mean((Myx$titer-expected)^2))
rmse_simulated &lt;- apply(simdata,1,function(t) mean((t-expected)^2))

hist(rmse_simulated,freq=F)
abline(v=rmse_observed,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>So, is this a good model?</p>
<p>Of course, the goodness-of-fit tests you can run with simulated data are limited only by your imagination! Be dangerous!</p>
<p>NOTE: the above methods <em>do not account for parameter uncertainty</em>. This model is the maximum likelihood model- that is, the point estimates are assumed to represent the true model!!</p>
<p>If we wanted to account for parameter uncertainty in the ML framework, we could (for example) use profile likelihood CIs to estimate parameter bounds and we could plug-in bounds that incorporate parameter uncertainty.</p>
</div>
<div id="bayesian" class="section level3">
<h3>Bayesian!</h3>
<p>Accounting for parameter uncertainty is simple in a Bayesian framework…</p>
<p>In general, a posterior predictive check involves generating new data under the fitted model and comparing with the observed data.</p>
<p>First we need to fit the model in JAGS.</p>
<p>Note that we can write the predictions directly into the JAGS code</p>
<pre class="r"><code>library(R2jags)</code></pre>
<pre><code>## Loading required package: rjags</code></pre>
<pre><code>## Loading required package: coda</code></pre>
<pre><code>## Linked to JAGS 4.2.0</code></pre>
<pre><code>## Loaded modules: basemod,bugs</code></pre>
<pre><code>## 
## Attaching package: &#39;R2jags&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:coda&#39;:
## 
##     traceplot</code></pre>
<pre class="r"><code>library(lattice)

cat(&quot;
model {
  
  #############
  # LIKELIHOOD
  ############
  for(obs in 1:n.observations){
    expected[obs] &lt;- a*day[obs]*exp(-b*day[obs])  # Ricker
    titer[obs] ~ dgamma(shape,shape/expected[obs])
    titer.sim[obs] ~ dgamma(shape,shape/expected[obs])    # simulate new data (accounting for parameter uncertainty!)
    
  }
  
  #############
  # PRIORS
  ############
  shape ~ dgamma(0.001,0.001)
  a ~ dunif(0,10)
  b ~ dunif(0,10)

  #############
  # SIMULATED DATA FOR VISUALIZATION
  #############

  for(day2 in 1:10){
    expected.new[day2] &lt;- a*day2*exp(-b*day2)  # Ricker
    titer.new[day2] ~ dgamma(shape,shape/expected.new[day2])
  }


  #############
  # DERIVED QUANTITIES
  #############
  for(obs in 1:n.observations){
    SE_obs[obs] &lt;- pow(titer[obs]-expected[obs],2)      
    SE_sim[obs] &lt;- pow(titer.sim[obs]-expected[obs],2)
  }

  RMSE_obs &lt;- sqrt(mean(SE_obs[]))
  RMSE_sim &lt;- sqrt(mean(SE_sim[]))
}
&quot;, file=&quot;BUGSmod_ricker1.txt&quot;)</code></pre>
<p>Let’s run the model!</p>
<pre class="r"><code>myx.data.for.bugs &lt;- list(
  titer = Myx$titer,
  day = Myx$day,
  n.observations = length(Myx$titer)
)

init.vals.for.bugs &lt;- function(){
  list(
    shape=runif(1,20,100),
    a=runif(1,0.5,1.5),
    b=runif(1,0.1,0.3)
  )
}

params.to.store &lt;- c(&quot;shape&quot;,&quot;a&quot;,&quot;b&quot;,&quot;RMSE_obs&quot;,&quot;RMSE_sim&quot;,&quot;titer.new&quot;)    # specify the parameters we want to get the posteriors for

jags.fit &lt;- jags(data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,n.iter=50000,model.file=&quot;BUGSmod_ricker1.txt&quot;,n.chains = 3,n.burnin = 5000,n.thin = 20 )</code></pre>
<pre><code>## module glm loaded</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 27
##    Unobserved stochastic nodes: 40
##    Total graph size: 458
## 
## Initializing model</code></pre>
<pre class="r"><code>jags.fit.mcmc &lt;- as.mcmc(jags.fit)

posterior &lt;- as.data.frame(jags.fit$BUGSoutput$sims.list)</code></pre>
<p>Assuming convergence, let’s move on to the goodness-of-fit part! First, let’s visualize the observed data against the cloud of data that could be produced under this model!</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15),type=&quot;n&quot;)
expected &lt;- Ricker(mean(posterior$a),mean(posterior$b),1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;red&quot;)

boxplot(x=as.list(posterior[,7:16]),at=1:10,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,border=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Looks pretty good so far! Let’s look at another posterior predictive check…</p>
<pre class="r"><code>plot(posterior$RMSE_sim~posterior$RMSE_obs, main=&quot;posterior predictive check&quot;)
abline(0,1,col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>p.value=length(which(as.vector(jags.fit.mcmc[,&quot;RMSE_sim&quot;][[1]])&gt;as.vector(jags.fit.mcmc[,&quot;RMSE_obs&quot;][[1]])))/length(as.vector(jags.fit.mcmc[,&quot;RMSE_sim&quot;][[1]]))
p.value</code></pre>
<pre><code>## [1] 0.6595556</code></pre>
<p>Okay, the fit seems more or less reasonable!</p>
</div>
</div>
<div id="predictive-ability" class="section level2">
<h2>Predictive ability</h2>
<p>In many cases, goodness-of-fit is not ultimately what we are most interested in. What we really want to know is whether the model does a good job at predicting the response variable.</p>
<p>One way to do this would be for us to focus on an R-squared or pseudo-R2 statistic:</p>
<p><span class="math inline">\(R^2 = 1-\frac{SS_{res}}{SS_{tot}}\)</span></p>
<p><span class="math inline">\(PseudoR^2 = 1-(\frac{logLik_{mod}}{logLik_{null}})\)</span></p>
<p>Let’s compute these metrics for the fitted data:</p>
<pre class="r"><code>SS_res &lt;- sum((Myx$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],Myx$day))^2)
SS_tot &lt;- sum((Myx$titer-mean(Myx$titer))^2)
Rsquared &lt;- 1-SS_res/SS_tot

cat(&quot;R-squared = &quot;, Rsquared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared =  0.46578</code></pre>
<pre class="r"><code>NegLogLik_null &lt;- function(params){
  -sum(dgamma(Myx$titer,shape=params[2],scale=params[1]/params[2],log = T))
}

init.params &lt;- c(mean=7,shape=50)

MaxLik_null &lt;- optim(par=init.params, fn=NegLogLik_null)

McFadden &lt;- 1-(MaxLik$value/MaxLik_null$value)
cat(&quot;McFadden&#39;s R-squared = &quot;, McFadden) </code></pre>
<pre><code>## McFadden&#39;s R-squared =  0.2165806</code></pre>
<p>Another way to evaluate model skill, or performance, is to use root mean squared error:</p>
<p><span class="math inline">\(RMSE = sqrt(mean(residuals^2))\)</span></p>
<p>RMSE gives a good indicator of the mean error rate, which is often useful and interpretable in an absolute way. For example, the modeled temperature is usually within 1.3 degrees C of the true temperature…</p>
<pre class="r"><code>RMSE = sqrt(mean((Myx$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],Myx$day))^2))
cat(&quot;RMSE = &quot;, RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE =  0.6767347</code></pre>
<p>So is our model <em>good</em>???</p>
<p>Does it really do a good job at prediction? Is the model over-fitted?</p>
<div id="validation" class="section level3">
<h3>Validation</h3>
<p>Let’s imagine we collect some new Myxomatosis titer data, and it looks like this:</p>
<pre class="r"><code>newdata &lt;- data.frame(
  grade = 1,
  day = c(2,3,4,5,6,7,8),
  titer = c(4.4,7.2,6.8,5.9,9.1,8.3,8.8)
)
newdata</code></pre>
<pre><code>##   grade day titer
## 1     1   2   4.4
## 2     1   3   7.2
## 3     1   4   6.8
## 4     1   5   5.9
## 5     1   6   9.1
## 6     1   7   8.3
## 7     1   8   8.8</code></pre>
<p>First we might simply visualize the new data against the cloud of data possibly produced under the fitted model…</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15),type=&quot;n&quot;,xlab=&quot;days&quot;,ylab=&quot;titer&quot;)
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;green&quot;)

expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
simdata &lt;- array(0,dim=c(1000,10))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(10,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}

upper &lt;- apply(simdata,2,function(t) quantile(t,0.975))
lower &lt;- apply(simdata,2,function(t) quantile(t,0.025))

points(1:10,upper,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)
points(1:10,lower,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)

boxplot(x=as.list(as.data.frame(simdata)),at=1:10,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,border=&quot;green&quot;)
points(newdata$day,newdata$titer,cex=1.5,pch=20,col=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20,col=&quot;black&quot;)
legend(&quot;topleft&quot;,pch=c(20,20),col=c(&quot;black&quot;,&quot;red&quot;),legend=c(&quot;original data&quot;,&quot;validation data&quot;))</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Now let’s evaluate the skill of our model at predicting the new data… using our above measures of skill, or performance.</p>
<pre class="r"><code>SS_res &lt;- sum((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2)
SS_tot &lt;- sum((newdata$titer-mean(newdata$titer))^2)
Rsquared_validation &lt;- 1-SS_res/SS_tot

cat(&quot;R-squared = &quot;, Rsquared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared =  0.46578</code></pre>
<pre class="r"><code>expected &lt;- Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day)
McFadden_validation &lt;- 1-(sum(dgamma(newdata$titer,shape=MaxLik$par[&quot;shape&quot;],scale=expected/MaxLik$par[&quot;shape&quot;], log = T))/sum(dgamma(newdata$titer,shape=MaxLik_null$par[&quot;shape&quot;],scale=MaxLik_null$par[&quot;mean&quot;]/MaxLik_null$par[&quot;shape&quot;],log=T)))
cat(&quot;pseudo R-squared = &quot;, McFadden_validation, &quot;\n&quot;)</code></pre>
<pre><code>## pseudo R-squared =  0.2275702</code></pre>
<pre class="r"><code>RMSE = sqrt(mean((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2))
cat(&quot;RMSE = &quot;, RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE =  1.144803</code></pre>
<p>The above analyses seem to indicate that the model fits the new data well, and that the model is successfully able to explain some of the variation in the new data.</p>
</div>
<div id="generalityextrapolation" class="section level3">
<h3>Generality/extrapolation</h3>
<p>Imagine we collect some more new data, this time in which titers are measured from day 10 to 16. Let’s see if the model does a good job now!</p>
<pre class="r"><code>newdata &lt;- data.frame(
  grade = 1,
  day = c(10,11,12,13,14,15,16),
  titer = c(6.8,8.0,4.5,3.1,2.7,1.2,0.04)
)
newdata</code></pre>
<pre><code>##   grade day titer
## 1     1  10  6.80
## 2     1  11  8.00
## 3     1  12  4.50
## 4     1  13  3.10
## 5     1  14  2.70
## 6     1  15  1.20
## 7     1  16  0.04</code></pre>
<p>As before, let’s first simply visualize the new data against the cloud of data possibly produced under the fitted model…</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,20),ylim=c(0,15),type=&quot;n&quot;,xlab=&quot;days&quot;,ylab=&quot;titer&quot;)
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:20)
points(1:20,expected,type=&quot;l&quot;,col=&quot;green&quot;)

expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:20)
simdata &lt;- array(0,dim=c(1000,20))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(20,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}

upper &lt;- apply(simdata,2,function(t) quantile(t,0.975))
lower &lt;- apply(simdata,2,function(t) quantile(t,0.025))

points(1:20,upper,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)
points(1:20,lower,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)

boxplot(x=as.list(as.data.frame(simdata)),at=1:20,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,border=&quot;green&quot;)
points(newdata$day,newdata$titer,cex=1.5,pch=20,col=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20,col=&quot;black&quot;)
legend(&quot;topleft&quot;,pch=c(20,20),col=c(&quot;black&quot;,&quot;red&quot;),legend=c(&quot;original data&quot;,&quot;new data&quot;))</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Now let’s evaluate the skill of our model at predicting the withheld data… using our favorite measures of skill, or performance.</p>
<pre class="r"><code>SS_res &lt;- sum((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2)
SS_tot &lt;- sum((newdata$titer-mean(newdata$titer))^2)
Rsquared_validation &lt;- 1-SS_res/SS_tot

cat(&quot;R-squared = &quot;, Rsquared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared =  0.46578</code></pre>
<pre class="r"><code>expected &lt;- Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day)
McFadden_validation &lt;- 1-(sum(dgamma(newdata$titer,shape=MaxLik$par[&quot;shape&quot;],scale=expected/MaxLik$par[&quot;shape&quot;], log = T))/sum(dgamma(newdata$titer,shape=MaxLik_null$par[&quot;shape&quot;],scale=MaxLik_null$par[&quot;mean&quot;]/MaxLik_null$par[&quot;shape&quot;],log=T)))
cat(&quot;pseudo R-squared = &quot;, McFadden_validation, &quot;\n&quot;)</code></pre>
<pre><code>## pseudo R-squared =  -0.3697861</code></pre>
<pre class="r"><code>RMSE = sqrt(mean((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2))
cat(&quot;RMSE = &quot;, RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE =  2.196227</code></pre>
<p>Interestingly, this example shows a couple things:<br />
1) The model was very poor at extrapolating.<br />
2) The R-squared isn’t always the best measure of model performance.<br />
3) The McFadden pseudo-Rsquared can go below 0!</p>
</div>
<div id="cross-validation" class="section level3">
<h3>Cross-validation</h3>
<p>In many cases, you will not have new data against which to test the model. Cross-validation allows us to test the model anyway. Here is some pseudocode:</p>
<ol style="list-style-type: decimal">
<li>Partition the data into <em>k</em> partitions</li>
<li>Fit the model, leaving one data partition out at a time
<ul>
<li>loop through the partitions.<br />
</li>
<li>for each iteration of the loop, fit the model to all the data EXCEPT the observations in this partition<br />
</li>
<li>use this new fitted model to predict the response variable for all observations in this partition<br />
</li>
</ul></li>
<li>Compute overall model performance for the cross-validation!</li>
</ol>
<p>The most common forms of cross-validation are: (1) leave-one-out (jackknife) and (10-fold)</p>
<p>Let’s go through an example using the Myxomatosis data!</p>
<pre class="r"><code>##### 
# PARTITION THE DATA
####

n.folds &lt;- nrow(Myx)   # jackknife

Myx$fold &lt;- sample(c(1:n.folds),size=nrow(Myx),replace=FALSE)

init.params &lt;- c(a=1,b=0.2,shape=50)

Myx$pred_CV &lt;- 0
for(i in 1:n.folds){
  Myx2 &lt;- subset(Myx,fold!=i)   # observations to use for fitting 
  newfit &lt;- optim(par=init.params, fn=NegLogLik_func, data=Myx2)   # fit the model, leaving out this partition
  ndx &lt;- Myx$fold == i
  Myx$pred_CV[ndx] &lt;- Ricker(newfit$par[&#39;a&#39;],newfit$par[&#39;b&#39;],Myx$day[ndx])
}

Myx$pred_full &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],Myx$day)

Myx</code></pre>
<pre><code>##    grade day titer fold  pred_CV pred_full
## 1      1   2 5.207   18 5.037240  5.056415
## 2      1   2 5.734   12 4.964784  5.056415
## 3      1   2 6.613    3 4.848091  5.056415
## 4      1   3 5.997   19 6.421393  6.390345
## 5      1   3 6.612   27 6.372576  6.390345
## 6      1   3 6.810    4 6.356336  6.390345
## 7      1   4 5.930   13 7.239066  7.178825
## 8      1   4 6.501   24 7.210826  7.178825
## 9      1   4 7.182   14 7.179047  7.178825
## 10     1   4 7.292    5 7.173159  7.178825
## 11     1   5 7.819    8 7.550076  7.560555
## 12     1   5 7.489   10 7.562292  7.560555
## 13     1   5 6.918   22 7.585407  7.560555
## 14     1   5 6.808   16 7.589677  7.560555
## 15     1   6 6.235   25 7.710027  7.644080
## 16     1   6 6.916   23 7.678921  7.644080
## 17     1   2 4.196   15 5.170366  5.056415
## 18     1   9 7.682    7 6.680056  6.857841
## 19     1   8 8.189    9 7.113403  7.235101
## 20     1   7 7.707   21 7.500069  7.513847
## 21     1   7 7.597   26 7.506437  7.513847
## 22     1   8 7.112   20 7.251935  7.235101
## 23     1   8 7.354    6 7.221209  7.235101
## 24     1   6 7.158    1 7.666217  7.644080
## 25     1   6 7.466    2 7.651439  7.644080
## 26     1   6 7.927   17 7.630461  7.644080
## 27     1   6 8.499   11 7.602640  7.644080</code></pre>
<p>To assess how well the model is performing, let’s compute the root mean squared error for the full model vs the cross-validation:</p>
<pre class="r"><code>RMSE_full &lt;- sqrt(mean((Myx$titer-Myx$pred_full)^2))
RMSE_CV &lt;- sqrt(mean((Myx$titer-Myx$pred_CV)^2))
RMSE_full</code></pre>
<pre><code>## [1] 0.6767347</code></pre>
<pre class="r"><code>RMSE_CV</code></pre>
<pre><code>## [1] 0.7374805</code></pre>
<p>As expected, the RMSE is higher under cross-validation.</p>
<p>Is the model still okay? One way to look at this would be to assess the variance explained.</p>
<pre class="r"><code>VarExplained_full = 1 - mean((Myx$titer-Myx$pred_full)^2)/mean((Myx$titer-mean(Myx$titer))^2)
VarExplained_CV = 1 - mean((Myx$titer-Myx$pred_CV)^2)/mean((Myx$titer-mean(Myx$titer))^2)
VarExplained_full</code></pre>
<pre><code>## [1] 0.46578</code></pre>
<pre class="r"><code>VarExplained_CV</code></pre>
<pre><code>## [1] 0.3655692</code></pre>
<p>Clearly the model performance is lower for the cross-validation, and usually will be! But this is also a more honest evaluation of model performance.</p>
</div>
<div id="example-multiple-logistic-regression" class="section level3">
<h3>Example: multiple logistic regression</h3>
<p>NOTE: example modified from <a href="https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/">here</a></p>
<p>Let’s evaluate which factors were related to surviving the titanic disaster!</p>
<pre class="r"><code>titanic &lt;- read.csv(&quot;titanic.csv&quot;,header=T)
head(titanic)</code></pre>
<pre><code>##   PassengerId Survived Pclass
## 1           1        0      3
## 2           2        1      1
## 3           3        1      3
## 4           4        1      1
## 5           5        0      3
## 6           6        0      3
##                                                  Name    Sex Age SibSp
## 1                             Braund, Mr. Owen Harris   male  22     1
## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1
## 3                              Heikkinen, Miss. Laina female  26     0
## 4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1
## 5                            Allen, Mr. William Henry   male  35     0
## 6                                    Moran, Mr. James   male  NA     0
##   Parch           Ticket    Fare Cabin Embarked
## 1     0        A/5 21171  7.2500              S
## 2     0         PC 17599 71.2833   C85        C
## 3     0 STON/O2. 3101282  7.9250              S
## 4     0           113803 53.1000  C123        S
## 5     0           373450  8.0500              S
## 6     0           330877  8.4583              Q</code></pre>
<p>Our goal is to model the probability of surviving the titanic disaster as a function of covariates like sex, age, number of siblings or spouses onboard, number of parents or children, passenger fare, etc.</p>
<p>Let’s first build a simple logistic regression model for this problem.</p>
<pre class="r"><code>model1 &lt;- glm(Survived ~ Sex + SibSp + Parch + Fare, data=titanic, family=&quot;binomial&quot;)
summary(model1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Sex + SibSp + Parch + Fare, family = &quot;binomial&quot;, 
##     data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2371  -0.6229  -0.5964   0.7498   2.4891  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.946635   0.169354   5.590 2.28e-08 ***
## Sexmale     -2.642219   0.186114 -14.197  &lt; 2e-16 ***
## SibSp       -0.353892   0.098185  -3.604 0.000313 ***
## Parch       -0.200724   0.112037  -1.792 0.073200 .  
## Fare         0.014685   0.002644   5.553 2.80e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1186.66  on 890  degrees of freedom
## Residual deviance:  859.04  on 886  degrees of freedom
## AIC: 869.04
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Alternatively, we could write our own likelihood function!!</p>
<pre class="r"><code>params &lt;- c(
  int=1,
  male = -3,
  sibsp = 0,
  parch = 0,
  fare = 0.02
)

LikFunc &lt;- function(params){
  linear &lt;- params[&#39;int&#39;] + 
    params[&#39;male&#39;]*as.numeric(titanic$Sex==&quot;male&quot;) +
    params[&#39;sibsp&#39;]*titanic$SibSp +
    params[&#39;parch&#39;]*titanic$Parch +
    params[&#39;fare&#39;]*titanic$Fare
  logitlinear &lt;-  1/(1+exp(-(linear)))
  -sum(dbinom(titanic$Survived,size=1,prob = logitlinear,log=T))
}

LikFunc(params)</code></pre>
<pre><code>## [1] 457.3548</code></pre>
<pre class="r"><code>MLE &lt;- optim(fn=LikFunc,par = params)

MLE$par</code></pre>
<pre><code>##         int        male       sibsp       parch        fare 
##  1.00192952 -2.68340590 -0.37983132 -0.23729121  0.01494866</code></pre>
<p>Interesting- the results are very slightly different from the glm results…!</p>
<p>Since we are interested in evaluating model performance, let’s see how well the model predicts which people survived the titanic disaster…</p>
<p>BUT… since the response is binary, our model skill metrics don’t really work too well!</p>
<p>For binary responses, the ROC curve is a good way of evaluating model performance!</p>
<pre class="r"><code>library(ROCR)
library(rms)


###################################
#################### CROSS VALIDATION CODE

n.folds = 10
foldVector = rep(c(1:n.folds),times=floor(length(titanic$Survived)/9))[1:length(titanic$Survived)]
#n.folds = length(titanic$resp_factor)
#foldVector &lt;- c(1:length(titanic$resp_factor))

counter = 1
CVprediction &lt;- numeric(nrow(titanic))
CVobserved &lt;- numeric(nrow(titanic))
realprediction &lt;- numeric(nrow(titanic))
realdata &lt;- numeric(nrow(titanic))

counter=1

#test &lt;- numeric(nrow(titanic))
for(i in 1:n.folds){
  model &lt;- glm(formula = Survived ~ Sex + SibSp + Parch + Fare, family = &quot;binomial&quot;, data = titanic[which(foldVector!=i),]) 
  predict_CV  &lt;- plogis(predict(model,newdata=titanic[which(foldVector==i),])) 
  predict_real  &lt;-  plogis(predict(model1,newdata=titanic[which(foldVector==i),]))
  REAL &lt;- titanic$Survived[which(foldVector==i)]
  for(j in 1:length(which(foldVector==i))){
    CVprediction[counter] &lt;- predict_CV[j]
    CVobserved[counter] &lt;-  REAL[j]      
    realprediction[counter] &lt;- predict_real[j]   
    realdata[counter] &lt;- REAL[j]         
    counter = counter + 1  
  }
}

CV_RMSE = sqrt(mean((CVobserved-CVprediction)^2))       # root mean squared error for holdout samples in 10-fold cross-validation ...
real_RMSE = sqrt(mean((CVobserved-realprediction)^2))  # root mean squared error for residuals from final model

# print RMSE statistics
CV_RMSE </code></pre>
<pre><code>## [1] 0.3983029</code></pre>
<pre class="r"><code>real_RMSE   </code></pre>
<pre><code>## [1] 0.3937799</code></pre>
<pre class="r"><code>par(mfrow=c(2,1))
pred &lt;- prediction(CVprediction,CVobserved)     # for holdout samples in cross-validation
perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)
auc &lt;- performance(pred,&quot;auc&quot;)
plot(perf)
text(.9,.1,paste(&quot;AUC = &quot;,round(auc@y.values[[1]],2),sep=&quot;&quot;))

pred &lt;- prediction(realprediction,CVobserved)     # for final model
perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)
auc &lt;- performance(pred,&quot;auc&quot;)
plot(perf)
text(.9,.1,paste(&quot;AUC = &quot;,round(auc@y.values[[1]],2),sep=&quot;&quot;))</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>### display confusion matrix and kappa for a single threshold

cutoff = 0.5

trueLabels &lt;- CVobserved
predLabels &lt;- ifelse(CVprediction&gt;=cutoff,1,0)    
tot &lt;- length(CVobserved)
tp &lt;- length(which((trueLabels==1)&amp;(predLabels==1)))  
tn &lt;- length(which((trueLabels==0)&amp;(predLabels==0)))
fp &lt;- length(which((trueLabels==0)&amp;(predLabels==1)))
fn &lt;- length(which((trueLabels==1)&amp;(predLabels==0)))
pr_agree &lt;- (tp+tn)/tot    # overall agreement, or accuracy
pr_agree_rand &lt;- ((tp+fn)/tot)*((tp+fp)/tot)+((fn+tn)/tot)*((fp+tn)/tot)
kappa &lt;- (pr_agree-pr_agree_rand)/(1-pr_agree_rand)
kappa</code></pre>
<pre><code>## [1] 0.5410852</code></pre>
<pre class="r"><code>matrix(c(tp,fp,fn,tn),nrow=2,ncol=2)</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  231  111
## [2,]   79  470</code></pre>
<pre class="r"><code>sensitivity &lt;- tp/(tp+fn)
specificity &lt;- tn/(tn+fp)
toterror &lt;- (fn+fp)/tot
sensitivity</code></pre>
<pre><code>## [1] 0.6754386</code></pre>
<pre class="r"><code>specificity</code></pre>
<pre><code>## [1] 0.856102</code></pre>
<pre class="r"><code>toterror</code></pre>
<pre><code>## [1] 0.2132435</code></pre>
<pre class="r"><code>CVprediction[which(CVprediction==1)] &lt;- 0.9999
CVprediction[which(CVprediction==0)] &lt;- 0.0001
realprediction[which(realprediction==1)] &lt;- 0.9999
realprediction[which(realprediction==0)] &lt;- 0.0001



realdata = CVobserved
fit_deviance_CV &lt;- mean(-2*(dbinom(CVobserved,1,CVprediction,log=T)-dbinom(realdata,1,realdata,log=T)))
fit_deviance_real &lt;- mean(-2*(dbinom(CVobserved,1,realprediction,log=T)-dbinom(realdata,1,realdata,log=T)))
null_deviance &lt;- mean(-2*(dbinom(CVobserved,1,mean(CVobserved),log=T)-dbinom(realdata,1,realdata,log=T)))
deviance_explained_CV &lt;- (null_deviance-fit_deviance_CV)/null_deviance   # based on holdout samples
deviance_explained_real &lt;- (null_deviance-fit_deviance_real)/null_deviance   # based on full model...

deviance_explained_CV</code></pre>
<pre><code>## [1] 0.2564415</code></pre>
<pre class="r"><code>deviance_explained_real</code></pre>
<pre><code>## [1] 0.2760832</code></pre>
<p>NEXT: do the same analysis with Random Forest…</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
