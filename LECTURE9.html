<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 746" />


<title>Model Performance Evaluation</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
    <li>
      <a href="GIT_tutorial.html">Demo: version control in Git</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Model Performance Evaluation</h1>
<h4 class="author">NRES 746</h4>
<h4 class="date">Fall 2023</h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a
href="LECTURE9.R">click here</a> for the companion R script for this
lecture.</p>
<p>In this course so far, we have constructed data-generating models and
fitted these models to observed data using likelihood-based methods (ML
and Bayesian inference). We also have explored a range of methods to
account for <em>structural uncertainty</em> (which of a set of candidate
models could plausibly have generated our observed data).</p>
<p>But even after we have fitted a model to data, even after we have
compared a suite of plausible models and selected the best one, are we
really sure that the model is <em>good</em>?</p>
<p>What does it even mean to say that the model is good?</p>
<p>Usually, we mean one or more of the following:</p>
<ol style="list-style-type: decimal">
<li><strong>Goodness-of-fit</strong>: The data could easily/reasonably
have been generated under the fitted model (the model is adequate)</li>
<li><strong>Predictive performance</strong>: The fitted model performs
well at predicting responses for out-of-sample data (the model is
useful)</li>
<li><strong>Generality</strong>: The model performs well at
extrapolating responses for out-of-sample data that are outside the
range of those in the training set (the model is really useful!)</li>
</ol>
<div id="goodness-of-fit" class="section level2">
<h2>Goodness-of-fit</h2>
<p>We have already looked at a variety of methods for evaluating
goodness-of-fit. In general, you can use data simulation (virtual
ecology) to evaluate whether your fitted model is capable of generating
the observed data. In a frequentist/ML context this is often called a
<strong>parametric bootstrap</strong></p>
<p><strong>Q</strong>: What is the difference between a non-parametric
bootstrap (usually just called a ‘bootstrap’) and a parametric
bootstrap?</p>
<p>For example, we can overlay the observed data (or a model performance
statistic like R-squared) on a cloud of points representing the range of
data sets (or performance statistics) possibly produced under the fitted
model.</p>
<p>Or, we can use a “plug-in” prediction bounds as a substitute for the
cloud of data sets possibly produced under the model.</p>
<p>Often, we compare a target summary statistic (e.g., Deviance, RMSE,
R-squared) for the observed data with the range of that target summary
statistic produced under the inferred, stochastic data generating
model.</p>
<p>Let’s return to the Myxomatosis dataset!</p>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)  #Data set from grade 1 of myxo data
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>Now let’s use ML to fit the Ricker model with a gamma error
distribution.</p>
<pre class="r"><code># Fit the model with ML -----------------------------

Ricker &lt;- function(a,b,predvar) a*predvar*exp(-b*predvar)
  
NegLogLik_func &lt;- function(params,data){
  expected &lt;- Ricker(params[1],params[2],data$day)
  -sum(dgamma(data$titer,shape=params[3],scale=expected/params[3],log = T))
}

init.params &lt;- c(a=1,b=0.2,shape=50)
NegLogLik_func(init.params,data=Myx)</code></pre>
<pre><code>## [1] 2336.22</code></pre>
<pre class="r"><code>MaxLik &lt;- optim(par=init.params, fn=NegLogLik_func, data=Myx)

MaxLik</code></pre>
<pre><code>## $par
##          a          b      shape 
##  3.5614933  0.1713346 90.6790545 
## 
## $value
## [1] 29.50917
## 
## $counts
## function gradient 
##      202       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>How might we evaluate goodness-of-fit in this case??</p>
<div id="plug-in-prediction-bounds" class="section level3">
<h3>‘Plug-in’ prediction bounds</h3>
<p>The simplest way is just to plot the expected value along with
“plug-in” bounds around that prediction, to represent the range of data
likely to be produced under the model. We have done this before!</p>
<pre class="r"><code># Plug-in prediction interval -------------------------

plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15))
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;green&quot;)

upper &lt;- qgamma(0.975,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
lower &lt;- qgamma(0.025,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])

points(1:10,upper,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)
points(1:10,lower,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>This gives us a simple and useful way to visualize
goodness-of-fit.</p>
</div>
<div id="simulated-datasets" class="section level3">
<h3>Simulated datasets!</h3>
<p>Alternatively, we could generate simulated data sets under the
best-fit model (parametric bootstrap!):</p>
<pre class="r"><code># Parametric bootstrap!  -------------------------------------

plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15),type=&quot;n&quot;)
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;green&quot;)

uniquedays &lt;- sort(unique(Myx$day))
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],uniquedays)
simdata &lt;- array(0,dim=c(1000,length(uniquedays)))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(length(uniquedays),shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}

upper &lt;- apply(simdata,2,function(t) quantile(t,0.975))
lower &lt;- apply(simdata,2,function(t) quantile(t,0.025))

points(uniquedays,upper,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)
points(uniquedays,lower,type=&quot;l&quot;,col=&quot;red&quot;,lty=2)

boxplot(x=as.list(as.data.frame(simdata)),at=uniquedays,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,col=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Using simulated data, we can go further- we can compare the simulated
data vs the observed data more quantitatively. For example, we can
compute the root mean squared error (RMSE) for the simulated datasets
and compare that with the root mean squared error for the observed
data.</p>
<pre class="r"><code># Compare observed error statistic with expected range of error statistic as part of parametric bootstrap analysis

expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],Myx$day)
simdata &lt;- array(0,dim=c(1000,length(Myx$day)))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(length(Myx$day),shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}
 
rmse_observed &lt;- sqrt(mean((Myx$titer-expected)^2))
rmse_simulated &lt;- apply(simdata,1,function(t) mean((t-expected)^2))

hist(rmse_simulated,freq=F)
abline(v=rmse_observed,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>So, is this a good model?</p>
<p>Of course, the goodness-of-fit tests you can run with simulated data
are limited only by your imagination! <strong>Be dangerous!</strong></p>
<p>NOTE: the above methods <em>do not account for parameter
uncertainty</em>. This model is the maximum likelihood model- that is,
the point estimates are assumed to represent the true model!</p>
<p>If we wanted to account for parameter uncertainty in the ML
framework, we could (for example) use profile-likelihood CIs to estimate
parameter bounds and we could plug-in bounds that incorporate parameter
uncertainty. Or we could treat the CIs for the parameter estimates as a
probability distribution (e.g., uniform) and simulate datasets across
this range of parameter uncertainty (but what would a frequentist
statistician say about this?).</p>
</div>
<div id="bayesian" class="section level3">
<h3>Bayesian!</h3>
<p>Accounting for prediction uncertainty is simple in a Bayesian
framework… (this is part of lab 4!)</p>
<p>In general, a <strong>posterior predictive check</strong> involves
generating new data sets under the fitted model that are equivalent to
the observed data set (i.e., same sample size and covariate values) and
comparing with the observed data.</p>
<p>First we need to fit the model in JAGS.</p>
<p>Note that we can write the predictions directly into the JAGS
code</p>
<pre class="r"><code># Bayesian goodness-of-fit  -------------------------

library(R2jags)
library(lattice)

cat(&quot;
model {
  
  #############
  # LIKELIHOOD
  ############
  for(obs in 1:n.observations){
    expected[obs] &lt;- a*day[obs]*exp(-b*day[obs])  # Ricker
    titer[obs] ~ dgamma(shape,shape/expected[obs])
    titer.sim[obs] ~ dgamma(shape,shape/expected[obs])    # simulate new data (accounting for parameter uncertainty!
  }
  
  #############
  # PRIORS
  ############
  shape ~ dgamma(0.001,0.001)
  a ~ dunif(0,10)
  b ~ dunif(0,10)

  #############
  # SIMULATED DATA FOR VISUALIZATION
  #############

  for(day2 in 1:10){
    expected.new[day2] &lt;- a*day2*exp(-b*day2)  # Ricker
    titer.new[day2] ~ dgamma(shape,shape/expected.new[day2])
  }


  #############
  # DERIVED QUANTITIES
  #############
  for(obs in 1:n.observations){
    SE_obs[obs] &lt;- pow(titer[obs]-expected[obs],2)      
    SE_sim[obs] &lt;- pow(titer.sim[obs]-expected[obs],2)
  }

  RMSE_obs &lt;- sqrt(mean(SE_obs[]))
  RMSE_sim &lt;- sqrt(mean(SE_sim[]))
}
&quot;, file=&quot;BUGSmod_ricker1.txt&quot;)</code></pre>
<p>Let’s run the model!</p>
<pre class="r"><code>myx.data.for.bugs &lt;- list(
  titer = Myx$titer,
  day = Myx$day,
  n.observations = length(Myx$titer)
)

init.vals.for.bugs &lt;- function(){
  list(
    shape=runif(1,20,100),
    a=runif(1,0.5,1.5),
    b=runif(1,0.1,0.3)
  )
}

params.to.store &lt;- c(&quot;shape&quot;,&quot;a&quot;,&quot;b&quot;,&quot;RMSE_obs&quot;,&quot;RMSE_sim&quot;,&quot;titer.new&quot;)    # specify the parameters we want to get the posteriors for

jags.fit &lt;- jags(data=myx.data.for.bugs,inits=init.vals.for.bugs,parameters.to.save=params.to.store,n.iter=50000,model.file=&quot;BUGSmod_ricker1.txt&quot;,n.chains = 3,n.burnin = 5000,n.thin = 20 )</code></pre>
<pre><code>## module glm loaded</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 27
##    Unobserved stochastic nodes: 40
##    Total graph size: 458
## 
## Initializing model</code></pre>
<pre class="r"><code>jags.fit.mcmc &lt;- as.mcmc(jags.fit)

posterior &lt;- as.data.frame(jags.fit$BUGSoutput$sims.list)</code></pre>
<p>Assuming convergence, let’s move on to the goodness-of-fit part!
First, let’s visualize the observed data against the cloud of data that
could be produced under this model!</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15),type=&quot;n&quot;)
expected &lt;- Ricker(mean(posterior$a),mean(posterior$b),1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;red&quot;)

boxplot(x=as.list(posterior[,7:16]),at=1:10,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,border=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Looks pretty good so far! Let’s look at another posterior predictive
check…</p>
<pre class="r"><code>plot(posterior$RMSE_sim~posterior$RMSE_obs, main=&quot;posterior predictive check&quot;)
abline(0,1,col=&quot;red&quot;,lwd=2)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>p.value=length(which(as.vector(jags.fit.mcmc[,&quot;RMSE_sim&quot;][[1]])&gt;as.vector(jags.fit.mcmc[,&quot;RMSE_obs&quot;][[1]])))/length(as.vector(jags.fit.mcmc[,&quot;RMSE_sim&quot;][[1]]))
p.value</code></pre>
<pre><code>## [1] 0.6751111</code></pre>
<p>Okay, the fit seems more or less reasonable!</p>
</div>
</div>
<div id="predictive-ability" class="section level2">
<h2>Predictive ability</h2>
<p>In many cases, goodness-of-fit is not ultimately what we are most
interested in. What we really want to know is whether the model does a
good job at predicting the response. Is our model useful in some
way?</p>
<p>One way to do this would be for us to focus on an R-squared or
pseudo-R2 statistic:</p>
<p><span class="math inline">\(R^2 =
1-\frac{SS_{res}}{SS_{tot}}\)</span></p>
<p><span class="math inline">\(PseudoR^2 =
1-(\frac{logLik_{mod}}{logLik_{null}})\)</span></p>
<p>Let’s compute these metrics for the fitted data:</p>
<pre class="r"><code># Summary statistics of a models &quot;usefulness&quot; (e.g., R-squared)  -------------------

SS_res &lt;- sum((Myx$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],Myx$day))^2)
SS_tot &lt;- sum((Myx$titer-mean(Myx$titer))^2)
Rsquared &lt;- 1-SS_res/SS_tot

cat(&quot;R-squared = &quot;, Rsquared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared =  0.46578</code></pre>
<pre class="r"><code># Fit the null likelihood model!

NegLogLik_null &lt;- function(params){
  -sum(dgamma(Myx$titer,shape=params[2],scale=params[1]/params[2],log = T))
}

init.params &lt;- c(mean=7,shape=50)

MaxLik_null &lt;- optim(par=init.params, fn=NegLogLik_null)

McFadden &lt;- 1-(MaxLik$value/MaxLik_null$value)
cat(&quot;McFadden&#39;s R-squared = &quot;, McFadden) </code></pre>
<pre><code>## McFadden&#39;s R-squared =  0.2165806</code></pre>
<p>Another way to evaluate model skill, or performance, is to use
<strong>root mean squared error (RMSE)</strong>:</p>
<p><span class="math inline">\(RMSE =
sqrt(mean(residuals^2))\)</span></p>
<p>RMSE gives a good indicator of the mean error rate, which is often
useful and understandable. For example, “the predicted temperature is
usually within 1.3 degrees C of the true temperature”…</p>
<pre class="r"><code>RMSE = sqrt(mean((Myx$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],Myx$day))^2))
cat(&quot;RMSE = &quot;, RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE =  0.6767347</code></pre>
<p>So the average error in units of titer (in log10 rabbit infectious
doses)</p>
<p>So is our model <em>good</em>???</p>
<p>Does it do a good job at prediction?</p>
<p><strong>Q</strong>: Does an over-fitted model have acceptable
goodness-of-fit?</p>
<p><strong>Q</strong>: Does an over-fitted model have acceptable
performance, evaluated as R-squared or RMSE?</p>
<p><strong>Q</strong>: Will an over-fitted model perform well when
predicting to new samples (that were not used in model fitting)?</p>
<div id="validation" class="section level3">
<h3>Validation</h3>
<p>Let’s imagine we collect some new Myxomatosis titer data (for grade 1
virus), and it looks like this:</p>
<pre class="r"><code># Collect new data that were not used in model fitting

newdata &lt;- data.frame(
  grade = 1,
  day = c(2,3,4,5,6,7,8),
  titer = c(4.4,7.2,6.8,5.9,9.1,8.3,8.8)
)
newdata</code></pre>
<pre><code>##   grade day titer
## 1     1   2   4.4
## 2     1   3   7.2
## 3     1   4   6.8
## 4     1   5   5.9
## 5     1   6   9.1
## 6     1   7   8.3
## 7     1   8   8.8</code></pre>
<p>First we might simply visualize the new data against the cloud of
data possibly produced under the fitted model…</p>
<pre class="r"><code># Validation #1  -------------------------

plot(Myx$titer~Myx$day,xlim=c(0,10),ylim=c(0,15),type=&quot;n&quot;,xlab=&quot;days&quot;,ylab=&quot;titer&quot;)
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
points(1:10,expected,type=&quot;l&quot;,col=&quot;green&quot;)

expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:10)
simdata &lt;- array(0,dim=c(1000,10))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(10,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}

upper &lt;- apply(simdata,2,function(t) quantile(t,0.975))
lower &lt;- apply(simdata,2,function(t) quantile(t,0.025))

points(1:10,upper,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)
points(1:10,lower,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)

boxplot(x=as.list(as.data.frame(simdata)),at=1:10,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,border=&quot;green&quot;)
points(newdata$day,newdata$titer,cex=1.5,pch=20,col=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20,col=&quot;black&quot;)
legend(&quot;topleft&quot;,pch=c(20,20),col=c(&quot;black&quot;,&quot;red&quot;),legend=c(&quot;original data&quot;,&quot;validation data&quot;))</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Now let’s evaluate the skill of our model at predicting the new data…
using our above measures of skill, or performance.</p>
<pre class="r"><code>SS_res &lt;- sum((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2)
SS_tot &lt;- sum((newdata$titer-mean(newdata$titer))^2)
Rsquared_validation &lt;- 1-SS_res/SS_tot

cat(&quot;R-squared = &quot;, Rsquared, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared =  0.46578</code></pre>
<pre class="r"><code>expected &lt;- Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day)
McFadden_validation &lt;- 1-(sum(dgamma(newdata$titer,shape=MaxLik$par[&quot;shape&quot;],scale=expected/MaxLik$par[&quot;shape&quot;], log = T))/sum(dgamma(newdata$titer,shape=MaxLik_null$par[&quot;shape&quot;],scale=MaxLik_null$par[&quot;mean&quot;]/MaxLik_null$par[&quot;shape&quot;],log=T)))
cat(&quot;pseudo R-squared = &quot;, McFadden_validation, &quot;\n&quot;)</code></pre>
<pre><code>## pseudo R-squared =  0.2275702</code></pre>
<pre class="r"><code>RMSE = sqrt(mean((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2))
cat(&quot;RMSE = &quot;, RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE =  1.144803</code></pre>
<p>The above analyses seem to indicate that the model fits the new data
well, and that the model is successfully able to explain some of the
variation in the new data.</p>
</div>
<div id="generalityextrapolation" class="section level3">
<h3>Generality/extrapolation</h3>
<p>Imagine we collect some more new data, this time in which titers are
measured from day 10 to 16. Let’s see if the model does a good job
now!</p>
<pre class="r"><code># Validation #2 ---------------------------

newdata &lt;- data.frame(        # imagine these are new observations...
  grade = 1,
  day = c(10,11,12,13,14,15,16),
  titer = c(6.8,8.0,4.5,3.1,2.7,1.2,0.04)
)
newdata</code></pre>
<pre><code>##   grade day titer
## 1     1  10  6.80
## 2     1  11  8.00
## 3     1  12  4.50
## 4     1  13  3.10
## 5     1  14  2.70
## 6     1  15  1.20
## 7     1  16  0.04</code></pre>
<p>As before, let’s first simply visualize the new data against the
cloud of data possibly produced under the fitted model…</p>
<pre class="r"><code>plot(Myx$titer~Myx$day,xlim=c(0,20),ylim=c(0,15),type=&quot;n&quot;,xlab=&quot;days&quot;,ylab=&quot;titer&quot;)
expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:20)
points(1:20,expected,type=&quot;l&quot;,col=&quot;green&quot;)

expected &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],1:20)
simdata &lt;- array(0,dim=c(1000,20))
for(i in 1:1000){
  simdata[i,] &lt;- rgamma(20,shape=MaxLik$par[&#39;shape&#39;],scale=expected/MaxLik$par[&#39;shape&#39;])
}

upper &lt;- apply(simdata,2,function(t) quantile(t,0.975))
lower &lt;- apply(simdata,2,function(t) quantile(t,0.025))

points(1:20,upper,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)
points(1:20,lower,type=&quot;l&quot;,col=&quot;green&quot;,lty=2)

boxplot(x=as.list(as.data.frame(simdata)),at=1:20,add=T,boxwex=0.25,xaxt=&quot;n&quot;,range=0,border=&quot;green&quot;)
points(newdata$day,newdata$titer,cex=1.5,pch=20,col=&quot;red&quot;)
points(Myx$day,Myx$titer,cex=1.5,pch=20,col=&quot;black&quot;)
legend(&quot;topleft&quot;,pch=c(20,20),col=c(&quot;black&quot;,&quot;red&quot;),legend=c(&quot;original data&quot;,&quot;new data&quot;))</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Now let’s evaluate the skill of our model at predicting the withheld
data… using our favorite measures of skill, or performance.</p>
<pre class="r"><code>SS_res &lt;- sum((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2)
SS_tot &lt;- sum((newdata$titer-mean(newdata$titer))^2)
Rsquared_validation &lt;- 1-SS_res/SS_tot

cat(&quot;R-squared = &quot;, Rsquared_validation, &quot;\n&quot;)</code></pre>
<pre><code>## R-squared =  0.3208912</code></pre>
<pre class="r"><code>expected &lt;- Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day)
McFadden_validation &lt;- 1-(sum(dgamma(newdata$titer,shape=MaxLik$par[&quot;shape&quot;],scale=expected/MaxLik$par[&quot;shape&quot;], log = T))/sum(dgamma(newdata$titer,shape=MaxLik_null$par[&quot;shape&quot;],scale=MaxLik_null$par[&quot;mean&quot;]/MaxLik_null$par[&quot;shape&quot;],log=T)))
cat(&quot;pseudo R-squared = &quot;, McFadden_validation, &quot;\n&quot;)</code></pre>
<pre><code>## pseudo R-squared =  -0.3697861</code></pre>
<pre class="r"><code>RMSE = sqrt(mean((newdata$titer-Ricker(MaxLik$par[&quot;a&quot;],MaxLik$par[&quot;b&quot;],newdata$day))^2))
cat(&quot;RMSE = &quot;, RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## RMSE =  2.196227</code></pre>
<p>Interestingly, this example shows a couple things:<br />
1) Models are often not very good at extrapolation.<br />
2) The R-squared isn’t always the best measure of model
performance.<br />
3) The McFadden pseudo-Rsquared can go below 0!<br />
4) It’s really important to evaluate goodness-of-fit in addition to
model performance!</p>
</div>
<div id="cross-validation" class="section level3">
<h3>Cross-validation</h3>
<p>In many cases, you will not have new data against which to test the
model. Cross-validation allows us to test the model anyway. Here is some
pseudocode:</p>
<ol style="list-style-type: decimal">
<li>Partition the data into <em>k</em> partitions</li>
<li>Fit the model, leaving one data partition out at a time
<ul>
<li>loop through the partitions.<br />
</li>
<li>for each iteration of the loop, fit the model to all the data EXCEPT
the observations in this partition<br />
</li>
<li>use this new fitted model to predict the response variable for all
observations in this partition<br />
</li>
</ul></li>
<li>Compute overall model performance for the cross-validation!</li>
</ol>
<p>The most common forms of cross-validation are: (1) leave-one-out
(jackknife) and (10-fold)</p>
<p>Let’s go through an example using the Myxomatosis data!</p>
<pre class="r"><code># CROSS-VALIDATION ------------------


# PARTITION THE DATA

n.folds &lt;- nrow(Myx)   # jackknife

Myx$fold &lt;- sample(c(1:n.folds),size=nrow(Myx),replace=FALSE)

init.params &lt;- c(a=1,b=0.2,shape=50)

Myx$pred_CV &lt;- 0
for(i in 1:n.folds){
  Myx2 &lt;- subset(Myx,fold!=i)   # observations to use for fitting 
  newfit &lt;- optim(par=init.params, fn=NegLogLik_func, data=Myx2)   # fit the model, leaving out this partition
  ndx &lt;- Myx$fold == i
  Myx$pred_CV[ndx] &lt;- Ricker(newfit$par[&#39;a&#39;],newfit$par[&#39;b&#39;],Myx$day[ndx])
}

Myx$pred_full &lt;- Ricker(MaxLik$par[&#39;a&#39;],MaxLik$par[&#39;b&#39;],Myx$day)

Myx</code></pre>
<pre><code>##    grade day titer fold  pred_CV pred_full
## 1      1   2 5.207   17 5.037240  5.056415
## 2      1   2 5.734    3 4.964784  5.056415
## 3      1   2 6.613   21 4.848091  5.056415
## 4      1   3 5.997   20 6.421393  6.390345
## 5      1   3 6.612   24 6.372576  6.390345
## 6      1   3 6.810   27 6.356336  6.390345
## 7      1   4 5.930    6 7.239066  7.178825
## 8      1   4 6.501    2 7.210826  7.178825
## 9      1   4 7.182    7 7.179047  7.178825
## 10     1   4 7.292   12 7.173159  7.178825
## 11     1   5 7.819   10 7.550076  7.560555
## 12     1   5 7.489    4 7.562292  7.560555
## 13     1   5 6.918   19 7.585407  7.560555
## 14     1   5 6.808    8 7.589677  7.560555
## 15     1   6 6.235   16 7.710027  7.644080
## 16     1   6 6.916   13 7.678921  7.644080
## 17     1   2 4.196   23 5.170366  5.056415
## 18     1   9 7.682    1 6.680056  6.857841
## 19     1   8 8.189   15 7.113403  7.235101
## 20     1   7 7.707    9 7.500069  7.513847
## 21     1   7 7.597   14 7.506437  7.513847
## 22     1   8 7.112   11 7.251935  7.235101
## 23     1   8 7.354   26 7.221209  7.235101
## 24     1   6 7.158   18 7.666217  7.644080
## 25     1   6 7.466   25 7.651439  7.644080
## 26     1   6 7.927   22 7.630461  7.644080
## 27     1   6 8.499    5 7.602640  7.644080</code></pre>
<p>To assess how well the model is performing, let’s compute the root
mean squared error for the full model vs the cross-validation:</p>
<pre class="r"><code>RMSE_full &lt;- sqrt(mean((Myx$titer-Myx$pred_full)^2))
RMSE_CV &lt;- sqrt(mean((Myx$titer-Myx$pred_CV)^2))
RMSE_full</code></pre>
<pre><code>## [1] 0.6767347</code></pre>
<pre class="r"><code>RMSE_CV</code></pre>
<pre><code>## [1] 0.7374805</code></pre>
<p>As expected, the RMSE is higher under cross-validation.</p>
<p>Is the model still okay? One way to look at this would be to assess
the variance explained.</p>
<pre class="r"><code>VarExplained_full = 1 - mean((Myx$titer-Myx$pred_full)^2)/mean((Myx$titer-mean(Myx$titer))^2)
VarExplained_CV = 1 - mean((Myx$titer-Myx$pred_CV)^2)/mean((Myx$titer-mean(Myx$titer))^2)
VarExplained_full</code></pre>
<pre><code>## [1] 0.46578</code></pre>
<pre class="r"><code>VarExplained_CV</code></pre>
<pre><code>## [1] 0.3655692</code></pre>
<p>Clearly the model performance is lower under cross-validation, and
almost always will be! But this is also a more honest evaluation of
model performance.</p>
</div>
</div>
<div id="example-multiple-logistic-regression" class="section level2">
<h2>Example: multiple logistic regression</h2>
<p>NOTE: example modified from <a
href="https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/">here</a></p>
<p>Let’s evaluate which factors were related to surviving the titanic
disaster!</p>
<p>You can load the titanic data example <a href="titanic.csv">here</a>.
Alternatively you can use the ‘titanic’ package in R!</p>
<pre class="r"><code># Cross-validation: titanic disaster example!  --------------------

titanic &lt;- read.csv(&quot;titanic.csv&quot;,header=T)
head(titanic)</code></pre>
<pre><code>##   PassengerId Survived Pclass
## 1           1        0      3
## 2           2        1      1
## 3           3        1      3
## 4           4        1      1
## 5           5        0      3
## 6           6        0      3
##                                                  Name    Sex Age SibSp Parch
## 1                             Braund, Mr. Owen Harris   male  22     1     0
## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1     0
## 3                              Heikkinen, Miss. Laina female  26     0     0
## 4        Futrelle, Mrs. Jacques Heath (Lily May Peel) female  35     1     0
## 5                            Allen, Mr. William Henry   male  35     0     0
## 6                                    Moran, Mr. James   male  NA     0     0
##             Ticket    Fare Cabin Embarked
## 1        A/5 21171  7.2500              S
## 2         PC 17599 71.2833   C85        C
## 3 STON/O2. 3101282  7.9250              S
## 4           113803 53.1000  C123        S
## 5           373450  8.0500              S
## 6           330877  8.4583              Q</code></pre>
<pre class="r"><code>library(titanic)            # alternative!
titanic &lt;- titanic_train</code></pre>
<p>Our goal is to model the probability of surviving the titanic
disaster as a function of covariates like sex, age, number of siblings
or spouses on board, number of parents or children, passenger fare,
etc.</p>
<p>Let’s first build a simple logistic regression model for this
problem.</p>
<pre class="r"><code>titanic2 &lt;- na.omit(titanic)
model1 &lt;- glm(Survived ~ Sex + scale(Age) + scale(SibSp) + scale(Parch) + scale(Fare), data=titanic2, family=&quot;binomial&quot;)    #logistic regression
summary(model1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Sex + scale(Age) + scale(SibSp) + scale(Parch) + 
##     scale(Fare), family = &quot;binomial&quot;, data = titanic2)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5661  -0.6868  -0.5385   0.7323   2.3106  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    1.2071     0.1591   7.587 3.27e-14 ***
## Sexmale       -2.5334     0.2052 -12.348  &lt; 2e-16 ***
## scale(Age)    -0.3125     0.1043  -2.996 0.002739 ** 
## scale(SibSp)  -0.3796     0.1140  -3.330 0.000868 ***
## scale(Parch)  -0.1984     0.1001  -1.982 0.047523 *  
## scale(Fare)    0.9127     0.1663   5.487 4.08e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 695.26  on 708  degrees of freedom
## AIC: 707.26
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Alternatively, we could write our own likelihood function!!</p>
<pre class="r"><code>params &lt;- c(
  int=1,
  male = -1,
  age = 0,
  sibsp = 0,
  parch = 0,
  fare = 0
)

LikFunc &lt;- function(params){
  linear &lt;- params[&#39;int&#39;] + 
    params[&#39;male&#39;]*as.numeric(titanic2$Sex==&quot;male&quot;) +
    params[&#39;age&#39;]*scale(titanic2$Age) +
    params[&#39;sibsp&#39;]*scale(titanic2$SibSp) +
    params[&#39;parch&#39;]*scale(titanic2$Parch) +
    params[&#39;fare&#39;]*scale(titanic2$Fare)
  logitlinear &lt;-  1/(1+exp(-(linear)))
  -sum(dbinom(titanic2$Survived,size=1,prob = logitlinear,log=T))
}

LikFunc(params)</code></pre>
<pre><code>## [1] 459.757</code></pre>
<pre class="r"><code>MLE &lt;- optim(fn=LikFunc,par = params)

MLE$par</code></pre>
<pre><code>##        int       male        age      sibsp      parch       fare 
##  1.1988722 -2.5318874 -0.3174421 -0.3816480 -0.2047207  0.9109768</code></pre>
<p>Interesting- the results are very slightly different from the glm
results…! (I’m not sure why…)</p>
<div id="univariate-relationships" class="section level3">
<h3>Univariate relationships</h3>
<p>Let’s visualize the univariate relationships in this model! One way
to do this is to hold all other variables at their mean value…</p>
<pre class="r"><code>SibSp_range &lt;- range(titanic$SibSp)
Parch_range &lt;- range(titanic$Parch)
Fare_range &lt;- range(titanic$Fare)
Age_range &lt;- range(titanic$Age,na.rm = T)</code></pre>
<p>First, let’s look at fare</p>
<pre class="r"><code>### 

plot(titanic$Survived~titanic$Fare,pch=16,xlab=&quot;FARE ($)&quot;,ylab=&quot;Survived!&quot;)

predict_df &lt;- data.frame(
  Sex = &quot;male&quot;,
  Age = mean(titanic$Age,na.rm=T),
  SibSp = mean(titanic$SibSp),
  Parch = mean(titanic$Parch),
  Fare = seq(Fare_range[1],Fare_range[2])
)

probSurv &lt;- predict(model1,predict_df,type=&quot;response&quot;)

lines(seq(Fare_range[1],Fare_range[2]),probSurv)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Next, let’s look at Sex:</p>
<pre class="r"><code>### 

predict_df &lt;- data.frame(
  Sex = c(&quot;male&quot;,&quot;female&quot;),
  Age = mean(titanic$Age,na.rm=T),
  SibSp = mean(titanic$SibSp),
  Parch = mean(titanic$Parch),
  Fare = mean(titanic$Fare,na.rm=T)
)

tapply(titanic$Survived,titanic$Sex,mean)[2:1]</code></pre>
<pre><code>##      male    female 
## 0.1889081 0.7420382</code></pre>
<pre class="r"><code>probSurv &lt;- predict(model1,predict_df,type=&quot;response&quot;)
names(probSurv) &lt;- c(&quot;male&quot;,&quot;female&quot;)

probSurv</code></pre>
<pre><code>##      male    female 
## 0.2039291 0.7634165</code></pre>
<p>Now, let’s look at age:</p>
<pre class="r"><code>plot(titanic$Survived~titanic$Age,pch=16,xlab=&quot;AGE&quot;,ylab=&quot;Survived!&quot;)

predict_df &lt;- data.frame(
  Sex = &quot;male&quot;,
  Age = seq(Age_range[1],Age_range[2]),
  SibSp = mean(titanic$SibSp),
  Parch = mean(titanic$Parch),
  Fare = mean(titanic$Fare,na.rm=T)
)

probSurv &lt;- predict(model1,predict_df,type=&quot;response&quot;)

lines(seq(Age_range[1],Age_range[2]),probSurv)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Now, let’s look at number of siblings/spouses:</p>
<pre class="r"><code>### 

plot(titanic$Survived~titanic$SibSp,pch=16,xlab=&quot;# of Siblings/spouses&quot;,ylab=&quot;Survived!&quot;)

predict_df &lt;- data.frame(
  Sex = &quot;male&quot;,
  Age = mean(titanic$Age,na.rm=T),
  SibSp = seq(SibSp_range[1],SibSp_range[2],0.01),
  Parch = mean(titanic$Parch),
  Fare = mean(titanic$Fare,na.rm=T)
)

probSurv &lt;- predict(model1,predict_df,type=&quot;response&quot;)

lines(seq(SibSp_range[1],SibSp_range[2],0.01),probSurv)</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="performance-evaluation-validation" class="section level3">
<h3>Performance evaluation / validation</h3>
<p>Since we are interested in evaluating model performance, let’s see
how well the model predicts which people survived the titanic
disaster…</p>
<p>First of all, we will use cross-validation to evaluate model
predictive performance</p>
<p>BUT… since the response is binary, our model skill metrics don’t
really work too well!</p>
<p>For binary responses (0 or 1), the ROC curve (and area under the
curve statistic) is a good way of evaluating model performance!</p>
<p>First let’s prepare the workspace- load the packages!</p>
<pre class="r"><code>library(ROCR)
library(rms)</code></pre>
<pre class="r"><code>model1 &lt;- glm(Survived ~ Sex + SibSp + Parch + Fare, data=titanic, family=&quot;binomial&quot;)</code></pre>
<p>Then, we set the number of “folds” for cross-validation</p>
<pre class="r"><code># CROSS VALIDATION CODE FOR BINARY RESPONSE  ----------------------------

n.folds = 10       # set the number of &quot;folds&quot;
foldVector = rep(c(1:n.folds),times=floor(length(titanic$Survived)/9))[1:length(titanic$Survived)]</code></pre>
<p>Then, we do the cross validation, looping through each fold of the
data, leaving out each fold in turn for model training.</p>
<pre class="r"><code>CV_df &lt;- data.frame(
  CVprediction = numeric(nrow(titanic)),      # make a data frame for storage
  realprediction = 0,
  realdata = 0
)

for(i in 1:n.folds){
  fit_ndx &lt;- which(foldVector!=i)
  validate_ndx &lt;- which(foldVector==i)
  model &lt;- glm(formula = Survived ~ Sex + SibSp + Parch + Fare, family = &quot;binomial&quot;, data = titanic[fit_ndx,]) 
  CV_df$CVprediction[validate_ndx] &lt;-  plogis(predict(model,newdata=titanic[validate_ndx,])) 
  CV_df$realprediction[validate_ndx]  &lt;-  plogis(predict(model1,newdata=titanic[validate_ndx,]))
  CV_df$realdata[validate_ndx] &lt;- titanic$Survived[validate_ndx]
}

CV_RMSE = sqrt(mean((CV_df$realdata - CV_df$CVprediction)^2))       # root mean squared error for holdout samples in 10-fold cross-validation
real_RMSE = sqrt(mean((CV_df$realdata - CV_df$realprediction)^2))  # root mean squared error for residuals from final model

# print RMSE statistics

cat(&quot;The RMSE for the model under cross-validation is: &quot;, CV_RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## The RMSE for the model under cross-validation is:  0.3983029</code></pre>
<pre class="r"><code>cat(&quot;The RMSE for the model using all data for training is: &quot;, real_RMSE, &quot;\n&quot;)</code></pre>
<pre><code>## The RMSE for the model using all data for training is:  0.3937799</code></pre>
<p>However, RMSE doesn’t have much value for binary responses like this!
Here we use the ROC curve instead.</p>
</div>
<div id="aside-roc-curve-and-auc" class="section level3">
<h3>Aside: ROC curve and AUC</h3>
<p>ROC is a useful <em>threshold-independent</em> metric for evaluating
the performance of a binary classifier!</p>
<p>What does it mean to be threshold independent? It helps to look at
the data we have for evaluating model performance</p>
<pre class="r"><code>head(CV_df)</code></pre>
<pre><code>##   CVprediction realprediction realdata
## 1    0.1247408      0.1253180        0
## 2    0.8317980      0.8374769        1
## 3    0.7568756      0.7432668        1
## 4    0.7904207      0.7977907        1
## 5    0.1755321      0.1711690        0
## 6    0.1813810      0.1720213        0</code></pre>
<p>To effectively compare our predictions with the observation we have
to pick some threshold above which we call the prediction a “1” and
below which it is a zero.</p>
<p>A threshold-independent classifier like ROC considers model
performance across all possible thresholds.</p>
<p>Here are some example ROC curves:</p>
<p><img src="roccomp.jpg" /></p>
<p>The ROC curve is generated by plotting the true positive rate against
the false positive rate for all possible thresholds. For example, take
the extremes:</p>
<p>Imagine the threshold is zero, so we predict all outcomes are a 1;
that is, all people are predicted to survive the titanic disaster. In
this case, the true positive rate is exactly 1 (all those that survived
were predicted to survive), and the false positive rate (the rate at
which mortalities were incorrectly classified as survival) is also
1!</p>
<p>Imagine the opposite case: we predict everyone died. In this case,
the true positive rate is 0 (all those that survived were predicted to
not survive), as is the false positive rate (all mortalities are
correctly classified!)</p>
<p>Now imagine a threshold of around 0.5- we call everything above this
a “1”. Now we HOPE that the observations with predictions above 0.5 are
<em>enriched</em> in true positives relative to false positives. Maybe
we get 80% correct classification of survivors and say 10 incorrect
classification of mortalities (false positives).</p>
<p><strong>Q</strong> What would a perfect classifier look like?</p>
<p><strong>Q</strong> What would a classifier look like that performed
no better than random chance?</p>
<p><strong>Q</strong> What if the classifier performed worse than random
chance?</p>
<p>Now, let’s consider the meaning of the area under the ROC curve. This
is an important skill metric for a binary classifier!</p>
<p>Okay, back to the example! Let’s plot out the ROC curves!</p>
<pre class="r"><code>par(mfrow=c(2,1))
pred &lt;- prediction(CV_df$CVprediction,CV_df$realdata)     # for holdout samples in cross-validation
perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)
auc &lt;- performance(pred,&quot;auc&quot;)
plot(perf, main=&quot;Cross-validation&quot;)
text(.9,.1,paste(&quot;AUC = &quot;,round(auc@y.values[[1]],2),sep=&quot;&quot;))

pred &lt;- prediction(CV_df$realprediction,CV_df$realdata)     # for final model
perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)
auc &lt;- performance(pred,&quot;auc&quot;)
plot(perf, main=&quot;All data&quot;)
text(.9,.1,paste(&quot;AUC = &quot;,round(auc@y.values[[1]],2),sep=&quot;&quot;))</code></pre>
<p><img src="LECTURE9_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Finally, we can use the same pseudo-R-squared metric we learned above
as an alternative metric of performance</p>
<pre class="r"><code>CV_df$CVprediction[which(CV_df$CVprediction==1)] &lt;- 0.9999       # ensure that all predictions are not exactly 0 or 1
CV_df$CVprediction[which(CV_df$CVprediction==0)] &lt;- 0.0001
CV_df$realprediction[which(CV_df$realprediction==1)] &lt;- 0.9999
CV_df$realprediction[which(CV_df$realprediction==0)] &lt;- 0.0001

fit_deviance_CV &lt;- mean(-2*(dbinom(CV_df$realdata,1,CV_df$CVprediction,log=T)-dbinom(CV_df$realdata,1,CV_df$realdata,log=T)))
fit_deviance_real &lt;- mean(-2*(dbinom(CV_df$realdata,1,CV_df$realprediction,log=T)-dbinom(CV_df$realdata,1,CV_df$realdata,log=T)))
null_deviance &lt;- mean(-2*(dbinom(CV_df$realdata,1,mean(CV_df$realdata),log=T)-dbinom(CV_df$realdata,1,CV_df$realdata,log=T)))
deviance_explained_CV &lt;- (null_deviance-fit_deviance_CV)/null_deviance   # based on holdout samples
deviance_explained_real &lt;- (null_deviance-fit_deviance_real)/null_deviance   # based on full model...

# print RMSE statistics

cat(&quot;The McFadden R2 for the model under cross-validation is: &quot;, deviance_explained_CV, &quot;\n&quot;)</code></pre>
<pre><code>## The McFadden R2 for the model under cross-validation is:  0.2564415</code></pre>
<pre class="r"><code>cat(&quot;The McFadden R2 for the model using all data for training is: &quot;, deviance_explained_real, &quot;\n&quot;)</code></pre>
<pre><code>## The McFadden R2 for the model using all data for training is:  0.2760832</code></pre>
<p><a href="LECTURE10.html">–go to next lecture–</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
