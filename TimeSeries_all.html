<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Jack Tarricone, Mariana Webb, Tracy Shane" />


<title>Time Series</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LECTURE10.html">Machine Learning</a>
    </li>
    <li>
      <a href="TimeSeries_all.html">Time Series</a>
    </li>
    <li>
      <a href="spatial_autocorrelation.html">Spatial Autocorrelation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Time Series</h1>
<h4 class="author">Jack Tarricone, Mariana Webb, Tracy Shane</h4>

</div>


<p>Here is the download link for the R script for this lecture: <a href="TimeSeries_all.R">time series script</a></p>
<div id="import-packages" class="section level2">
<h2>Import Packages</h2>
<pre class="r"><code>library(rnoaa) # package for downloading NOAA, NCDC, and GHCN data
library(forecast) # time series forecasting
library(tseries) # useful functions and tests 
library(tsutils) # more tests
library(tidyverse)
library(lubridate) # date management
library(ggplot2)
library(dataRetrieval) # packaage for downloading USGS stream flow data
library(dplyr) # for data formatting 
library(trend) # for MK test and sen&#39;s slope
library(zyp) # y-intercept calc for sen&#39;s slope
library(changepoint)</code></pre>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="what-is-a-time-series" class="section level4">
<h4>What is a time series?</h4>
<p>A time series is a sequence of repeat measurements that are obtained over time. Analysis of time series data allows us to better understand how things are changing with respect to time, ranging from milliseconds to eons.</p>
<div class="figure">
<img src="Global_Temps.png" alt="" />
<p class="caption">This plot shows the global annual average temperature anomaly from 1860 to 2020.</p>
</div>
</div>
<div id="why-is-time-series-analysis-important" class="section level4">
<h4>Why is time series analysis important?</h4>
<p>Data collected near to the time that other data is collected is often correlated. We want to look at trends in data or for change points due to treatment or management. It is helpful for developing forecasting models.</p>
</div>
<div id="what-do-we-need-to-do-time-series-analysis" class="section level4">
<h4>What do we need to do time series analysis?</h4>
<ol style="list-style-type: decimal">
<li>A sequence of a minimum of 30 observations (but some say 60 observations)</li>
<li>A sequence that spans enough cycles of the data to model them accurately</li>
<li>A sequence that, if seasonal, contains enough seasons to model them accurately</li>
<li>Data collected from a representative sample of the population</li>
</ol>
</div>
<div id="time-series-terminology" class="section level4">
<h4>Time series terminology</h4>
<p><em>Discrete</em> vs. <em>Continuous</em>: a time series is discrete if measurements are taken at regularly-spaced intervals in time, continuous if measurements are taken continuously.</p>
<p><em>Stationarity</em> vs. <em>Non-Stationarity</em>: a time series is stationary if the mean and covariance of the data set stays constant over time. <img src="stationarity.png" alt="https://towardsdatascience.com/achieving-stationarity-with-time-series-data-abd59fd8d5a0" /></p>
<p><em>Lag</em>: shifting a time series <em>i</em> periods back.</p>
<p><em>Stochastic Processes</em>: a process defined by random variables, defined on the same probability space.</p>
<p><em>Forecasting</em>: making predictions about future time series behavior based on a model fit to the historic data.</p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>Time Series is represented as Y<sub>-1</sub>, Y<sub>0</sub>, [Y<sub>1</sub>, Y<sub>2</sub>, ….., Y<sub>N</sub>], Y<sub>N+1</sub>, ….</p>
<p>The observed time series in [Y<sub>1</sub>, Y<sub>2</sub>, ….., Y<sub>N</sub>] is the probability sample of the complete time series and is given weights as specified by the statistical model.</p>
<p>Full Model:</p>
<p>Y<sub>t</sub> = N(a<sub>t</sub>) + X(I<sub>t</sub>)</p>
<p>Where: at is the t th observation of a strictly exogenous innovation time series with a white noise property: a<sub>t</sub> ~ iid Normal(0, σ<sup>2</sup><sub>a</sub>) and, I<sub>t</sub> is the t<sup>th</sup> observation of a causal time series, usually in the format of a binary variable of presence or absence of a treatment, or it can be a purely stochastic time series.</p>
<p>Solved for a<sub>t</sub> with the white noise properties, we will often use the solved model for statistical tests:</p>
<p>a<sub>t</sub> = N<sup>-1</sup>[Y<sub>t</sub>- X(I<sub>t</sub>)]</p>
</div>
<div id="experiemental-designs" class="section level2">
<h2>Experiemental Designs</h2>
<p>Three main types of time-series experiments:</p>
<ol style="list-style-type: decimal">
<li>Descriptive
<ul>
<li>Observation of trends and cycles, interpretation of time series components</li>
<li>Precedes hypothesis testing</li>
<li>Unable to rule out alternative hypotheses</li>
<li>Often uninterpretable in terms of causation</li>
</ul></li>
<li>Correlational
<ul>
<li>Infer a causal relationship between two series from their covariance</li>
<li>Reliant on strong theoretical foundation, otherwise invalid</li>
<li>Usually limited to small segments of economics and psychology <img src="pirates.png" alt="https://www.datasciencecentral.com/profiles/blogs/hilarious-graphs-and-pirates-prove-that-correlation-is-not" /></li>
</ul></li>
<li>Experimental or Quasi-experimental
<ul>
<li>Interrupted Time Series Designs: tests latent causal effects due temporally discrete intervention or treatment. Divides the time series in the pre and post intervention segments to test hypotheses.</li>
</ul></li>
</ol>
</div>
</div>
<div id="getting-started-with-time-series-data" class="section level1">
<h1>Getting Started with Time Series Data</h1>
<div id="importing-time-series-data" class="section level2">
<h2>Importing Time Series Data</h2>
<p>For this example, we are using temperature and precipitation data from NOAA’s GHCN. We use the <code>rnoaa</code> package to download monthly data for the Reno Airport station for 80 years between 1940 and 2020.<br />
</p>
<p>Get an NCEL authentication key from <a href="https://www.ncdc.noaa.gov//cdo-web/token.\" class="uri">https://www.ncdc.noaa.gov//cdo-web/token.\</a></p>
<p>“PRCP” = Total Monthly Precipitation. Given in millimeters.<br />
“EMXP” = Extreme Maximum Precipitation. Given in millimeters.<br />
“TMIN” = Monthly Minimum Temperature. Average of daily minimum temperature given in Celsius.<br />
“TMAX” = Monthly Maximum Temperature. Average of daily maximum temperature given in Celsius.<br />
</p>
<p>For more information about the GHCN monthly data set: <a href="https://www.ncei.noaa.gov/pub/data/metadata/documents/GSOMReadme.txt" class="uri">https://www.ncei.noaa.gov/pub/data/metadata/documents/GSOMReadme.txt</a></p>
<pre class="r"><code>stationID &lt;- &#39;USW00023185&#39; # Reno airport (1937-03-01 to present)
dataset &lt;- &#39;GSOM&#39;   # Global Summary of the Month
climateVars &lt;- c(&#39;TMIN&#39;, &#39;TMAX&#39;, &#39;PRCP&#39;, &#39;EMXP&#39;) # select climate variables to download
startDate &lt;- as.Date(&#39;1940-01-01&#39;)
endDate &lt;- as.Date(&#39;2020-12-31&#39;)
GHCN &lt;- getGHCN(dataset, stationID, climateVars, startDate, endDate) # Call to download data using a wrapper function</code></pre>
<p>A great first step in time series analysis is to plot all the data to see what we’re working with.</p>
<pre class="r"><code>ggplot(GHCN, aes(as.Date(date), value, color = datatype)) +
  geom_point() + 
  facet_wrap(~datatype, scales = &#39;free&#39;) +
  scale_x_date(date_breaks = &#39;20 years&#39;, date_labels = &quot;%Y&quot;) +
  labs(x = &quot;&quot;, y = &quot;&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="r-time-series-objects" class="section level2">
<h2>R Time Series Objects</h2>
<p>The <code>ts</code> object is part of base r but other packages like <code>zoo</code> and <code>xts</code> provide additional ways to create and manage time series objects. A <code>ts</code> object is a very simple, taking a vector or matrix of the time series data values, a start and/or end time for the period of observations, and a frequency which indicates the number of observations per unit time. Let’s convert our minimum temperature time series into a <code>ts</code> object.</p>
<pre class="r"><code>tmin &lt;- GHCN %&gt;% 
  filter(datatype == &#39;TMIN&#39;) %&gt;%  # select only &#39;TMIN&#39; data from downloaded data frame
  select(value) # select only the data value column, we don&#39;t need the stationID, date, or datatype columns

# the ts() function converts to an r time series object
# input data is a vector or matrix of data
tmin.ts &lt;- ts(tmin, frequency = 12, start = c(1940, 1))

head(tmin.ts, n = 60) # look at the first 5 years of data</code></pre>
<pre><code>##        Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
## 1940 -4.85 -2.09 -2.69 -0.48  5.11  8.67  8.26  8.51  4.74  0.91 -5.25 -5.55
## 1941 -4.05 -2.09 -4.63 -2.00  3.35  6.16  9.75  8.08  1.92 -0.88 -4.06 -4.43
## 1942 -9.41 -4.72 -4.85 -0.45  1.68  4.72  8.62  7.74  2.85 -1.15 -3.74 -4.35
## 1943 -6.57 -3.96 -1.42  1.29  2.57  4.22  9.34  5.54  4.33 -0.10 -6.11 -8.38
## 1944 -6.75 -4.96 -4.98 -2.22  2.86  4.62  7.33  5.46  3.92 -0.18 -3.40 -7.81</code></pre>
<pre class="r"><code>plot.ts(tmin.ts, ylab = &quot;Min. Temp. (C)&quot;) # plot the time series</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># explore the time series object and other information it contains
start(tmin.ts)</code></pre>
<pre><code>## [1] 1940    1</code></pre>
<pre class="r"><code>end(tmin.ts)</code></pre>
<pre><code>## [1] 2020   12</code></pre>
<pre class="r"><code>frequency(tmin.ts)</code></pre>
<pre><code>## [1] 12</code></pre>
</div>
<div id="time-series-decomposition" class="section level2">
<h2>Time Series Decomposition</h2>
<p><strong>Decomposition</strong> is a way to parse out the different contributing signals of a time series. Frequently, decomposition is used to remove seasonality, making the long-term trend clearer. Decomposition can also be used to detrend a time series. The random signal in a time series can be used to identify outliers and anomalous data.<br />
</p>
<p>Before detrending, we need to determine if we’re working with an additive or multiplicative time series.<br />
</p>
<p><strong>Additive</strong> time series don’t change in magnitude over time.<br />
</p>
<p><strong>Multiplicative</strong> time series see an increase in seasonal magnitude over time.</p>
<div class="figure">
<img src="decomposition.png" alt="" />
<p class="caption"><a href="https://anomaly.io/seasonal-trend-decomposition-in-r/index.html" class="uri">https://anomaly.io/seasonal-trend-decomposition-in-r/index.html</a></p>
</div>
<p>Is the Reno Airport minimum temperature time series additive or multiplicative? The <code>mseastest()</code> function in the <code>tsutils</code> function provides a quick test for seasonality.</p>
<pre class="r"><code>plot.ts(tmin.ts, ylab = &quot;Min. Temp. (C)&quot;) # plot the time series</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>tsutils::mseastest(tmin.ts, outplot = 1)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<pre><code>## $is.multiplicative
## [1] TRUE
## 
## $statistic
## [1] 0.5326024
## 
## $pvalue
## [1] 1.544398e-07</code></pre>
<p>Looks like this time series is multiplicative! Let’s decompose the plot multiplicatively.</p>
<pre class="r"><code>tminDecompose &lt;- decompose(tmin.ts, type = &quot;multi&quot;)
plot(tminDecompose)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The observed plot represents the original observed values for monthly minimum temperature at the Reno Airport.<br />
</p>
<p>The trend plot shows the underlying trend of the data. We can see here a positive trend in the data, suggesting increasing temperatures over the course of the 80 year data period.<br />
</p>
<p>The seasonal plot shows patterns that repeat at a regular interval. In this case, the temperature data shows strong seasonailty with lower values in the winter months and higher values in the summer months.<br />
</p>
<p>The random plot shows the residuals of the time series after the trend and seasonal parts are removed.</p>
</div>
<div id="time-series-adjustments" class="section level2">
<h2>Time Series Adjustments</h2>
<p>If the seasonality of the temperature is not of interest, we can simply subtract the seasonality from the original time series to get a seasonally-adjusted time series.</p>
<pre class="r"><code>tminSeasonAdj &lt;- tmin.ts - tminDecompose$seasonal

plot.ts(tminSeasonAdj, 
        main = &quot;Seasonally-Adjusted Minimum Monthly Temperature&quot;,
        ylab = &quot;Min. Temp. (C)&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>In other cases, such as running a regression on two time series, we might want to remove the trend from a non-stationary time series. To remove the trend from the dataset, we can simply subtract the trend from the original time series.</p>
<pre class="r"><code>tminTrendAdj &lt;- tmin.ts - tminDecompose$trend

plot.ts(tminTrendAdj, 
        main = &quot;Detrended Minimum Monthly Temperature&quot;,
        ylab = &quot;Min. Temp. (C)&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="time-series-forecasting-and-forecasting-objects" class="section level2">
<h2>Time Series Forecasting and Forecasting Objects</h2>
<p>Another object type in r relevant to time series is the <code>forecast</code> object. A <code>forecast</code> object contains prediction intervals, fitted values, residuals, information about the method and data used to forecast. There are two types of forecast models. In the first, a function directly produces a <code>forecast</code> object. In the second, we first fit a model to the data and only then use a function to forecast from that model.<br />
</p>
<p>Here, we will do a simple forecast using a Seasonal and Trend Decompostion using Loess Forecasting Model (STLF) and the naive method, which uses the most recent values of the time series to project the time series into the future. Other possible methods include ETS, ARIMA, and Random Walk Drift.</p>
<pre class="r"><code>tminForecast &lt;- forecast::stlf(tmin.ts, method = &quot;naive&quot;)

plot(tminForecast, 
        main = &quot;Minimum Monthly Temperature with Forecasting through 2022&quot;,
        ylab = &quot;Min. Temp. (C)&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
</div>
<div id="hydroclimatic-trend-analysis" class="section level1">
<h1>Hydroclimatic Trend Analysis</h1>
<p>For this section, we’ll work through a trend analysis using USGS streamflow data. We’ll start by downloading the data directly into R, formatting the date information, and then calculating hydrologic statistics which will be tested for significant trends. This type of analysis is common in hydrologic and climatologic science.</p>
<div class="figure">
<img src="yuba.png" alt="" />
<p class="caption">This map shows the location on the gauge, with the drainage area shaded in grey.</p>
</div>
<p>This gauge was selected because there are no significant diversions or damns in the drainage basin above. Therefore, any trends we may find should be related to changes in the hydroclimatology of the area.</p>
<pre class="r"><code># identifying parameters for data download
siteNumber &lt;- &quot;11413000&quot; # USGS gauge number
parameterCd &lt;- &quot;00060&quot;  # mean daily discharge in cfs
startDate &lt;- &quot;1940-10-01&quot; # starting date bc missing few years in the 30&#39;s
endDate &lt;- &quot;2021-11-01&quot; # date today</code></pre>
<p>We’ll use the <code>dataRetrevial</code> package to download the daily stream gauge data into R. To select a different gauge, simply look up the site number and download.</p>
<pre class="r"><code># download data using readNWISdv function
yuba_raw &lt;- dataRetrieval::readNWISdv(siteNumber, parameterCd, startDate, endDate)
head(yuba_raw) # check</code></pre>
<pre><code>##   agency_cd  site_no       Date X_00060_00003 X_00060_00003_cd
## 1      USGS 11413000 1940-10-01           148                A
## 2      USGS 11413000 1940-10-02           148                A
## 3      USGS 11413000 1940-10-03           160                A
## 4      USGS 11413000 1940-10-04           157                A
## 5      USGS 11413000 1940-10-05           152                A
## 6      USGS 11413000 1940-10-06           147                A</code></pre>
<pre class="r"><code>colnames(yuba_raw)[4] &lt;-&quot;discharge_cfs&quot; # rename column 4
yuba_raw &lt;-yuba_raw[,-5] # delete unused col</code></pre>
<div id="date-formatting" class="section level4">
<h4>Date Formatting</h4>
<p>When conducting time series analysis in R, it’s important to have your date information formatted so that you find it easy to work with. We will create additional columns using <code>lubridate</code> for time information to help filter and group the data for further analysis.</p>
<pre class="r"><code>class(yuba_raw$Date) # check it&#39;s actually a data not a character string</code></pre>
<pre><code>## [1] &quot;Date&quot;</code></pre>
<pre class="r"><code>yuba_raw$year &lt;-lubridate::year(yuba_raw$Date) # add year col
yuba_raw$month &lt;-lubridate::month(yuba_raw$Date) # add month col
yuba_raw$day &lt;-lubridate::day(yuba_raw$Date) # add day of month col
yuba_raw$doy &lt;-lubridate::yday(yuba_raw$Date) # add day of year col</code></pre>
<p>A water year (WY) is a standard time metric for evaluating streamflow and snowpack data in hydrology. It starts on October 1st and ends on September 30th of the following year. The water year is defined by year in which the September 30th date occurs. WY 2022 began on October 1st of last month!</p>
<pre class="r"><code>yuba_raw &lt;-dataRetrieval::addWaterYear(yuba_raw)
head(yuba_raw) # check</code></pre>
<pre><code>##   agency_cd  site_no       Date waterYear discharge_cfs year month day doy
## 1      USGS 11413000 1940-10-01      1941           148 1940    10   1 275
## 2      USGS 11413000 1940-10-02      1941           148 1940    10   2 276
## 3      USGS 11413000 1940-10-03      1941           160 1940    10   3 277
## 4      USGS 11413000 1940-10-04      1941           157 1940    10   4 278
## 5      USGS 11413000 1940-10-05      1941           152 1940    10   5 279
## 6      USGS 11413000 1940-10-06      1941           147 1940    10   6 280</code></pre>
<p>Now using <code>dplyr</code> we’ll group by water year, and create a column that is a sequence or count of days begining on October 1st. This will create a metric for day of water year (dowy), account for leap and non-leap years correctly.</p>
<pre class="r"><code>yuba &lt;- as.data.frame(dplyr::group_by(yuba_raw,waterYear) 
                      %&gt;% mutate(dowy = seq(1:n())))
head(yuba)</code></pre>
<pre><code>##   agency_cd  site_no       Date waterYear discharge_cfs year month day doy dowy
## 1      USGS 11413000 1940-10-01      1941           148 1940    10   1 275    1
## 2      USGS 11413000 1940-10-02      1941           148 1940    10   2 276    2
## 3      USGS 11413000 1940-10-03      1941           160 1940    10   3 277    3
## 4      USGS 11413000 1940-10-04      1941           157 1940    10   4 278    4
## 5      USGS 11413000 1940-10-05      1941           152 1940    10   5 279    5
## 6      USGS 11413000 1940-10-06      1941           147 1940    10   6 280    6</code></pre>
<pre class="r"><code>tail(yuba)</code></pre>
<pre><code>##       agency_cd  site_no       Date waterYear discharge_cfs year month day doy
## 29612      USGS 11413000 2021-10-27      2022           640 2021    10  27 300
## 29613      USGS 11413000 2021-10-28      2022           627 2021    10  28 301
## 29614      USGS 11413000 2021-10-29      2022           522 2021    10  29 302
## 29615      USGS 11413000 2021-10-30      2022           461 2021    10  30 303
## 29616      USGS 11413000 2021-10-31      2022           441 2021    10  31 304
## 29617      USGS 11413000 2021-11-01      2022           397 2021    11   1 305
##       dowy
## 29612   27
## 29613   28
## 29614   29
## 29615   30
## 29616   31
## 29617   32</code></pre>
<p>Let’s plot the full daily time series using <code>ggplot2</code> Dr. Shoemaker’s favorite plotting package!</p>
<pre class="r"><code># set theme for plot, changes background to white
ggplot2::theme_set(theme_light(base_size =11))

# plot
ggplot(yuba, aes(y = discharge_cfs, x = Date))+
  geom_line(size = .3) + # change line width
  labs(title=&quot;Yuba near Goodyears Bar,CA Discharge&quot;, 
       y=&quot;Discharge (cfs)&quot;, x=&quot;Date&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Plotting just WY 2021 with the added extra month, notice the spike from the recent AR event!</p>
<pre class="r"><code># filter the date down to the 2021 + 1 month water year and plot
yuba_wy2021 &lt;-dplyr::filter(yuba, Date &gt;= as.Date(&quot;2020-10-01&quot;) &amp; Date &lt;= as.Date(&quot;2021-11-01&quot;))

# plot wy 2021
ggplot(yuba_wy2021, aes(y = discharge_cfs, x = Date))+
  geom_line(size = .7, col = &quot;darkblue&quot;) + # change width, set color
  labs(title=&quot;Yuba near Goodyears Bar,CA Discharge WY 2021&quot;, 
       y=&quot;Discharge (cfs)&quot;, x=&quot;Date&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="streamflow-metrics" class="section level4">
<h4>Streamflow Metrics</h4>
<p>In the Sierra Nevada, the vast majority of the water flows from the mountains during the spring snowmelt season. Water managers and scientists must predict the timing and magnitude of peak streamflow to allocate the resource properly for a range of different purposes, including ecological benefit.</p>
<pre class="r"><code># create a dataframe of maximum daily discharge per year
wy_max &lt;-as.data.frame(yuba %&gt;%
  group_by(waterYear) %&gt;%
  filter(discharge_cfs == max(discharge_cfs, na.rm=TRUE)))

head(wy_max)            </code></pre>
<pre><code>##   agency_cd  site_no       Date waterYear discharge_cfs year month day doy dowy
## 1      USGS 11413000 1940-12-27      1941          5240 1940    12  27 362   88
## 2      USGS 11413000 1942-01-27      1942          7640 1942     1  27  27  119
## 3      USGS 11413000 1943-01-21      1943          9580 1943     1  21  21  113
## 4      USGS 11413000 1944-05-08      1944          2510 1944     5   8 129  221
## 5      USGS 11413000 1945-02-02      1945          6670 1945     2   2  33  125
## 6      USGS 11413000 1945-12-29      1946          5730 1945    12  29 363   90</code></pre>
<pre class="r"><code># max day of water year
ggplot(wy_max, aes(y = dowy, x = Date)) +
  geom_point(col = &quot;goldenrod&quot;)+
  geom_smooth(method = &quot;lm&quot;, se = FALSE)+ # add lm trend line
  labs(title=&quot;Yuba Max Annual Discharge DOWY&quot;, 
       y=&quot;Day of Water Year&quot;, x=&quot;Date&quot;)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code># max discharge cfs
ggplot(wy_max, aes(y = discharge_cfs, x = Date)) +
  geom_point()+
  geom_smooth(method = &quot;lm&quot;, se = FALSE)+ # add lm trend line
  labs(title=&quot;Yuba Max Annual Discharge DOWY Discharge&quot;, 
       y=&quot;Discharge (cfs)&quot;, x=&quot;Date&quot;)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
<p>A bit tangential, but let’s calculate the maximum observed flow for the whole time series.</p>
<pre class="r"><code>max &lt;-dplyr::filter(yuba, discharge_cfs == max(discharge_cfs, na.rm=TRUE))
print(max)</code></pre>
<pre><code>##   agency_cd  site_no       Date waterYear discharge_cfs year month day doy dowy
## 1      USGS 11413000 1997-01-02      1997         29600 1997     1   2   2   94</code></pre>
<p>This was a massive rain-on-snow AR event that caused flows much larger than last month! More information <a href="https://www.cnrfc.noaa.gov/storm_summaries/jan1997storms.php">here</a></p>
</div>
<div id="calculating-the-onset-of-spring-flows" class="section level4">
<h4>Calculating the Onset of Spring Flows</h4>
<p><a href="https://journals.ametsoc.org/view/journals/clim/18/8/jcli3321.1.xml">Stewart et al. (2004)</a> showed that the onset or time of spring stream flow in the Sierra is changing. Using the newly formatted data with the additional date information, we can to create a function that calculates date of spring flow onset in DOWY. I won’t go into detail of how, and am not 100% confident it’s correct, but let’s assume it is for the purposes of this exercise.</p>
<pre class="r"><code>melt_onset_dowy &lt;-function(df){
  
  # crop days until until spring and early summer  
  wy_spring &lt;-filter(df, dowy &gt; 100 &amp; dowy &lt; 250) # crop days until until spring and early summer
  mean_spring &lt;-mean(wy_spring$discharge_cfs) # find meann discharge of spring date
  above_mean_days &lt;-wy_spring$dowy[which(wy_spring$discharge_cfs &gt; mean_spring)] # find dates which discharge &gt; mean
  diff_vect &lt;-diff(above_mean_days)
  results &lt;-rle(diff_vect)
  meets &lt;-as.numeric(min(which(results$lengths == max(results$lengths))))
  values_to_cut &lt;-results$lengths[1:(meets-1)]
  #sum the lengths
  index_values_until_start &lt;-as.numeric(sum(values_to_cut))
  #subtract that many values off to get your SDD
  onset_dowy &lt;-as.integer(min(above_mean_days[-c(1:index_values_until_start)]))
  return(onset_dowy)
}</code></pre>
<p>Now we can apply this function to each water year and create a dataframe of water year’s and date of spring flow onset</p>
<pre class="r"><code># create sequence of available years
years &lt;-as.matrix(seq(min(yuba$waterYear),2021,1))

# create an empty matrix to read the data into
loop_mat &lt;-matrix(nrow = length(years))

# filter data to stop at WY 2021
yuba_loop &lt;-dplyr::filter(yuba, Date &gt;= as.Date(&quot;1940-10-01&quot;) &amp; Date &lt;= as.Date(&quot;2021-09-30&quot;))
tail(yuba_loop)</code></pre>
<pre><code>##       agency_cd  site_no       Date waterYear discharge_cfs year month day doy
## 29580      USGS 11413000 2021-09-25      2021          74.5 2021     9  25 268
## 29581      USGS 11413000 2021-09-26      2021          74.2 2021     9  26 269
## 29582      USGS 11413000 2021-09-27      2021          75.6 2021     9  27 270
## 29583      USGS 11413000 2021-09-28      2021          94.2 2021     9  28 271
## 29584      USGS 11413000 2021-09-29      2021          83.0 2021     9  29 272
## 29585      USGS 11413000 2021-09-30      2021          80.1 2021     9  30 273
##       dowy
## 29580  360
## 29581  361
## 29582  362
## 29583  363
## 29584  364
## 29585  365</code></pre>
<pre class="r"><code># loop the function for calculating melt onset through all the way years
for (i in 1:length(years)){
  
  yearly_df &lt;-subset(yuba_loop, waterYear == years[i]) # sequence through years to create yearly df
  loop_mat[i] &lt;-melt_onset_dowy(yearly_df) # apply function and sequentially save in empty matrix

}

# bind years vector and new melt onset dates
melt_onset_df &lt;-as.data.frame(cbind(years, loop_mat))
colnames(melt_onset_df)[1] &lt;-&quot;waterYear&quot;
colnames(melt_onset_df)[2] &lt;-&quot;spring_flow_dowy&quot;
tail(melt_onset_df) # check</code></pre>
<pre><code>##    waterYear spring_flow_dowy
## 76      2016              157
## 77      2017              215
## 78      2018              172
## 79      2019              184
## 80      2020              188
## 81      2021              180</code></pre>
<p>Quick test plot of the spring flow onset metric we just created, with a linear model laid on top.</p>
<pre class="r"><code># plot the new data and add a &quot;trend line&quot;, just visually
# seems to trend towards early flow onset
ggplot(melt_onset_df, aes(y = spring_flow_dowy, x = waterYear)) +
  geom_point(col = &quot;red&quot;)+
  geom_smooth(method = &quot;lm&quot;, se = FALSE) + 
  labs(title=&quot;Yuba Spring Flow Onset Date&quot;, 
       y=&quot;Day of Water year&quot;, x=&quot;Water Year&quot;)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>A few test annual hydrographs to make sure the function is working properly.</p>
<pre class="r"><code>yuba_wy2021 &lt;-filter(yuba, waterYear == 2021) # subset wy 2021
melt_date &lt;-melt_onset_df$spring_flow_dowy[which(melt_onset_df$waterYear == &quot;2021&quot;)]

# for wy2021
ggplot(yuba_wy2021, aes(y = discharge_cfs, x = dowy))+
  geom_line(size = .7, col = &quot;green&quot;) + # change width, set color
  geom_vline(xintercept = melt_date, col = &quot;red&quot;) + # date calculated
  labs(title=&quot;Yuba near Goodyears Bar,CA Discharge WY 2021&quot;, 
       y=&quot;Discharge (cfs)&quot;, x=&quot;Date&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code># 1970 has the earliest calculated date of 105 or January 15th!
min &lt;-min(melt_onset_df$spring_flow_dowy) 
yuba_wy1970 &lt;-filter(yuba, waterYear == 1970) # low year

# plot to check our function is being reasonable here
# and it is, shows the variability in the hydroclimatic system
ggplot(yuba_wy1970, aes(y = discharge_cfs, x = dowy))+
  geom_line(size = .7, col = &quot;cyan&quot;) + # change width, set color
  geom_vline(xintercept = min, col = &quot;red&quot;) + # date calculated
  labs(title=&quot;Yuba near Goodyears Bar,CA Discharge WY 1970&quot;, 
       y=&quot;Discharge (cfs)&quot;, x=&quot;Date&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
</div>
<div id="mann-kendallsens-slope-trend-analysis" class="section level4">
<h4>Mann-Kendall/Sen’s Slope Trend Analysis</h4>
<p>We estimated the trend line on the above spring flow timing plot from a linear regression. This statistical test assumes the data a normal distribution, which is rarely the case in hydrology or many earth science fields. We’ll turn to the Mann-Kendall (MK) test, which does not require a normal distribution. We’ll then estimate the trend’s slope using the Thiel-Sen estimator or Sen’s Slope.</p>
<pre class="r"><code># test the data for normality
hist(melt_onset_df$spring_flow_dowy, breaks = 30)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code># but let&#39;s use a Shapiro test to check for real
shapiro.test(melt_onset_df$spring_flow_dowy)  </code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  melt_onset_df$spring_flow_dowy
## W = 0.92861, p-value = 0.0002267</code></pre>
<p>P-value is below .05, therefore we reject the null hypothesis of a normal distrubution.</p>
<pre class="r"><code># We can turned to the Mann-Kendall trend test, which is non-parametric, not requiring a normal dist
# this only tells us if there is a significant trend, not the estimated slope
mk_results &lt;-trend::mk.test(melt_onset_df$spring_flow_dowy)
print(mk_results) # significant</code></pre>
<pre><code>## 
##  Mann-Kendall trend test
## 
## data:  melt_onset_df$spring_flow_dowy
## z = -2.2237, n = 81, p-value = 0.02617
## alternative hypothesis: true S is not equal to 0
## sample estimates:
##             S          varS           tau 
##  -546.0000000 60070.0000000    -0.1695685</code></pre>
<pre class="r"><code># for a slope estimate we uses the Sen&#39;s slope function
sens_result &lt;-trend::sens.slope(melt_onset_df$spring_flow_dowy)
print(sens_result)</code></pre>
<pre><code>## 
##  Sen&#39;s slope
## 
## data:  melt_onset_df$spring_flow_dowy
## z = -2.2237, n = 81, p-value = 0.02617
## alternative hypothesis: true z is not equal to 0
## 95 percent confidence interval:
##  -0.48148148 -0.02040816
## sample estimates:
## Sen&#39;s slope 
##  -0.2400926</code></pre>
<pre class="r"><code>slope_est &lt;-as.numeric(sens_result[[1]]) # slope estimate

# the trend package only gives us slope, not a y-int estimate for plotting
# we&#39;ll use the zyp package for that
y_int_results &lt;-zyp::zyp.sen(spring_flow_dowy ~ waterYear, melt_onset_df) # y comes first!!
print(y_int_results) # inspect</code></pre>
<pre><code>## 
## Call:
## NULL
## 
## Coefficients:
## Intercept  waterYear  
##  677.7401    -0.2401</code></pre>
<pre class="r"><code>y_int &lt;-y_int_results$coefficients[[1]] # pull out y-int estimate for ploting</code></pre>
<p>Plot the data with the significant trend</p>
<pre class="r"><code># not let&#39;s plot the data with our MK estimate trend, instead of lm one from ggplot
ggplot(melt_onset_df, aes(y = spring_flow_dowy, x = waterYear)) +
  geom_point(col = &quot;red&quot;)+
  # geom_smooth(method = &quot;lm&quot;, se = FALSE) + 
  # use info from our MK and Sen&#39;s test to graph the trend line
  geom_abline(intercept = y_int, slope = slope_est, color=&quot;blue&quot;, size=1.2) +
  labs(title=&quot;Yuba Spring Flow Onset Date MK Trend&quot;, 
       y=&quot;Day of Water year&quot;, x=&quot;Water Year&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>We see a significant trend of 2.4 days/decade earlier spring streamflow!</p>
</div>
</div>
<div id="second-streamflow-analysis-example" class="section level1">
<h1>Second Streamflow Analysis Example</h1>
<p>For this section, we’ll also work through a trend analysis using USGS streamflow data, using the same procedures as Jack has already shown you. With this dataset I will demonstrate other options for forecasting with an ARIMA model and performing change point detection.</p>
<div class="figure">
<img src="martin.png" alt="" />
<p class="caption">This map shows the location on the Martin Creek gauge, just north and east of Paradise Valley, Nevada (green marker).</p>
</div>
<p>This gauge was selected because two fires (2011, 2018) have impacted the watershed recently and there could be an opportunity to see changes in the trend or seasonality of the data.</p>
<p>We’ll use the same procedures already shown via the <code>dataRetrevial</code> package to download daily stream gauge data.</p>
<p>First, we will visualize the time series. Conveniently, there are 100 years of data for this gauge.</p>
<pre class="r"><code># quick test plot to visualize the time series
plot(martin$Date, martin$discharge_cfs, type = &quot;l&quot;,
     main = &quot;Martin Creek Discharge,NV&quot;,
     ylab = &quot;Discharge (cfs)&quot;,
     xlab = &quot;Date&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Next we will convert the average daily cfs recordings to a time series object and take a look at the summary of the data.</p>
<pre class="r"><code>df_day_ts &lt;- ts(data = martin$discharge_cfs, frequency = 365.25, start = c(1921,10), end= c(2021,10))
summary(df_day_ts)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    2.00    7.00   10.00   34.05   32.00 2500.00</code></pre>
<div id="time-series-decomposition-1" class="section level4">
<h4>Time Series Decomposition</h4>
<p>Now we’ll proceed with decomposing the data to visualize the trend, seasonality, and residuals. In order to decompose, we need to select either an additive or multiplicative model. In this case, the data fits an additive model. If you aren’t sure which case your data falls into, you can calculate the sums of squares of the autocorrelation function for the additive decomposition and the multiplicative decomposition of the data to see which model gives you the smallest sum of squares value. In this case the additive decomposition achieves the smallest sum of squares.</p>
<pre class="r"><code>#decompose the data both ways
ddata &lt;- decompose(df_day_ts, type = &quot;additive&quot;)
ddatam &lt;- decompose(df_day_ts, type = &quot;multiplicative&quot;)
plot(ddata) #additive</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre class="r"><code>plot(ddatam) #multiplicative</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-30-2.png" width="672" /></p>
<pre class="r"><code>#compare which model has smaller sums of squares for the ACF of the residuals
add_info &lt;- sum(acf(ddata$random, lag.max = length(ddata$random), na.action = na.omit)$acf^2)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-30-3.png" width="672" /></p>
<pre class="r"><code>mult_info &lt;- sum(acf(ddatam$random, lag.max = length(ddatam$random), na.action = na.omit)$acf^2)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-30-4.png" width="672" /></p>
<pre class="r"><code>add_info</code></pre>
<pre><code>## [1] 14.77871</code></pre>
<pre class="r"><code>#mult_info
#we will use ddata for the remainder of the demo</code></pre>
<p>Let’s visualize the decomposed data more closely by looking just at the trend component.</p>
<pre class="r"><code>#Now we visualize our decomposed data and our trend component
#plot(ddata)
plot(ddata$trend)
abline(reg=lm(ddata$trend~time(ddata$trend)), col=&quot;red&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>We see we might have a positive trend and we might have seasonality to our data (as we would expect).</p>
</div>
<div id="testing-time-series-forecasing-assumptions" class="section level4">
<h4>Testing Time Series Forecasing Assumptions</h4>
<p>In order to build time series forecasting models, the data has to meed the assumptions of the model. Forecasting models, whether ARMA (autoregressive moving average) or ARIMA (autoregressive integrated moving average), rely on data that is well-behaved: stationary (no trend), has uniform variance, and is independent, or doesn’t exhibit strong temporal autocorrelation.</p>
<p>We can perform several tests to check these assumptions about our data before building a forecasting model.</p>
<pre class="r"><code># examine the data for normality
hist(ddata$trend, breaks = 30)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre class="r"><code># try autocorrelation function (ACF) (Autoregressive process) and the partial autocorrelation function (PACF) (Moving Average process) to examine the data
acf(df_day_ts,lag.max = length(df_day_ts),
    xlab = &quot;lag #&quot;, ylab = &#39;ACF&#39;,main=&#39;ACF results in very gradual attenuation&#39;)  #possible non-stationarry</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
<pre class="r"><code>pacf(df_day_ts, lag.max = length(df_day_ts), xlab = &quot;lag #&quot;, ylab = &#39;PACF&#39;,main=&#39;PACF results in rapid dampening&#39;) #possible stationary</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-32-3.png" width="672" /></p>
<pre class="r"><code>#Ljung-Box test for independence
#the Ljung-Box test examines whether there is significant evidence for non-zero 
#correlations at given lags. If p-value &gt;0.05, we accept the null hypothesis of independence.
lag.length = 25
options(warn=-1)
Box.test(df_day_ts, lag=lag.length, type=&quot;Ljung-Box&quot;)</code></pre>
<pre><code>## 
##  Box-Ljung test
## 
## data:  df_day_ts
## X-squared = 314519, df = 25, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code>#p-value suggests we should reject the null and assume data is not independent

#Another test we can conduct is the Augmented Dickey–Fuller (ADF) t-statistic 
#test to find if the series has a unit root. Non-stationary data will have a large p-value.
tseries::adf.test(df_day_ts)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  df_day_ts
## Dickey-Fuller = -16.584, Lag order = 33, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>#this test suggests that we might have stationary data

#A fourth test we can use to check assumptions is to check for stationarity or trend is the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test.
tseries::kpss.test(df_day_ts, null=&quot;Level&quot;) #results in p-value = 0.01, reject null</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  df_day_ts
## KPSS Level = 0.7853, Truncation lag parameter = 17, p-value = 0.01</code></pre>
<pre class="r"><code>#this test suggests that our data might not be stationary</code></pre>
</div>
<div id="testing-for-differencing-needed-to-remove-trend-andor-seasonality" class="section level4">
<h4>Testing For Differencing Needed to Remove Trend and/or Seasonality</h4>
<p>We can use the forecast package to test how many lag differences we may need to do in order for our data to become stationary.</p>
<pre class="r"><code>forecast::ndiffs(df_day_ts)  # number of differences need to make it stationary</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>forecast::nsdiffs(df_day_ts) # number for seasonal differencing needed</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>#if we needed to difference (as some of our tests tell us we might):
stationaryCFS &lt;- diff(df_day_ts, differences=1)
#examine differenced data
plot(stationaryCFS, type=&quot;l&quot;, main=&quot;Differenced and Stationary&quot;)  # appears to be stationary</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>hist(stationaryCFS)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-33-2.png" width="672" /></p>
<p>And we can verify if our differencing procedure above worked or not.</p>
<pre class="r"><code>lag.length = 25
options(warn=-1)
Box.test(stationaryCFS, lag=lag.length, type=&quot;Ljung-Box&quot;) #still autocorrelated</code></pre>
<pre><code>## 
##  Box-Ljung test
## 
## data:  stationaryCFS
## X-squared = 2148.7, df = 25, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code>tseries::adf.test(stationaryCFS) #stationary</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  stationaryCFS
## Dickey-Fuller = -42.236, Lag order = 33, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>tseries::kpss.test(stationaryCFS) #stationary</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  stationaryCFS
## KPSS Level = 0.00059564, Truncation lag parameter = 17, p-value = 0.1</code></pre>
<pre class="r"><code>#Our data might not meet the normality assumption and it is still autocorrelated. We may need to mean-center our data to reduce problems with forecasting and reduce problems due to temporal autocorrelation</code></pre>
</div>
<div id="arima-model-development" class="section level4">
<h4>ARIMA Model Development</h4>
<p>Now that we are fairly certain our data at least meets the stationary assumption, we will go ahead and fit an ARIMA model using the auto.arima function. We are going to box-cox transform the data to reduce the effects of our non-normal distribution on fitting the model.</p>
<pre class="r"><code>options(warn=-1)
mymodel &lt;- forecast::auto.arima(stationaryCFS, lambda = &quot;auto&quot;, parallel = TRUE, num.cores = 8)
mymodel</code></pre>
<pre><code>## Series: stationaryCFS 
## ARIMA(3,0,1) with zero mean 
## Box Cox transformation: lambda= 1.246594 
## 
## Coefficients:
##          ar1     ar2      ar3      ma1
##       0.4472  0.0292  -0.0172  -0.8746
## s.e.  0.0070  0.0061   0.0059   0.0047
## 
## sigma^2 estimated as 17385:  log likelihood=-230128.6
## AIC=460267.1   AICc=460267.1   BIC=460309.7</code></pre>
</div>
<div id="forecasting-using-an-arima-model" class="section level4">
<h4>Forecasting using an ARIMA model</h4>
<p>Now we can use the tools in the forecast package to forecast our stream gauge data into the future.</p>
<pre class="r"><code>#let&#39;s try forecasting now that we have an ARIMA model built
fit &lt;- forecast::Arima(df_day_ts, c(3,0,1)) #ARIMA model shows three coefficients for the AR component, 0 seasonal differences, and 1 coefficient for the MA component
plot(forecast::forecast(fit))</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
<div id="change-point-detection" class="section level4">
<h4>Change Point Detection</h4>
<p>Well that is a pretty poor forecast, but what if what we really want to know is if the fires that happened in 2011 and 2018 might have a large enough effect to cause a change in trend or a change in mean run-off, causing an increase in CFS at the gauge.</p>
<p>Let’s examine how to detect if change points might exist using the <code>changepoint</code> package.</p>
<pre class="r"><code>#we write a function that will help us look for the correct penalty value to set in our change point analysis
cptfn &lt;- function(data, pen) {
  ans &lt;- changepoint::cpt.mean(data, test.stat=&quot;Normal&quot;, method = &quot;PELT&quot;, penalty = &quot;Manual&quot;, pen.value = pen) 
  length(cpts(ans)) +1
}
# run our cptfn function expecting there is a signal with a known change point
pen.vals &lt;- seq(0, 12,.2) #set some penalty values to use in the elbow plot
elbowplotData &lt;- unlist(lapply(pen.vals, function(p) 
  cptfn(data = df_day_ts, pen = p))) #apply our function on the data
#and now we plot the penalty parameters as a function of the time series
plot(pen.vals,elbowplotData, 
     xlab = &quot;PELT penalty parameter&quot;,
     ylab = &quot; &quot;,
     main = &quot; &quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<pre class="r"><code>#look at the graphs, especially the PELT Penalty Parameter to determine
#where the change point might be

penalty.val &lt;- 6 # this value is determined from elbow plots

#next we use cpt.mean and the SegNeigh method with a max number of change points set to 4 to detect change points in the data
options(warn=-1)
cptm_data &lt;- changepoint::cpt.mean(df_day_ts, penalty=&#39;Manual&#39;,pen.value=penalty.val,method=&#39;SegNeigh&#39;, Q=4)

cpts_dat &lt;- changepoint::cpts(cptm_data) # change point time points
cpts_dat #gives us the time points</code></pre>
<pre><code>## [1] 22415 22842 22897</code></pre>
<p>We really want to visualize the data. It is much easier to see the change point means on the decomposed trend data than it is to see it on the original data, so we will perform the above for the decomposed trend data as well and see what differences occur.</p>
<pre class="r"><code>trend_narm &lt;- na.omit(ddata$trend)  #decomposed trend data without the NA&#39;s
options(warn=-1)
cptm_trend &lt;- changepoint::cpt.mean(trend_narm, penalty=&#39;Manual&#39;,pen.value=penalty.val,method=&#39;SegNeigh&#39;, Q=4) 
cpts_trnd &lt;- changepoint::cpts(cptm_trend) # change point time points
cpts_trnd #gives us the time points</code></pre>
<pre><code>## [1]  5684 22150 22873</code></pre>
<pre class="r"><code>plot(cptm_data,
     xlab = &quot;Time&quot;,
     ylab = &quot;Avg. Daily Discharge (cfs)&quot;,
     main = &quot;Change in Mean Signal&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<pre class="r"><code>plot(cptm_trend,
     xlab = &quot;Time&quot;,
     ylab = &quot;Decomposed Trend of CFS&quot;,
     main = &quot;Change in Mean Signal&quot;)</code></pre>
<p><img src="TimeSeries_all_files/figure-html/unnamed-chunk-38-2.png" width="672" /></p>
<p>This concludes the three demonstrations of how to work with time series data. We hope that you now have some ideas on how to tackle time series analysis with your own datasets.</p>
</div>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<ol style="list-style-type: decimal">
<li>Time series analysis is a huge topic with a wide array of uses.</li>
<li>Time series data can be analyzed using a variety of different packages in R.</li>
<li>Much of the time series literature focuses on econometrics and forecasting which is often not relevant to ecologists/hydrologists.</li>
<li>Many researchers fail to properly explore the nature of their time series data and subsequently get their papers rejected.</li>
<li>We’re living in a no-analog, non-stationary time. Previous resource-management methods assumed stationarity and we now have to adapt.</li>
</ol>
<p><img src="StationarityDead.png" /></p>
<div id="references-and-resources" class="section level2">
<h2>References and Resources</h2>
<p>Coghlan, A. A Little Book of R For Time Series. Release 0.2. Sep 10, 2018. <a href="https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html" class="uri">https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html</a>. Accessed on 11/1/2021.</p>
<p>Hyndman, R.J. and G. Athanasopoulos. Forecasting: Principles and Practice. 2nd Ed. OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on 11/1/2021.</p>
<p>McCleary, R., D. McDowall, B.J. Bartos. Design and Analysis of Time Series Experiments. Oxford University Press, New York, NY. 2017. 359 p.</p>
<p>Perry, J.N, R.H. Smith, I.P. Wolwod, and D.R. Morse, Ed. Chaos in Real Data: the analysis of non-linear dynamics from short ecological time series. Kluwer Academic Publishers, Dordrecht, Netherlands. 2000. 218 p.</p>
<p>Yaffee, R. and M. McGee. Introduction to Time Series Analysis and Forecasting with Applications of SAS and SPSS. Academic Press. San Diego, CA. 2000. 511 p.</p>
<div id="useful-weblinks-for-time-series-analysis" class="section level4">
<h4>Useful Weblinks for Time Series Analysis</h4>
<p><a href="https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/" class="uri">https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/</a></p>
<p><a href="http://r-statistics.co/Time-Series-Analysis-With-R.html" class="uri">http://r-statistics.co/Time-Series-Analysis-With-R.html</a></p>
<p><a href="https://www.r-bloggers.com/2017/02/is-my-time-series-additive-or-multiplicative/" class="uri">https://www.r-bloggers.com/2017/02/is-my-time-series-additive-or-multiplicative/</a></p>
<p><a href="https://rcompanion.org/handbook/F_13.html" class="uri">https://rcompanion.org/handbook/F_13.html</a> #Mann-Kendall non-parametric test</p>
<p><a href="http://rpubs.com/richkt/269797" class="uri">http://rpubs.com/richkt/269797</a> #Stationarity testing</p>
<p><a href="https://otexts.com/fpp2/" class="uri">https://otexts.com/fpp2/</a> # Forecasting Principles and Practice, 2 ed.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
