<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-27" />

<title>Optimization!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The virtual ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimization!</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 27, 2016</em></h4>

</div>


<p>We can’t maximize a likelihood function without an optimization algorithm.</p>
<p>We can’t optimize a management plan, as in the power analysis problem, without an optimization algorithm.</p>
<p>Clearly, we need optimization algorithms!! In addition, they provide an excellent example of how computers (often via brute force algorithms) have overtaken mathematics for doing statistics.</p>
<p>You may not have built your own optimization algorithm before, but you have probably taken advantage of optimization algorithms. For example, if you have performed a glm or a non-linear regression in R, you have exploited numerical optimization algorithms!</p>
<p>We will discuss optimization in the context of maximum likelihood estimation. Let’s start with the most simple of all optimization algorithms:</p>
<div id="brute-force" class="section level2">
<h2>Brute Force</h2>
<p>Just like we did for the two-dimensional likelihood surface, we could evaluate the likelihood at tiny intervals across a broad range of parameter values. Then we can just identify the parameter set that produces the maximum likelihood across all evaluated parameter sets.</p>
<div id="positives" class="section level3">
<h3>Positives</h3>
<ul>
<li>Simple!! (Conceptually very straightforward)</li>
<li>Identify false peaks!</li>
<li>Undeterred by discontinuities in the likelihood surface</li>
</ul>
</div>
<div id="negatives" class="section level3">
<h3>Negatives</h3>
<ul>
<li>Speed: even slower and less efficient than a typical ecologist is willing to accept!</li>
<li>Resolution: we may specify the wrong interval size. Even so, we can only get the answer to within plus or minus the interval size.</li>
</ul>
</div>
<div id="example-dataset-myxomatosis-titer-in-rabbits" class="section level3">
<h3>Example dataset: Myxomatosis titer in rabbits</h3>
<p>Let’s use Bolker’s myxomatosis example dataset to illustrate our optimization issues:</p>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>For this example, we are modeling the distribution of measured titers (virus loads) for Australian rabbits. Bolker chose to use a Gamma distribution. Here is the empirical distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We need to estimate the gamma rate and shape parameters that best fit this empirical distribution. Here is one example of a Gamma fit to this distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Let’s build a likelihood function for this problem!</p>
<pre class="r"><code>GammaLikelihoodFunction &lt;- function(params){
  sum(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=T))
}

params &lt;- c(40,0.15) 
names(params) &lt;- c(&quot;shape&quot;,&quot;scale&quot;)
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] -49.58983</code></pre>
<p>Now let’s optimize using ‘optim’ like we did before, to find the MLE!</p>
<pre class="r"><code>ctrl &lt;- list(fnscale=-1)   # maximize rather than minimize!!
MLE &lt;- optim(fn=GammaLikelihoodFunction,par=params,control=ctrl,method=&quot;BFGS&quot;)</code></pre>
<pre><code>## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced</code></pre>
<pre class="r"><code>MLE$par</code></pre>
<pre><code>##      shape      scale 
## 49.3666607  0.1402629</code></pre>
<p>Let’s visualize the fit of the MLE in this case…</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=MLE$par[&quot;shape&quot;],scale=MLE$par[&quot;scale&quot;]),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Okay, now what if we want to try optimizing with the brute force method…</p>
<pre class="r"><code>##############
# define 2-D parameter space!
##############

shapevec &lt;- seq(10,100,by=0.1)   
scalevec &lt;- seq(0.01,0.3,by=0.001)

##############
# define the likelihood surface across this grid within parameter space
##############

surface &lt;- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  for(j in 1:length(scalevec)){
    newparams[&#39;scale&#39;] &lt;- scalevec[j]
    surface[i,j] &lt;- GammaLikelihoodFunction(newparams) 
  }
}

############
# Visualize the likelihood surface
############

image(x=shapevec,y=scalevec,z=surface,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface,levels=c(-30,-40,-80,-500),add=T)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Now what is the maximum likelihood estimate?</p>
<pre class="r"><code>ndx &lt;- which(surface==max(surface),arr.ind=T)
shapevec[ndx[,1]]</code></pre>
<pre><code>## [1] 49.8</code></pre>
<pre class="r"><code>scalevec[ndx[,2]]</code></pre>
<pre><code>## [1] 0.139</code></pre>
</div>
</div>
<div id="derivative-based-methods" class="section level2">
<h2>Derivative based methods!</h2>
<p>If we assume that the likelihood surface is smooth and has only one minimum, we can develop very efficient optimization algorithms. In general, derivative based methods look for the point in parameter space where the derivative of the likelihood function is zero. That is, the peak!</p>
<p>Let’s imagine we are interested in determining the shape parameter, given a known scale parameter. To use derivative based methods, let’s first build a function that estimates the slope of the function at any arbtrary point in parameter space:</p>
<pre class="r"><code>params &lt;- MLE$par
SlopeFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- GammaLikelihoodFunction(params+c(tiny,0))
  low &lt;- GammaLikelihoodFunction(params-c(tiny,0))
  slope &lt;- (high-low)/(tiny*2)
  return(slope)
}

SlopeFunc(shape_guess=30)</code></pre>
<pre><code>## [1] 13.62666</code></pre>
<p>Now let’s visualize this!</p>
<pre class="r"><code>shapevec &lt;- seq(10,100,by=0.1)   

##############
# define the likelihood surface
##############

surface &lt;- numeric(length(shapevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  surface[i] &lt;- GammaLikelihoodFunction(newparams) 
}

plot(surface~shapevec,type=&quot;l&quot;)
point &lt;- GammaLikelihoodFunction(c(shape=30,MLE$par[&#39;scale&#39;]))
slope &lt;- SlopeFunc(shape_guess=30)
lines(c(20,40),c(point-slope*10,point+slope*10),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We also need a function to compute the second derivative, or the curvature…</p>
<pre class="r"><code>params &lt;- MLE$par
CurvatureFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- SlopeFunc(shape_guess+tiny)
  low &lt;- SlopeFunc(shape_guess-tiny)
  curvature &lt;- (high-low)/(tiny*2)
  return(curvature)
}

CurvatureFunc(shape_guess=30)</code></pre>
<pre><code>## [1] -0.9151666</code></pre>
<p>Okay, now we can implement a derivative-based optimization algorithm!</p>
<p>Essentially, we are trying to find the point where the derivative of the likelihood function is zero (the root of the function!).</p>
<p>The simplest derivative based optimization algorithm is the Newton-Raphson algorithm. Here is the pseudocode:</p>
<ul>
<li>pick a guess for a parameter value</li>
<li>compute the derivative of the likelihood function for that guess</li>
<li>compute the slope of the derivative (curvature) of the likelihood function for that guess</li>
<li>Extrapolate linearly to try to find the root (where the derivative of the likelihood function should be zero if the slope of the likelihood function were linear)</li>
<li>repeat until the derivative of the likelihood function is close enough to zero (within a specified tolerance)</li>
</ul>
<p>Let’s first visualize the shape of the first derivative of the likelihood function</p>
<pre class="r"><code>firstderiv &lt;- numeric(length(shapevec))   # initialize storage variable

for(i in 1:length(shapevec)){
  firstderiv[i] &lt;- SlopeFunc(shapevec[i]) 
}

plot(firstderiv~shapevec,type=&quot;l&quot;)
abline(h=0,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Let’s use the Newton method to find the root. First we pick a starting value. Say we pick 80.</p>
<p>First compute the derivatives:</p>
<pre class="r"><code>firstderiv &lt;- SlopeFunc(80)
secondderiv &lt;- CurvatureFunc(80)
firstderiv</code></pre>
<pre><code>## [1] -13.13913</code></pre>
<pre class="r"><code>secondderiv</code></pre>
<pre><code>## [1] -0.3396182</code></pre>
<p>Now let’s use this linear function to extrapolate to where the first derivative is equal to zero:</p>
<pre class="r"><code>oldguess &lt;- 80
newguess &lt;- oldguess - firstderiv/secondderiv
newguess</code></pre>
<pre><code>## [1] 41.31206</code></pre>
<p>Our new guess is that the shape parameter is 41.31. Let’s do it again!</p>
<pre class="r"><code>oldguess &lt;- 41.31
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess) 
newguess</code></pre>
<pre><code>## [1] 48.66339</code></pre>
<p>Okay, we’re already getting close to our MLE of around 49.36. Let’s do it again:</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36237</code></pre>
<p>And again!</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36746</code></pre>
<p>And again!!!</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36746</code></pre>
<p>Wow, in just a few steps we already basically found the root. Let’s find the root for real, using an algorithm…</p>
<pre class="r"><code>NewtonMethod &lt;- function(firstguess,tolerance=0.0000001){
  deriv &lt;- SlopeFunc(firstguess)
  oldguess &lt;- firstguess
  counter &lt;- 0
  while(abs(deriv)&gt;tolerance){
    oldguess&lt;-newguess
    deriv &lt;- SlopeFunc(oldguess)
    newguess &lt;- oldguess - deriv/CurvatureFunc(oldguess)
    counter=counter+1
  }
  mle &lt;- list()
  mle$estimate &lt;- newguess
  mle$likelihood &lt;- GammaLikelihoodFunction(c(shape=newguess,MLE$par[&#39;scale&#39;]))
  mle$iterations &lt;- counter
  return(mle)
}


newMLE &lt;- NewtonMethod(firstguess=80)
newMLE</code></pre>
<pre><code>## $estimate
## [1] 49.36746
## 
## $likelihood
## [1] -37.6673
## 
## $iterations
## [1] 1</code></pre>
<p>Hopefully this illustrates the power of optimization algorithms!!</p>
</div>
<div id="simplex-method" class="section level2">
<h2>Simplex method</h2>
</div>
<div id="simulated-annealing" class="section level2">
<h2>Simulated annealing!!!</h2>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
