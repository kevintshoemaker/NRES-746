<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-27" />

<title>Optimization!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood and optimization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Generalized Additive Models (GAMs).pdf">GAMs</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimization!</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 27, 2016</em></h4>

</div>


<p>We can’t maximize a likelihood function without an optimization algorithm.</p>
<p>We can’t optimize a sampling or monitoring regime, as in the power analysis problem, without an optimization algorithm.</p>
<p>Clearly, we need optimization algorithms!! In addition, they provide an excellent example of how computers (often via brute force algorithms) have superseded pure mathematics for performing statistical analysis.</p>
<p>You may not have built your own optimization algorithm before, but you have probably taken advantage of optimization algorithms. For example, if you have performed a glm or a non-linear regression in R, you have exploited numerical optimization algorithms!</p>
<p>We will discuss optimization in the context of maximum likelihood estimation. Let’s start with the most simple of all optimization algorithms:</p>
<div id="brute-force" class="section level2">
<h2>Brute Force</h2>
<p>Just like we did for the two-dimensional likelihood surface, we could evaluate the likelihood at tiny intervals across a broad range of parameter values. Then we can just identify the parameter set that produces the maximum likelihood across all evaluated parameter sets.</p>
<div id="positives" class="section level3">
<h3>Positives</h3>
<ul>
<li>Simple!! (Conceptually very straightforward)</li>
<li>Identify false peaks!</li>
<li>Undeterred by discontinuities in the likelihood surface</li>
</ul>
</div>
<div id="negatives" class="section level3">
<h3>Negatives</h3>
<ul>
<li>Speed: even slower and less efficient than a typical ecologist is willing to accept!</li>
<li>Resolution: we may specify the wrong interval size. Even so, we can only get the answer to within plus or minus the interval size.</li>
</ul>
</div>
<div id="example-dataset-myxomatosis-titer-in-rabbits" class="section level3">
<h3>Example dataset: Myxomatosis titer in rabbits</h3>
<p>Let’s use Bolker’s myxomatosis example dataset to illustrate our optimization issues:</p>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>For this example, we are modeling the distribution of measured titers (virus loads) for Australian rabbits. Bolker chose to use a Gamma distribution. Here is the empirical distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We need to estimate the gamma rate and shape parameters that best fit this empirical distribution. Here is one example of a Gamma fit to this distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Let’s build a likelihood function for this problem!</p>
<pre class="r"><code>GammaLikelihoodFunction &lt;- function(params){
  sum(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=T))
}

params &lt;- c(40,0.15) 
names(params) &lt;- c(&quot;shape&quot;,&quot;scale&quot;)
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] -49.58983</code></pre>
<p>Now let’s optimize using ‘optim’ like we did before, to find the MLE!</p>
<pre class="r"><code>ctrl &lt;- list(fnscale=-1)   # maximize rather than minimize!!
MLE &lt;- optim(fn=GammaLikelihoodFunction,par=params,control=ctrl,method=&quot;BFGS&quot;)</code></pre>
<pre><code>## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced</code></pre>
<pre class="r"><code>MLE$par</code></pre>
<pre><code>##      shape      scale 
## 49.3666607  0.1402629</code></pre>
<p>Let’s visualize the fit of the MLE in this case…</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=MLE$par[&quot;shape&quot;],scale=MLE$par[&quot;scale&quot;]),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Okay, now what if we want to try optimizing with the brute force method…</p>
<pre class="r"><code>##############
# define 2-D parameter space!
##############

shapevec &lt;- seq(10,100,by=0.1)   
scalevec &lt;- seq(0.01,0.3,by=0.001)

##############
# define the likelihood surface across this grid within parameter space
##############

surface2D &lt;- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  for(j in 1:length(scalevec)){
    newparams[&#39;scale&#39;] &lt;- scalevec[j]
    surface2D[i,j] &lt;- GammaLikelihoodFunction(newparams) 
  }
}

############
# Visualize the likelihood surface
############

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Now what is the maximum likelihood estimate?</p>
<pre class="r"><code>ndx &lt;- which(surface==max(surface),arr.ind=T)
shapevec[ndx[,1]]</code></pre>
<pre><code>## [1] 49.8</code></pre>
<pre class="r"><code>scalevec[ndx[,2]]</code></pre>
<pre><code>## [1] 0.139</code></pre>
</div>
</div>
<div id="derivative-based-methods" class="section level2">
<h2>Derivative based methods!</h2>
<p>If we assume that the likelihood surface is smooth and has only one minimum, we can develop very efficient optimization algorithms. In general, derivative based methods look for the point in parameter space where the derivative of the likelihood function is zero. That is, the peak!</p>
<p>Let’s imagine we are interested in determining the shape parameter, given a known scale parameter. To use derivative based methods, let’s first build a function that estimates the slope of the function at any arbtrary point in parameter space:</p>
<pre class="r"><code>params &lt;- MLE$par
SlopeFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- GammaLikelihoodFunction(params+c(tiny,0))
  low &lt;- GammaLikelihoodFunction(params-c(tiny,0))
  slope &lt;- (high-low)/(tiny*2)
  return(slope)
}

SlopeFunc(shape_guess=30)</code></pre>
<pre><code>## [1] 13.62666</code></pre>
<p>Now let’s visualize this!</p>
<pre class="r"><code>shapevec &lt;- seq(10,100,by=0.1)   

##############
# define the likelihood surface
##############

surface1D &lt;- numeric(length(shapevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  surface1D[i] &lt;- GammaLikelihoodFunction(newparams) 
}

plot(surface1D~shapevec,type=&quot;l&quot;)
point &lt;- GammaLikelihoodFunction(c(shape=30,MLE$par[&#39;scale&#39;]))
slope &lt;- SlopeFunc(shape_guess=30)
lines(c(20,40),c(point-slope*10,point+slope*10),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We also need a function to compute the second derivative, or the curvature…</p>
<pre class="r"><code>params &lt;- MLE$par
CurvatureFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- SlopeFunc(shape_guess+tiny)
  low &lt;- SlopeFunc(shape_guess-tiny)
  curvature &lt;- (high-low)/(tiny*2)
  return(curvature)
}

CurvatureFunc(shape_guess=30)</code></pre>
<pre><code>## [1] -0.9151666</code></pre>
<p>Okay, now we can implement a derivative-based optimization algorithm!</p>
<p>Essentially, we are trying to find the point where the derivative of the likelihood function is zero (the root of the function!).</p>
<p>The simplest derivative based optimization algorithm is the Newton-Raphson algorithm. Here is the pseudocode:</p>
<ul>
<li>pick a guess for a parameter value</li>
<li>compute the derivative of the likelihood function for that guess</li>
<li>compute the slope of the derivative (curvature) of the likelihood function for that guess</li>
<li>Extrapolate linearly to try to find the root (where the derivative of the likelihood function should be zero if the slope of the likelihood function were linear)</li>
<li>repeat until the derivative of the likelihood function is close enough to zero (within a specified tolerance)</li>
</ul>
<p>Let’s first visualize the shape of the first derivative of the likelihood function</p>
<pre class="r"><code>firstderiv &lt;- numeric(length(shapevec))   # initialize storage variable

for(i in 1:length(shapevec)){
  firstderiv[i] &lt;- SlopeFunc(shapevec[i]) 
}

plot(firstderiv~shapevec,type=&quot;l&quot;)
abline(h=0,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Let’s use the Newton method to find the root. First we pick a starting value. Say we pick 80.</p>
<p>First compute the derivatives:</p>
<pre class="r"><code>firstderiv &lt;- SlopeFunc(80)
secondderiv &lt;- CurvatureFunc(80)
firstderiv</code></pre>
<pre><code>## [1] -13.13913</code></pre>
<pre class="r"><code>secondderiv</code></pre>
<pre><code>## [1] -0.3396182</code></pre>
<p>Now let’s use this linear function to extrapolate to where the first derivative is equal to zero:</p>
<pre class="r"><code>oldguess &lt;- 80
newguess &lt;- oldguess - firstderiv/secondderiv
newguess</code></pre>
<pre><code>## [1] 41.31206</code></pre>
<p>Our new guess is that the shape parameter is 41.31. Let’s do it again!</p>
<pre class="r"><code>oldguess &lt;- 41.31
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess) 
newguess</code></pre>
<pre><code>## [1] 48.66339</code></pre>
<p>Okay, we’re already getting close to our MLE of around 49.36. Let’s do it again:</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36237</code></pre>
<p>And again!</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36746</code></pre>
<p>And again!!!</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36746</code></pre>
<p>Wow, in just a few steps we already basically found the root. Let’s find the root for real, using an algorithm…</p>
<pre class="r"><code>NewtonMethod &lt;- function(firstguess,tolerance=0.0000001){
  deriv &lt;- SlopeFunc(firstguess)
  oldguess &lt;- firstguess
  counter &lt;- 0
  while(abs(deriv)&gt;tolerance){
    deriv &lt;- SlopeFunc(oldguess)
    newguess &lt;- oldguess - deriv/CurvatureFunc(oldguess)
    oldguess&lt;-newguess
    counter=counter+1
  }
  mle &lt;- list()
  mle$estimate &lt;- newguess
  mle$likelihood &lt;- GammaLikelihoodFunction(c(shape=newguess,MLE$par[&#39;scale&#39;]))
  mle$iterations &lt;- counter
  return(mle)
}


newMLE &lt;- NewtonMethod(firstguess=80)
newMLE</code></pre>
<pre><code>## $estimate
## [1] 49.36746
## 
## $likelihood
## [1] -37.6673
## 
## $iterations
## [1] 6</code></pre>
<p>Hopefully this illustrates the power of optimization algorithms!!</p>
<p>Note that this method and other derivative-based methods can work in multiple dimensions:</p>
</div>
<div id="derivative-free-optimization-methods" class="section level2">
<h2>Derivative-free optimization methods</h2>
<p>Derivative-free methods make no assumption about smoothness. In some ways, they represent a middle ground between the brute force method and the elegant but finnicky derivative-based methods, representing a balance between simplicity and generality.</p>
</div>
<div id="derivative-free-method-1-simplex-method" class="section level2">
<h2>Derivative-free method 1: simplex method</h2>
<p>This is the default opimization method for “optim”! That means that R used this method for optimizing the fuel economy example from the likelihood lecture.</p>
<div id="definition-simplex" class="section level4">
<h4>Definition: Simplex</h4>
<p>A <em>simplex</em> is the multi-dimensional analog of the triangle. In two dimensions, the triangle is the simplest shape possible. It has one more vertex than there are dimensions! In <em>n</em> dimensions, a simplex is defined by <em>n+1</em> vertices.</p>
</div>
<div id="pseudocode-for-nelder-mead-simplex-algorithm" class="section level4">
<h4>Pseudocode for Nelder-Mead simplex algorithm</h4>
<p>Set up an initial simplex in parameter space (often based on a user’s initial guess).</p>
<p>Continue the following steps until your answer is good enough:</p>
<blockquote>
<ul>
<li>Start by identifying the <em>worst</em> vertex (the one with the lowest likelihood)<br />
</li>
<li>Take the worst vertex and reflect it across the shape represented by the other vertices.<br />
</li>
<li>If the likelihood is higher for the reflected point, double the length of the jump!<br />
</li>
<li>If this jump was bad (lower likelihood) then try a point that’s only half as far out as the initial try.<br />
</li>
<li>If this jump was also bad, then contract the simplex around the current highest-likelihood vertex.</li>
</ul>
</blockquote>
<p><strong>Q</strong>: What does the simplex look like for a one-dimensional optimization problem?</p>
<p><strong>Q</strong>: Is this method likely to be good at avoiding false peaks in the likelihood surface?</p>
</div>
<div id="example-simplex-method" class="section level3">
<h3>Example: Simplex method</h3>
<p><strong>Step 1:</strong> Set up an initial simplex in parameter space</p>
<pre class="r"><code>firstguess &lt;- c(shape=40,scale=0.25)   # &quot;user&quot; first guess   # shape=80,scale=0.212

simplex &lt;- list()
 
           # set up the initial simplex based on the first guess...
simplex[[&#39;vertex1&#39;]] &lt;- firstguess + c(5,0.05)
simplex[[&#39;vertex2&#39;]] &lt;- firstguess + c(-5,-0.05)
simplex[[&#39;vertex3&#39;]] &lt;- firstguess + c(5,-0.05)

simplex</code></pre>
<pre><code>## $vertex1
## shape scale 
##  45.0   0.3 
## 
## $vertex2
## shape scale 
##  35.0   0.2 
## 
## $vertex3
## shape scale 
##  45.0   0.2</code></pre>
<p>Let’s plot the simplex…</p>
<pre class="r"><code>    ## first let&#39;s make a function to plot the simplex on a 2-D likelihood surface...

addSimplex &lt;- function(simplex,col=&quot;red&quot;){
  temp &lt;- as.data.frame(simplex)    # easier to work with data frame here
  points(x=temp[1,c(1,2,3,1)], y=temp[2,c(1,2,3,1)],type=&quot;b&quot;,lwd=2,col=col)
}

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
addSimplex(simplex)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Now let’s evaluate the log likelihood at each vertex</p>
<pre class="r"><code>SimplexLik &lt;- function(simplex){
  newvec &lt;- unlist(lapply(simplex,GammaLikelihoodFunction))   # note use of apply instead of for loop...
  return(newvec)
}

SimplexLik(simplex)</code></pre>
<pre><code>##    vertex1    vertex2    vertex3 
## -257.13414  -38.46251  -76.07737</code></pre>
<p>Now let’s develop a function to implement our first move through parameter space, according to the rules defined above…</p>
<pre class="r"><code>#####
# Helper Functions
#####

## this function relects the worst vertex across the remaining vector
ReflectIt &lt;- function(oldsimplex,WorstVertex){
  
    ## re-arrange simplex- worst must be first
  worstndx &lt;- which(names(oldsimplex)==WorstVertex)
  otherndx &lt;- c(1:3)[-worstndx]
  newndx &lt;- c(worstndx,otherndx) 
  
    ## translate so that vertex 2 is the origin (0,0)
  translate &lt;- oldsimplex[[newndx[2]]]
  newsimplex &lt;- list(oldsimplex[[1]]-translate,oldsimplex[[2]]-translate,oldsimplex[[3]]-translate)
  
    ## use vector reflection (reflect vertex 1 over a vector containing the origin and vertex 3) to find the reflection across a vector that includes the origin
  vdotl &lt;- sum(newsimplex[[newndx[1]]]*newsimplex[[newndx[3]]])
  ldotl &lt;- sum(newsimplex[[newndx[3]]]*newsimplex[[newndx[3]]])
  
  projection &lt;- (vdotl/ldotl)*newsimplex[[newndx[3]]]
  
  reflected &lt;- 2*projection-newsimplex[[newndx[1]]]
  
    ## translate back to the likelihood surface
  newsimplex[[newndx[1]]] &lt;- reflected
  newsimplex &lt;- list(newsimplex[[1]]+translate,newsimplex[[2]]+translate,newsimplex[[3]]+translate)
    ## return the new simplex
  names(newsimplex) &lt;- names(oldsimplex)
  
    ## generate some alternative jumps (or &quot;oozes&quot;!)...
  oldpoint &lt;- oldsimplex[[worstndx]]
  newpoint &lt;- newsimplex[[worstndx]]
  
  newpoint2 &lt;- newpoint-oldpoint
  double &lt;- newpoint2 * 2
  half &lt;- newpoint2 * 0.25
  
  alternates &lt;- list()
  alternates$reflected &lt;- newsimplex
  alternates$double &lt;- newsimplex 
  alternates$half &lt;- newsimplex 
  alternates$double[[worstndx]] &lt;- double + oldpoint
  alternates$half[[worstndx]] &lt;- half + oldpoint
  return(alternates)
}


ShrinkIt &lt;- function(oldsimplex,BestVertex){
  newsimplex &lt;- oldsimplex
  
      ## indices...
  bestndx &lt;- which(names(oldsimplex)==BestVertex)
  otherndx &lt;- c(1:3)[-bestndx]
  
  translate &lt;- oldsimplex[[bestndx]]
  
  i=2
  for(i in otherndx){
    newvector &lt;- oldsimplex[[i]]-translate
    shrinkvector &lt;- newvector * 0.5
    newsimplex[[i]] &lt;- shrinkvector + translate
  }
  
  return(newsimplex)
}


MoveTheSimplex &lt;- function(oldsimplex){     # incomplete nelder-mead
  newsimplex &lt;- oldsimplex  # 
           # Start by identifying the *worst* vertex (the one with the lowest likelihood)
  VertexLik &lt;- SimplexLik(newsimplex)
  WorstLik &lt;- min(VertexLik)
  WorstVertex &lt;- names(VertexLik[which.min(VertexLik)])    # identify vertex with lowest likelihood
  candidates &lt;- ReflectIt(oldsimplex=newsimplex,WorstVertex)      # reflect across the remaining edge
  CandidateLik &lt;- sapply(candidates,SimplexLik)                          # re-evaluate likelihood at the vertices...
  CandidateLik &lt;- apply(CandidateLik,c(1,2), function(t) ifelse(is.nan(t),-99999,t))
  bestCandidate &lt;- names(which.max(CandidateLik[WorstVertex,]))
  bestCandidateLik &lt;- CandidateLik[WorstVertex,bestCandidate]
  if(bestCandidateLik&gt;=WorstLik){
    newsimplex &lt;- candidates[[bestCandidate]]
  } else{
    BestVertex &lt;- names(VertexLik[which.max(VertexLik)])
    newsimplex &lt;- ShrinkIt(oldsimplex,BestVertex)
  }
  return(newsimplex)
}

oldsimplex &lt;- simplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)</code></pre>
<pre><code>## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced</code></pre>
<pre class="r"><code>image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
addSimplex(oldsimplex,col=&quot;green&quot;)
addSimplex(newsimplex)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Let’s try another few moves</p>
<pre class="r"><code>oldsimplex &lt;- newsimplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
addSimplex(oldsimplex,col=&quot;green&quot;)
addSimplex(newsimplex)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>And again!</p>
<pre class="r"><code>oldsimplex &lt;- newsimplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
addSimplex(oldsimplex,col=&quot;green&quot;)
addSimplex(newsimplex)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Again:</p>
<pre class="r"><code>oldsimplex &lt;- newsimplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
addSimplex(oldsimplex,col=&quot;green&quot;)
addSimplex(newsimplex)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>And another few times:</p>
<pre class="r"><code>par(mfrow=c(2,2))

for(i in 1:4){
  oldsimplex &lt;- newsimplex
  newsimplex &lt;- MoveTheSimplex(oldsimplex)
  
  image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
  contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
  addSimplex(oldsimplex,col=&quot;green&quot;)
  addSimplex(newsimplex)
}</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Now we can build a function and use the algorithm for optimizing!</p>
<pre class="r"><code>SimplexMethod &lt;- function(firstguess,tolerance=0.000001){
  initsimplex &lt;- list()
  initsimplex[[&#39;vertex1&#39;]] &lt;- firstguess + c(5,0.05)
  initsimplex[[&#39;vertex2&#39;]] &lt;- firstguess + c(-5,-0.05)
  initsimplex[[&#39;vertex3&#39;]] &lt;- firstguess + c(5,-0.05)
  VertexLik &lt;- SimplexLik(initsimplex)
  oldbestlik &lt;- VertexLik[which.max(VertexLik)]
  deltalik &lt;- 100
  counter &lt;- 0
  while(counter&lt;100){
    newsimplex &lt;- MoveTheSimplex(oldsimplex)
    VertexLik &lt;- SimplexLik(newsimplex)
    bestlik &lt;- VertexLik[which.max(VertexLik)]
    deltalik &lt;- bestlik-oldbestlik
    oldsimplex &lt;- newsimplex
    oldbestlik &lt;- bestlik
    counter &lt;- counter+1
  }
  mle &lt;- list()
  mle$estimate &lt;- newsimplex[[1]]
  mle$likelihood &lt;- bestlik
  mle$iterations &lt;- counter
  return(mle)
}


SimplexMethod(firstguess = c(shape=39,scale=0.28))</code></pre>
<pre><code>## $estimate
##      shape      scale 
## 44.9998845  0.1538688 
## 
## $likelihood
##   vertex2 
## -37.72978 
## 
## $iterations
## [1] 100</code></pre>
<p>Okay, it doesn’t seem to be optimizing very well. I probably didn’t implement it perfectly yet!</p>
<p>Can you see why this method is sometimes called the “amoeba” method of optimization??</p>
<p>In general, the simplex-based methods are very good at finding the general area of the MLE, but generally less efficient than the derivative-based methods- especially as you near the MLE.</p>
</div>
</div>
<div id="derivative-free-method-2-simulated-annealing-se." class="section level2">
<h2>Derivative-free method 2: simulated annealing (SE).</h2>
<p>Simulated annealing is one of my favorite optimization techniques. I think it serves as a good metaphor for problem-solving in general. When solving a problem, the first step is to think big, try to imagine whether we might be missing possible solutions. Then we settle (focus) on a general solution, learn more about how that solution applies to our problem, and ultimately get it done!</p>
<p>The temperature analogy is fun too! We start out “hot”- unfocused, frenzied, bouncing around - and we end up cold - crystal clear and focused!</p>
<div id="se-a-global-optimization-solution" class="section level3">
<h3>SE: A “global” optimization solution</h3>
<p>Simulated annealing is called a “global” optimization solution because it can deal with false peaks and other strangenesses that can arise in optimization problems (e.g., maximizing likelihood. )</p>
<div id="pseudocode-for-the-metropolis-simulated-annealing-routine" class="section level4">
<h4>Pseudocode for the Metropolis simulated annealing routine</h4>
<p>Pick an initial starting point and evaluate the likelihood.</p>
<p>Continue the following steps until your answer is good enough:</p>
<blockquote>
<ul>
<li>Pick a new point at random near your old point and compute the (log) likelihood<br />
</li>
<li>If the new value is better, accept it and start again<br />
</li>
<li>If the new value is worse, then
<ul>
<li>Pick a random number between zero and 1<br />
</li>
<li>Accept the new (worse) value anyway if the random number is less than exp(change in log likelihood/k). Otherwise, go back to the previous value<br />
</li>
</ul></li>
<li>Periodically (e.g. every 100 iterations) lower the value of <em>k</em> to make it harder to accept bad moves!</li>
</ul>
</blockquote>
<p>A variant of simulated annealing is available in the “optim” function in R (method = “SANN”)</p>
</div>
</div>
<div id="example-simulated-annealing" class="section level3">
<h3>Example: Simulated annealing!</h3>
<p>Let’s use the same familiar myxomatosis example!</p>
<pre class="r"><code>startingvals &lt;- c(shape=80,scale=0.15)
startinglik &lt;- GammaLikelihoodFunction(startingvals)
startinglik</code></pre>
<pre><code>## [1] -313.6188</code></pre>
<pre class="r"><code>k = 100   # set the &quot;temperature&quot;
 
     # function for making new guesses
newGuess &lt;- function(oldguess=startingvals){
  maxshapejump &lt;- 5
  maxscalejump &lt;- 0.05
  jump &lt;- c(runif(1,-maxshapejump,maxshapejump),runif(1,-maxscalejump,maxscalejump))
  newguess &lt;- oldguess + jump
  return(newguess)
}
  # set a new &quot;guess&quot; near to the original guess

newGuess(oldguess=startingvals)     # each time is different- this is the first optimization procedure with randomness built in</code></pre>
<pre><code>##      shape      scale 
## 76.2892637  0.1624681</code></pre>
<pre class="r"><code>newGuess(oldguess=startingvals)</code></pre>
<pre><code>##      shape      scale 
## 82.5216222  0.1722611</code></pre>
<pre class="r"><code>newGuess(oldguess=startingvals)</code></pre>
<pre><code>##      shape      scale 
## 84.2982093  0.1320043</code></pre>
<p>Now let’s evaluate the difference in likelihood between the old and the new guess…</p>
<pre class="r"><code>LikDif &lt;- function(oldguess,newguess){
  oldLik &lt;- GammaLikelihoodFunction(oldguess)
  newLik &lt;- GammaLikelihoodFunction(newguess)
  return(newLik-oldLik)
}

newguess &lt;- newGuess(oldguess=startingvals)
loglikdif &lt;- LikDif(oldguess=startingvals,newguess)
loglikdif</code></pre>
<pre><code>## [1] -259.894</code></pre>
<p>Now let’s look at the Metropolis routine:</p>
<pre class="r"><code>k &lt;- 100
oldguess &lt;- startingvals
counter &lt;- 0
guesses &lt;- matrix(0,nrow=100,ncol=2)
colnames(guesses) &lt;- names(startingvals)
while(counter&lt;100){
  newguess &lt;- newGuess(oldguess)
  loglikdif &lt;- LikDif(oldguess,newguess)
  if(loglikdif&gt;0){ 
    oldguess &lt;- newguess
  }else{
    rand=runif(1)
    if(rand &lt;= exp(loglikdif/k)){
      oldguess &lt;- newguess   # accept even if worse!
    }
  }
  counter &lt;- counter + 1
  guesses[counter,] &lt;- oldguess
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Clearly this is the most inefficient, brute-force method we have seen so far. And also quite clearly, in the context of this class, the best and most fun (and dangerous!)</p>
<p>Let’s run it for longer, and with a smaller value of k..</p>
<pre class="r"><code>k &lt;- 10
oldguess &lt;- startingvals
counter &lt;- 0
guesses &lt;- matrix(0,nrow=1000,ncol=2)
colnames(guesses) &lt;- names(startingvals)
while(counter&lt;1000){
  newguess &lt;- newGuess(oldguess)
  loglikdif &lt;- LikDif(oldguess,newguess)
  if(loglikdif&gt;0){ 
    oldguess &lt;- newguess
  }else{
    rand=runif(1)
    if(rand &lt;= exp(loglikdif/k)){
      oldguess &lt;- newguess   # accept even if worse!
    }
  }
  counter &lt;- counter + 1
  guesses[counter,] &lt;- oldguess
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>This looks better! The search algorithm is finding the high-likelihood parts of parameter space pretty well!</p>
<p>Now let’s “cool” the temperature over time, let the algorithm settle down on a likelihood peak</p>
<pre class="r"><code>k &lt;- 100
oldguess &lt;- startingvals
counter &lt;- 0
guesses &lt;- matrix(0,nrow=10000,ncol=2)
colnames(guesses) &lt;- names(startingvals)
MLE &lt;- list(vals=startingvals,lik=GammaLikelihoodFunction(startingvals),step=0)
while(counter&lt;10000){
  newguess &lt;- newGuess(oldguess)
  loglikdif &lt;- LikDif(oldguess,newguess)
  if(loglikdif&gt;0){ 
    oldguess &lt;- newguess
  }else{
    rand=runif(1)
    if(rand &lt;= exp(loglikdif/k)){
      oldguess &lt;- newguess   # accept even if worse!
    }
  }
  counter &lt;- counter + 1
  if(counter%%100==0) k &lt;- k*0.8
  guesses[counter,] &lt;- oldguess
  thislik &lt;- GammaLikelihoodFunction(oldguess)
  if(thislik&gt;MLE$lik) MLE &lt;- list(vals=oldguess,lik=GammaLikelihoodFunction(oldguess),step=counter)
}

# visualize!

image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
lines(guesses,col=&quot;red&quot;)
points(MLE$vals[1],MLE$vals[2],col=&quot;green&quot;,pch=20,cex=3)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre class="r"><code>MLE</code></pre>
<pre><code>## $vals
##      shape      scale 
## 49.8639468  0.1389069 
## 
## $lik
## [1] -37.66739
## 
## $step
## [1] 5619</code></pre>
<p>As you can see, the simulated annealing method did pretty well. However, we needed thousands of iterations to do what other methods just take a few iterations to do. But, we might feel better that we have explored parameter space more thoroughly and avoided the potential problem of false peaks.</p>
</div>
</div>
<div id="other-methods" class="section level2">
<h2>Other methods</h2>
<p>As you can see, there are many ways to optimize- and the <em>optimal</em> optimization routine is not always obvious!</p>
<p>You can probably use some creative thinking and imagine your own optimization algorithm… For example, some have suggested combining the simplex method with the simulated annealing method! Opimization is an art!!</p>
</div>
<div id="what-about-the-confidence-interval" class="section level2">
<h2>What about the confidence interval??</h2>
<p>As you can see in the previous examples, most of the optimization techniques we have looked at do not explore parameter space enough to discern the shape of the likelihood surface around the maximum likelihood estimate. Therefore, we do not have the information we need to compute the confidence intervals around our parameter estimates. And what good is a point estimate without a corresponding estimate of uncertainty??</p>
<p>There are several techniques that are widely used to estimate and describe parameter uncertainty:</p>
<ol style="list-style-type: decimal">
<li>Brute force (expose the entire likelihood surface!)</li>
<li>Profile likelihood (the best way!)</li>
<li>Evaluate curvature at the MLE and use that to estimate error (very crude- but the default for many MLE routines!)</li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
