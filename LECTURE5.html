<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-09-27" />

<title>Optimization!</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The virtual ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Optimization!</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>September 27, 2016</em></h4>

</div>


<p>We can’t maximize a likelihood function without an optimization algorithm.</p>
<p>We can’t optimize a sampling or monitoring regime, as in the power analysis problem, without an optimization algorithm.</p>
<p>Clearly, we need optimization algorithms!! In addition, they provide an excellent example of how computers (often via brute force algorithms) have superseded pure mathematics for performing statistical analysis.</p>
<p>You may not have built your own optimization algorithm before, but you have probably taken advantage of optimization algorithms. For example, if you have performed a glm or a non-linear regression in R, you have exploited numerical optimization algorithms!</p>
<p>We will discuss optimization in the context of maximum likelihood estimation. Let’s start with the most simple of all optimization algorithms:</p>
<div id="brute-force" class="section level2">
<h2>Brute Force</h2>
<p>Just like we did for the two-dimensional likelihood surface, we could evaluate the likelihood at tiny intervals across a broad range of parameter values. Then we can just identify the parameter set that produces the maximum likelihood across all evaluated parameter sets.</p>
<div id="positives" class="section level3">
<h3>Positives</h3>
<ul>
<li>Simple!! (Conceptually very straightforward)</li>
<li>Identify false peaks!</li>
<li>Undeterred by discontinuities in the likelihood surface</li>
</ul>
</div>
<div id="negatives" class="section level3">
<h3>Negatives</h3>
<ul>
<li>Speed: even slower and less efficient than a typical ecologist is willing to accept!</li>
<li>Resolution: we may specify the wrong interval size. Even so, we can only get the answer to within plus or minus the interval size.</li>
</ul>
</div>
<div id="example-dataset-myxomatosis-titer-in-rabbits" class="section level3">
<h3>Example dataset: Myxomatosis titer in rabbits</h3>
<p>Let’s use Bolker’s myxomatosis example dataset to illustrate our optimization issues:</p>
<pre class="r"><code>library(emdbook)

MyxDat &lt;- MyxoTiter_sum
Myx &lt;- subset(MyxDat,grade==1)
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>For this example, we are modeling the distribution of measured titers (virus loads) for Australian rabbits. Bolker chose to use a Gamma distribution. Here is the empirical distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We need to estimate the gamma rate and shape parameters that best fit this empirical distribution. Here is one example of a Gamma fit to this distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Let’s build a likelihood function for this problem!</p>
<pre class="r"><code>GammaLikelihoodFunction &lt;- function(params){
  sum(dgamma(Myx$titer,shape=params[&#39;shape&#39;],scale=params[&#39;scale&#39;],log=T))
}

params &lt;- c(40,0.15) 
names(params) &lt;- c(&quot;shape&quot;,&quot;scale&quot;)
params</code></pre>
<pre><code>## shape scale 
## 40.00  0.15</code></pre>
<pre class="r"><code>GammaLikelihoodFunction(params)</code></pre>
<pre><code>## [1] -49.58983</code></pre>
<p>Now let’s optimize using ‘optim’ like we did before, to find the MLE!</p>
<pre class="r"><code>ctrl &lt;- list(fnscale=-1)   # maximize rather than minimize!!
MLE &lt;- optim(fn=GammaLikelihoodFunction,par=params,control=ctrl,method=&quot;BFGS&quot;)</code></pre>
<pre><code>## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced

## Warning in dgamma(Myx$titer, shape = params[&quot;shape&quot;], scale =
## params[&quot;scale&quot;], : NaNs produced</code></pre>
<pre class="r"><code>MLE$par</code></pre>
<pre><code>##      shape      scale 
## 49.3666607  0.1402629</code></pre>
<p>Let’s visualize the fit of the MLE in this case…</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=MLE$par[&quot;shape&quot;],scale=MLE$par[&quot;scale&quot;]),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Okay, now what if we want to try optimizing with the brute force method…</p>
<pre class="r"><code>##############
# define 2-D parameter space!
##############

shapevec &lt;- seq(10,100,by=0.1)   
scalevec &lt;- seq(0.01,0.3,by=0.001)

##############
# define the likelihood surface across this grid within parameter space
##############

surface &lt;- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  for(j in 1:length(scalevec)){
    newparams[&#39;scale&#39;] &lt;- scalevec[j]
    surface[i,j] &lt;- GammaLikelihoodFunction(newparams) 
  }
}

############
# Visualize the likelihood surface
############

image(x=shapevec,y=scalevec,z=surface,zlim=c(-1000,-30),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=surface,levels=c(-30,-40,-80,-500),add=T)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Now what is the maximum likelihood estimate?</p>
<pre class="r"><code>ndx &lt;- which(surface==max(surface),arr.ind=T)
shapevec[ndx[,1]]</code></pre>
<pre><code>## [1] 49.8</code></pre>
<pre class="r"><code>scalevec[ndx[,2]]</code></pre>
<pre><code>## [1] 0.139</code></pre>
</div>
</div>
<div id="derivative-based-methods" class="section level2">
<h2>Derivative based methods!</h2>
<p>If we assume that the likelihood surface is smooth and has only one minimum, we can develop very efficient optimization algorithms. In general, derivative based methods look for the point in parameter space where the derivative of the likelihood function is zero. That is, the peak!</p>
<p>Let’s imagine we are interested in determining the shape parameter, given a known scale parameter. To use derivative based methods, let’s first build a function that estimates the slope of the function at any arbtrary point in parameter space:</p>
<pre class="r"><code>params &lt;- MLE$par
SlopeFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- GammaLikelihoodFunction(params+c(tiny,0))
  low &lt;- GammaLikelihoodFunction(params-c(tiny,0))
  slope &lt;- (high-low)/(tiny*2)
  return(slope)
}

SlopeFunc(shape_guess=30)</code></pre>
<pre><code>## [1] 13.62666</code></pre>
<p>Now let’s visualize this!</p>
<pre class="r"><code>shapevec &lt;- seq(10,100,by=0.1)   

##############
# define the likelihood surface
##############

surface &lt;- numeric(length(shapevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  surface[i] &lt;- GammaLikelihoodFunction(newparams) 
}

plot(surface~shapevec,type=&quot;l&quot;)
point &lt;- GammaLikelihoodFunction(c(shape=30,MLE$par[&#39;scale&#39;]))
slope &lt;- SlopeFunc(shape_guess=30)
lines(c(20,40),c(point-slope*10,point+slope*10),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We also need a function to compute the second derivative, or the curvature…</p>
<pre class="r"><code>params &lt;- MLE$par
CurvatureFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- SlopeFunc(shape_guess+tiny)
  low &lt;- SlopeFunc(shape_guess-tiny)
  curvature &lt;- (high-low)/(tiny*2)
  return(curvature)
}

CurvatureFunc(shape_guess=30)</code></pre>
<pre><code>## [1] -0.9151666</code></pre>
<p>Okay, now we can implement a derivative-based optimization algorithm!</p>
<p>Essentially, we are trying to find the point where the derivative of the likelihood function is zero (the root of the function!).</p>
<p>The simplest derivative based optimization algorithm is the Newton-Raphson algorithm. Here is the pseudocode:</p>
<ul>
<li>pick a guess for a parameter value</li>
<li>compute the derivative of the likelihood function for that guess</li>
<li>compute the slope of the derivative (curvature) of the likelihood function for that guess</li>
<li>Extrapolate linearly to try to find the root (where the derivative of the likelihood function should be zero if the slope of the likelihood function were linear)</li>
<li>repeat until the derivative of the likelihood function is close enough to zero (within a specified tolerance)</li>
</ul>
<p>Let’s first visualize the shape of the first derivative of the likelihood function</p>
<pre class="r"><code>firstderiv &lt;- numeric(length(shapevec))   # initialize storage variable

for(i in 1:length(shapevec)){
  firstderiv[i] &lt;- SlopeFunc(shapevec[i]) 
}

plot(firstderiv~shapevec,type=&quot;l&quot;)
abline(h=0,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Let’s use the Newton method to find the root. First we pick a starting value. Say we pick 80.</p>
<p>First compute the derivatives:</p>
<pre class="r"><code>firstderiv &lt;- SlopeFunc(80)
secondderiv &lt;- CurvatureFunc(80)
firstderiv</code></pre>
<pre><code>## [1] -13.13913</code></pre>
<pre class="r"><code>secondderiv</code></pre>
<pre><code>## [1] -0.3396182</code></pre>
<p>Now let’s use this linear function to extrapolate to where the first derivative is equal to zero:</p>
<pre class="r"><code>oldguess &lt;- 80
newguess &lt;- oldguess - firstderiv/secondderiv
newguess</code></pre>
<pre><code>## [1] 41.31206</code></pre>
<p>Our new guess is that the shape parameter is 41.31. Let’s do it again!</p>
<pre class="r"><code>oldguess &lt;- 41.31
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess) 
newguess</code></pre>
<pre><code>## [1] 48.66339</code></pre>
<p>Okay, we’re already getting close to our MLE of around 49.36. Let’s do it again:</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36237</code></pre>
<p>And again!</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36746</code></pre>
<p>And again!!!</p>
<pre class="r"><code>oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.36746</code></pre>
<p>Wow, in just a few steps we already basically found the root. Let’s find the root for real, using an algorithm…</p>
<pre class="r"><code>NewtonMethod &lt;- function(firstguess,tolerance=0.0000001){
  deriv &lt;- SlopeFunc(firstguess)
  oldguess &lt;- firstguess
  counter &lt;- 0
  while(abs(deriv)&gt;tolerance){
    oldguess&lt;-newguess
    deriv &lt;- SlopeFunc(oldguess)
    newguess &lt;- oldguess - deriv/CurvatureFunc(oldguess)
    counter=counter+1
  }
  mle &lt;- list()
  mle$estimate &lt;- newguess
  mle$likelihood &lt;- GammaLikelihoodFunction(c(shape=newguess,MLE$par[&#39;scale&#39;]))
  mle$iterations &lt;- counter
  return(mle)
}


newMLE &lt;- NewtonMethod(firstguess=80)
newMLE</code></pre>
<pre><code>## $estimate
## [1] 49.36746
## 
## $likelihood
## [1] -37.6673
## 
## $iterations
## [1] 1</code></pre>
<p>Hopefully this illustrates the power of optimization algorithms!!</p>
<p>Note that this method and other derivative-based methods can and do work in multiple dimensions.</p>
</div>
<div id="derivative-free-optimization-methods" class="section level2">
<h2>Derivative-free optimization methods</h2>
<p>Derivative-free methods make no assumption about smoothness. In some ways, they represent a middle ground between the brute force method and the elegant but finnicky derivative-based methods, representing a balance between simplicity and generality.</p>
</div>
<div id="derivative-free-method-1-simplex-method" class="section level2">
<h2>Derivative-free method 1: simplex method</h2>
<p>This is the default opimization method for “optim”! That means that R used this method for optimizing the fuel economy example from the likelihood lecture.</p>
<div id="definition-simplex" class="section level4">
<h4>Definition: Simplex</h4>
<p>A <em>simplex</em> is the multi-dimensional analog of the triangle. In two dimensions, the triangle is the simplest shape possible. It has one more vertex than there are dimensions! In <em>n</em> dimensions, a simplex is defined by <em>n+1</em> vertices.</p>
</div>
<div id="pseudocode-for-nelder-mead-simplex-algorithm" class="section level4">
<h4>Pseudocode for Nelder-Mead simplex algorithm</h4>
<p>Set up an initial simplex in parameter space (often based on a user’s initial guess).</p>
<p>Continue the following steps until your answer is good enough: &gt; - Start by identifying the <em>worst</em> vertex (the one with the lowest likelihood) &gt; - Take the worst vertex and reflect it across the shape represented by the other vertices. &gt; - If the likelihood is higher for the reflected point, double the length of the jump! &gt; - If this jump was bad (lower likelihood) then try a point that’s only half as far out as the initial try. &gt; - If this jump was also bad, then contract the simplex around the current highest-likelihood vertex.</p>
<p><strong>Q</strong>: What does the simplex look like for a one-dimensional optimization problem?</p>
<p><strong>Q</strong>: Is this method likely to be good at avoiding false peaks in the likelihood surface?</p>
</div>
<div id="example-simplex-method" class="section level3">
<h3>Example: Simplex method</h3>
<p>As you can see, the simplex method is very good at finding the general area of the MLE, but generally less efficient than the derivative-based methods- especially as you near the MLE.</p>
</div>
</div>
<div id="derivative-free-method-2-simulated-annealing." class="section level2">
<h2>Derivative-free method 2: simulated annealing.</h2>
<p>Simulated annealing is one of my favorite techniques. I think it serves as a good metaphor for problem-solving in general. When solving a problem, the first step is to think big, try to imagine whether we might be missing possible solutions. Then we settle (focus) on a general solution, learn more about how that solution applies to our problem, and ultimately get it done!</p>
<p>The temperature analogy is fun too! We start out “hot”- unfocused, frenzied, bouncing around - and we end up cold - crystal clear and focused!</p>
<div id="se-a-global-optimization-solution" class="section level3">
<h3>SE: A “global” optimization solution</h3>
<p>Simulated annealing is called a “global” optimization solution because it can deal with false peaks and other strangenesses that can arise in optimization problems (e.g., maximizing likelihood. )</p>
<div id="pseudocode-for-the-metropolis-simulated-annealing-routine" class="section level4">
<h4>Pseudocode for the Metropolis simulated annealing routine</h4>
<p>Pick an initial starting point and evaluate the likelihood.</p>
<p>Continue the following steps until your answer is good enough: &gt; - Pick a new point at random near your old point and compute the likelihood &gt; - If the new value is better, accept it and start again &gt; - If the new value is worse, then &gt; - Pick a random number between zero and 1 &gt; - Accept the &gt; - If this jump was bad (lower likelihood) then try a point that’s only half as far out as the initial try. &gt; - If this jump was also bad, then shrink the simplex around the current highest-likelihood vertex.</p>
</div>
</div>
<div id="example-simulated-annealing" class="section level3">
<h3>Example: Simulated annealing!</h3>
</div>
</div>
<div id="other-methods" class="section level2">
<h2>Other methods</h2>
<p>As you can see, there are many ways to optimize- and the <em>optimal</em> optimization routine is not always obvious!</p>
<p>You can probably use some creative thinking and imagine your own optimization algorithm Opimization is an art!!</p>
</div>
<div id="what-about-the-confidence-interval" class="section level2">
<h2>What about the confidence interval??</h2>
<p>As you can see in the previous examples, most of the optimization techniques we have looked at do not explore parameter space enough to discern the shape of the likelihood surface around the maximum likelihood estimate. Therefore, we do not have the information we need to compute the confidence intervals around our parameter estimates. And what good is a point estimate without a corresponding estimate of uncertainty??</p>
<p>There are several techniques that are widely used to estimate and describe parameter uncertainty:</p>
<ol style="list-style-type: decimal">
<li>Brute force (reveal the entire likelihood surface!)</li>
<li>Profile likelihood!</li>
<li>Evaluate curvature at the MLE and use that to estimate error (very crude- but the default for many MLE routines!)</li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
