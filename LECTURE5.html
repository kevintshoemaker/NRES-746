<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 746" />


<title>Optimization!</title>

<script src="site_libs/header-attrs-2.24/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB_Instructions.html">Instructions for Labs</a>
    </li>
    <li>
      <a href="LAB3demo.html">Lab 3: Likelihood (intro)</a>
    </li>
    <li>
      <a href="LAB5.html">Lab 5: Model selection (optional)</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
    <li>
      <a href="GIT-tutorial.html">Demo: version control in Git</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Optimization!</h1>
<h4 class="author">NRES 746</h4>
<h4 class="date">Fall 2023</h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a
href="LECTURE5.R">click here</a> for the companion R-script for this
lecture.</p>
<div id="optimization" class="section level1">
<h1>Optimization</h1>
<p>We can’t maximize a likelihood function without an optimization
algorithm.</p>
<p>We can’t optimize a sampling or monitoring regime, as in the power
analysis problem, without an optimization algorithm.</p>
<p>Clearly, we need optimization algorithms!! In addition, they provide
an excellent example of how computer algorithms are so essential for
modern data analysis.</p>
<p>You may not have built your own optimization algorithm before, but
you’ve probably taken advantage of optimization algorithms that are
operating behind the scenes. For example, if you have performed a glmm
or a non-linear regression in R, you have exploited numerical
optimization routines!</p>
<p>We will discuss optimization in the context of maximum likelihood
estimation, and then we will discuss optimization and parameter
estimation in a Bayesian context.</p>
<p>Let’s start with the most conceptually simple of all optimization
algorithms – brute force!</p>
<p>NOTE: you won’t need to build your own optimization routines for this
class- the code in this lecture is for demonstration purposes only!</p>
<div id="brute-force" class="section level2">
<h2>Brute Force!</h2>
<p>Just like we did for the two-dimensional likelihood surface, we could
evaluate the likelihood at tiny intervals across a broad range of
parameter space. Then we can identify the parameter set associated with
the maximum likelihood across all evaluated parameter sets (and the
range of plausible parameter estimates!).</p>
<div id="positives" class="section level3">
<h3>Positives</h3>
<ul>
<li>Simple!! (conceptually very straightforward)<br />
</li>
<li>Identify false peaks! (guaranteed to find the MLE!)<br />
</li>
<li>Undeterred by discontinuities in the likelihood surface</li>
</ul>
</div>
<div id="negatives" class="section level3">
<h3>Negatives</h3>
<ul>
<li>Speed: even slower and less efficient than a typical ecologist is
willing to accept! Practically impossible for complex multi-dimensional
problems (<em>curse of dimensionality</em>)<br />
</li>
<li>Resolution: we can only get the answer to within plus or minus the
interval size.</li>
</ul>
</div>
<div id="example-dataset-myxomatosis-titer-in-rabbits"
class="section level3">
<h3>Example dataset: Myxomatosis titer in rabbits</h3>
<p>Let’s use Bolker’s myxomatosis example dataset (an example we’ll
return to frequently!) to illustrate:</p>
<pre class="r"><code># Explore Bolker&#39;s myxomatosis example   -------------------------

library(emdbook)    # this is the package provided to support the textbook!
library(ggplot2)
library(ggthemes)

MyxDat &lt;- MyxoTiter_sum         # load Bolker&#39;s example data
MyxDat$grade &lt;- as.factor(MyxDat$grade)

ggplot(MyxDat,aes(day,titer)) + 
  geom_point(aes(col=grade))  +
  facet_wrap(vars(grade), scales = &quot;free&quot;) +
  theme_clean()</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>Myx &lt;- subset(MyxDat,grade==1)    # subset: select most virulent
head(Myx)</code></pre>
<pre><code>##   grade day titer
## 1     1   2 5.207
## 2     1   2 5.734
## 3     1   2 6.613
## 4     1   3 5.997
## 5     1   3 6.612
## 6     1   3 6.810</code></pre>
<p>For this example, we’re modeling the distribution of measured titers
(virus loads) for Australian rabbits. Bolker chose to use a Gamma
distribution. Here is the empirical distribution:</p>
<pre class="r"><code>hist(Myx$titer,freq=FALSE)    # distribution of virus loads</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We need to estimate the gamma ‘rate’ and ‘shape’ parameters that best
fit this empirical distribution. Here is one example of a Gamma fit to
this distribution:</p>
<pre class="r"><code># Overlay a gamma distribution on the histogram -------------------

hist(Myx$titer,freq=FALSE)     # note the &quot;freq=FALSE&quot;, which displays densities of observations, and therefore makes histograms comparable with probability density functions
curve(dgamma(x,shape=40,rate=6),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>This is clearly not a great fit, but perhaps this would be an okay
starting point (optimization algorithms don’t really require perfect
starting points, just need to be in the ballpark)…</p>
<p>Let’s build a likelihood function for this problem!</p>
<pre class="r"><code># Build gamma likelihood function  ---------------------

GammaLikelihoodFunction &lt;- function(params){           # only one argument (params)- the data are hard-coded here (this is often the case with simple likelihood functions)
  -sum(dgamma(Myx$titer,shape=params[&#39;shape&#39;],rate=params[&#39;rate&#39;],log=T))     # use params and data to compute likelihood 
}

params &lt;- c(shape=40,rate=6) 
GammaLikelihoodFunction(params)    # test the function!</code></pre>
<pre><code>## [1] 38.74585</code></pre>
<p>Now let’s optimize using ‘optim()’ like we did before, to find the
MLE!</p>
<p>NOTE: “optim()” will throw some warnings here because it will try to
find the data likelihood for certain impossible parameter
combinations!</p>
<pre class="r"><code># Optimize using R&#39;s built-in &quot;optim()&quot; function: find the maximum likelihood estimate

MLE &lt;- optim(params,GammaLikelihoodFunction)  

MLE$par</code></pre>
<pre><code>##     shape      rate 
## 49.608753  7.164569</code></pre>
<pre class="r"><code>MLE$value</code></pre>
<pre><code>## [1] 37.66714</code></pre>
<p>We can ignore the warnings!</p>
<p>Let’s visualize the fit of the MLE in this case…</p>
<pre class="r"><code># visualize the maximum likelihood fit

hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=MLE$par[&quot;shape&quot;],rate=MLE$par[&quot;rate&quot;]),add=T,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Looks pretty good…</p>
<p>But as dangerous ecological statisticians we aren’t satisfied with
using a “black box” like the “optim()” function, we need to understand
what is going on behind the scenes. Let’s write our own optimizer!</p>
<p>We start with the conceptually simple, often computationally
impossible, brute force method…</p>
<pre class="r"><code># BRUTE FORCE ALTERNATIVE    ------------------------------

# define 2-D parameter space!

shapevec &lt;- seq(10,100,by=0.1)        # divide parameter space into tiny increments
ratevec &lt;- seq(0.5,30,by=0.05)

# define the likelihood surface across this grid within parameter space


surface2D &lt;- matrix(nrow=length(shapevec),ncol=length(ratevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  for(j in 1:length(ratevec)){
    newparams[&#39;rate&#39;] &lt;- ratevec[j]
    surface2D[i,j] &lt;- -1*GammaLikelihoodFunction(newparams)   # compute likelihood for every point in 2-d parameter space
  }
}

# Visualize the likelihood surface

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-250,-35),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-150),add=T)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now what is the maximum likelihood estimate?</p>
<pre class="r"><code># Find the MLE (brute force)  ------------------------

ndx &lt;- which(surface2D==max(surface2D),arr.ind=T)  # index of the max likelihood grid cell
shapevec[ndx[,1]]     </code></pre>
<pre><code>## [1] 49.5</code></pre>
<pre class="r"><code>ratevec[ndx[,2]]</code></pre>
<pre><code>## [1] 7.15</code></pre>
<pre class="r"><code>MLE$par  # compare with the answer from &quot;optim()&quot;</code></pre>
<pre><code>##     shape      rate 
## 49.608753  7.164569</code></pre>
<p><strong>Q</strong> how would we compute the profile likelihood
confidence intervals for the shape and scale parameters?</p>
</div>
</div>
<div id="derivative-based-methods" class="section level2">
<h2>Derivative based methods!</h2>
<p>If we assume that the likelihood surface is smooth (differentiable)
and has only one minimum, we can use very efficient derivative-based
optimization algorithms.</p>
<p>In general, derivative-based methods look for the point in parameter
space where the first derivative (slope) of the likelihood function is
zero (the ‘root’ of the likelihood function). That is, at the peak- or
the valley bottom.</p>
<p>Let’s imagine we are interested in determining the shape parameter,
given a known scale parameter for a gamma distribution. To use
derivative based methods, let’s first build a function that estimates
the slope of the function at any arbitrary point in parameter space:</p>
<pre class="r"><code># Derivative-based optimization methods   ------------------

# function for estimating the slope of the likelihood surface at any point in parameter space....

## NOTE: even here I&#39;m using a coarse, brute force method for estimating the first and second derivative of the likelihood function

params &lt;- MLE$par
SlopeFunc &lt;- function(shape_guess,tiny=0.001){      
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- GammaLikelihoodFunction(params+c(tiny,0))
  low &lt;- GammaLikelihoodFunction(params-c(tiny,0))
  slope &lt;- (high-low)/(tiny*2)
  return(slope)
}

SlopeFunc(shape_guess=30)    #try it!</code></pre>
<pre><code>## [1] -13.75925</code></pre>
<p>Now let’s visualize this!</p>
<pre class="r"><code># Visualize the slope of the likelihood function at different points in parameter space

shapevec &lt;- seq(10,100,by=0.1)   

# define the likelihood surface

surface1D &lt;- numeric(length(shapevec))   # initialize storage variable

newparams &lt;- params
for(i in 1:length(shapevec)){
  newparams[&#39;shape&#39;] &lt;- shapevec[i]
  surface1D[i] &lt;- GammaLikelihoodFunction(newparams) 
}

plot(surface1D~shapevec,type=&quot;l&quot;)
point &lt;- GammaLikelihoodFunction(c(shape=30,MLE$par[&#39;rate&#39;]))
slope &lt;- SlopeFunc(shape_guess=30)
lines(c(20,40),c(point-slope*10,point+slope*10),col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We also need a function to compute the second derivative, or the
curvature (rate of change in the slope)…</p>
<pre class="r"><code># function for estimating the curvature of the likelihood function at any point in parameter space

params &lt;- MLE$par
CurvatureFunc &lt;- function(shape_guess,tiny=0.001){
  params[&#39;shape&#39;] &lt;- shape_guess
  high &lt;- SlopeFunc(shape_guess+tiny)
  low &lt;- SlopeFunc(shape_guess-tiny)
  curvature &lt;- (high-low)/(tiny*2)   # how much the slope is changing in this region of the function
  return(curvature)
}

CurvatureFunc(shape_guess=30)   # try it!</code></pre>
<pre><code>## [1] 0.9151666</code></pre>
<p>Okay, now we can implement a derivative-based optimization
algorithm!</p>
<p>Essentially, we are trying to find the point where the derivative of
the likelihood function is zero (the root of the function!).</p>
<p>The simplest derivative-based optimization algorithm is the
<em>Newton-Raphson algorithm</em>. Here is the pseudocode:</p>
<ul>
<li>pick a guess for a parameter value<br />
</li>
<li>compute the first derivative of the likelihood function for that
guess<br />
</li>
<li>compute the slope of the first derivative (curvature, or second
derivative) of the likelihood function for that guess<br />
</li>
<li>Extrapolate linearly to try to find the root (where the first
derivative of the likelihood function should be zero assuming the rate
of change in the slope is constant)<br />
</li>
<li>repeat until the first derivative of the likelihood function is
close enough to zero (within a specified tolerance), using the new value
from the previous step as your initial guess.</li>
</ul>
<p>Let’s first visualize the shape of the first derivative of the
likelihood function</p>
<pre class="r"><code># First- visualize the gradient of the likelihood function

firstderiv &lt;- numeric(length(shapevec))   # initialize storage variable
for(i in 1:length(shapevec)){
  firstderiv[i] &lt;- SlopeFunc(shapevec[i]) 
}

plot(firstderiv~shapevec,type=&quot;l&quot;)
abline(h=0,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Let’s use the Newton method to find the <em>root</em> of the
likelihood function. First we pick a starting value. Say we pick 80.</p>
<p>First compute the derivatives:</p>
<pre class="r"><code># Now we can perform a simple, derivative-based optimization!

### Pick &quot;80&quot; as the starting value

firstderiv &lt;- SlopeFunc(80)           # evaluate the first and second derivatives
secondderiv &lt;- CurvatureFunc(80)
firstderiv</code></pre>
<pre><code>## [1] 13.00653</code></pre>
<pre class="r"><code>secondderiv</code></pre>
<pre><code>## [1] 0.3396182</code></pre>
<p>Now let’s use this linear function to extrapolate to where the first
derivative is equal to zero:</p>
<pre class="r"><code># Use this info to estimate the root

oldguess &lt;- 80
newguess &lt;- oldguess - firstderiv/secondderiv   # estimate the root (where first deriv is zero)
newguess</code></pre>
<pre><code>## [1] 41.70248</code></pre>
<p>Our new guess is that the shape parameter is 41.31. Let’s do it
again!</p>
<pre class="r"><code># Repeat this process

oldguess &lt;- 41.31
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess) 
newguess</code></pre>
<pre><code>## [1] 48.86382</code></pre>
<p>Okay, we’re already getting close to our MLE of around 49.36. Let’s
do it again:</p>
<pre class="r"><code># again...

oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.60238</code></pre>
<p>And again!</p>
<pre class="r"><code># again...

oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.60804</code></pre>
<p>And again!!!</p>
<pre class="r"><code># again...

oldguess&lt;-newguess
newguess &lt;- oldguess - SlopeFunc(oldguess)/CurvatureFunc(oldguess)
newguess</code></pre>
<pre><code>## [1] 49.60804</code></pre>
<p>Wow, in just a few iterations we already basically found the true
root. Let’s find the root for real, using an algorithm…</p>
<pre class="r"><code># Implement the Newton Method as a function!  ------------------

NewtonMethod &lt;- function(firstguess,tolerance=0.0000001){
  deriv &lt;- SlopeFunc(firstguess)
  oldguess &lt;- firstguess
  counter &lt;- 0
  while(abs(deriv)&gt;tolerance){
    deriv &lt;- SlopeFunc(oldguess)
    newguess &lt;- oldguess - deriv/CurvatureFunc(oldguess)
    oldguess&lt;-newguess
    counter=counter+1
  }
  mle &lt;- list()
  mle$estimate &lt;- newguess
  mle$likelihood &lt;- GammaLikelihoodFunction(c(shape=newguess,MLE$par[&#39;rate&#39;]))
  mle$iterations &lt;- counter
  return(mle)
}


newMLE &lt;- NewtonMethod(firstguess=80)
newMLE</code></pre>
<pre><code>## $estimate
## [1] 49.60804
## 
## $likelihood
## [1] 37.66714
## 
## $iterations
## [1] 6</code></pre>
<p>In just 6 steps we successfully identified the maximum likelihood
estimate to within 0.0000001 of the true value! How many computations
did we have to perform to use the brute force method?</p>
<p>Hopefully this illustrates the power of optimization algorithms!!</p>
<p>Note that this method and other derivative-based methods can work in
multiple dimensions! The only constraint here is that the likelihood
function is differentiable (smooth)</p>
</div>
<div id="derivative-free-optimization-methods" class="section level2">
<h2>Derivative-free optimization methods</h2>
<p>Derivative-free methods make no assumption about smoothness. In some
ways, they represent a middle ground between the brute force method and
the elegant but finicky derivative-based methods- walking a delicate
balance between simplicity and generality.</p>
<p>Derivative-free methods only require a likelihood function that
returns real numbers but have no additional requirements.</p>
</div>
<div id="derivative-free-method-1-simplex-method"
class="section level2">
<h2>Derivative-free method 1: simplex method</h2>
<p>This is the default optimization method for “optim()”! That means
that R used this method for optimizing the fuel economy example from the
previous lecture!</p>
<div id="definition-simplex" class="section level4">
<h4>Definition: Simplex</h4>
<p>A <em>simplex</em> is the multi-dimensional analog of the triangle.
In a two dimensional space, the triangle is the simplest shape possible
that encloses an area. It has just one more vertex than there are
dimensions! In <em>n</em> dimensions, a simplex is defined by
<em>n+1</em> vertices.</p>
</div>
<div id="pseudocode-for-nelder-mead-simplex-algorithm"
class="section level4">
<h4>Pseudocode for Nelder-Mead simplex algorithm</h4>
<p>Set up an initial simplex in parameter space, essentially
representing three initial guesses about the parameter values. NOTE:
when you use the Nelder-Mead algorithm in “optim()” you only specify one
initial value for each free parameter. “optim()”’s internal algorithm
turns that initial guess into a simplex prior to starting the
Nelder-Mead algorithm.</p>
<p>Continue the following steps until your answer is good enough:</p>
<ul>
<li>Start by identifying the <em>worst</em> vertex (the one with the
lowest likelihood)<br />
</li>
<li>REFLECT IT: Take the worst vertex and reflect it across the center
of the shape represented by the other vertices. This is your ‘proposal
vertex’. If the new likelihood at the reflected point is now the second
best of all the vertices in the new simplex, then replace the old vertex
with the proposal vertex.</li>
<li>EXPAND IT: If the likelihood is highest for the proposal vertex (out
of all the vertices), increase the length of the jump! If this increased
jump improves the likelihood even more, replace the old vertex with this
new extended-jump vertex. If the expansion is not as good as the
reflection, use the reflected vertex instead.<br />
</li>
<li>CONTRACT IT: If the original reflected vertex was bad (lower
likelihood than the original) then try a point closer to the original
vertex along the reflection line. If this point is better than the
original vertex, replace the old vertex with this new contracted
vertex.</li>
<li>If all reflections, expansions and contractions were worse than the
original vertex, then contract (shrink) the simplex toward the
highest-likelihood vertex.</li>
</ul>
<p><strong>Q</strong>: What does the simplex look like for a
one-dimensional optimization problem?</p>
<p><strong>Q</strong>: Is this method likely to be good at avoiding
false peaks in the likelihood surface?</p>
</div>
<div id="example-simplex-method" class="section level3">
<h3>Example: Simplex method</h3>
<p><strong>Step 1:</strong> Set up an initial simplex in parameter
space</p>
<pre class="r"><code># SIMPLEX OPTIMIZATION METHOD!   -----------------------

# set up an &quot;initial&quot; simplex

firstguess &lt;- c(shape=70,rate=5)   # &quot;user&quot; first guess 

simplex &lt;- list()
 
           # set up the initial simplex based on the first guess...
simplex[[&#39;vertex1&#39;]] &lt;- firstguess + c(3,1)
simplex[[&#39;vertex2&#39;]] &lt;- firstguess + c(-3,-1)
simplex[[&#39;vertex3&#39;]] &lt;- firstguess + c(3,-1)

simplex</code></pre>
<pre><code>## $vertex1
## shape  rate 
##    73     6 
## 
## $vertex2
## shape  rate 
##    67     4 
## 
## $vertex3
## shape  rate 
##    73     4</code></pre>
<p>Let’s plot the simplex…</p>
<pre class="r"><code>    ## first let&#39;s make a function to plot the simplex on a 2-D likelihood surface...

addSimplex &lt;- function(simplex,col=&quot;red&quot;){
  temp &lt;- as.data.frame(simplex)    # easier to work with data frame here
  points(x=temp[1,c(1,2,3,1)], y=temp[2,c(1,2,3,1)],type=&quot;b&quot;,lwd=2,col=col)
}

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-300,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-120),add=T)
addSimplex(simplex)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Now let’s evaluate the log likelihood at each vertex</p>
<pre class="r"><code># Evaluate log-likelihood at each vertex of the simplex

SimplexLik &lt;- function(simplex){
  newvec &lt;- -1*unlist(lapply(simplex,GammaLikelihoodFunction))   # note use of apply instead of for loop...
  return(newvec)
}

SimplexLik(simplex)</code></pre>
<pre><code>##   vertex1   vertex2   vertex3 
## -300.5682 -575.2062 -725.8400</code></pre>
<p>Now let’s develop functions to assist our moves through parameter
space, according to the rules defined above…</p>
<pre class="r"><code># Helper Functions

## this function reflects the worst vertex across the remaining vector

# values &lt;- SimplexLik(simplex)
# oldsimplex=simplex[order(values,decreasing = T)]   # note: must be sorted with worst vertex last
ReflectIt &lt;- function(oldsimplex){
  
  # vertnames &lt;- names(oldsimplex)
  n=length(oldsimplex[[1]])
  centroid &lt;- apply(t(as.data.frame(oldsimplex[1:n])),2,mean)
  
  reflected &lt;- centroid + (centroid - oldsimplex[[n+1]])
  expanded &lt;- centroid + 2*(centroid - oldsimplex[[n+1]])
  contracted &lt;- centroid + 0.5*(centroid - oldsimplex[[n+1]])
  
  alternates &lt;- list()
  alternates$reflected &lt;- oldsimplex
  alternates$expanded &lt;- oldsimplex 
  alternates$contracted &lt;- oldsimplex 
  alternates$reflected[[n+1]] &lt;- reflected
  alternates$expanded[[n+1]] &lt;- expanded
  alternates$contracted[[n+1]] &lt;- contracted
  return(alternates)
}
# ReflectIt(oldsimplex)


ShrinkIt &lt;- function(oldsimplex){
  n &lt;- length(oldsimplex[[1]])
  X.vert &lt;- t(as.data.frame(oldsimplex[(1:(n+1))]))
  temp &lt;- sweep(0.5*sweep(X.vert, 2, oldsimplex[[1]], FUN = &quot;-&quot;), 2, X.vert[1, ], FUN=&quot;+&quot;)
  temp2 &lt;- as.data.frame(t(temp))
  lapply(temp2,function(t) c(shape=t[1],rate=t[2])  )
}


MoveTheSimplex &lt;- function(oldsimplex){     # (incomplete) nelder-mead algorithm
  newsimplex &lt;- oldsimplex  # 
           # Start by sorting the simplex (worst vertex last)
  VertexLik &lt;- SimplexLik(newsimplex)
  newsimplex &lt;- newsimplex[order(VertexLik,decreasing=T)]
  liks &lt;- VertexLik[order(VertexLik,decreasing=T)]
  worstLik &lt;- liks[3]
  secondworstLik &lt;- liks[2]
  bestLik &lt;- liks[1]
  
  candidates &lt;- ReflectIt(oldsimplex=newsimplex)      # reflect across the remaining edge
  CandidateLik &lt;- sapply(candidates,SimplexLik)                          # re-evaluate likelihood at the vertices...
  CandidateLik &lt;- apply(CandidateLik,c(1,2), function(t) ifelse(is.nan(t),-99999,t))
  bestCandidate &lt;- names(which.max(CandidateLik[3,]))
  bestCandidateLik &lt;- CandidateLik[3,bestCandidate]
  
  if((CandidateLik[3,&quot;reflected&quot;]&lt;=bestLik)&amp;(CandidateLik[3,&quot;reflected&quot;]&gt;secondworstLik)){
    newsimplex &lt;- candidates[[&quot;reflected&quot;]]
  }else if (CandidateLik[3,&quot;reflected&quot;]&gt;bestLik){
    if(CandidateLik[3,&quot;expanded&quot;]&gt;CandidateLik[3,&quot;reflected&quot;]){
      newsimplex &lt;- candidates[[&quot;expanded&quot;]]
    }else{
      newsimplex &lt;- candidates[[&quot;reflected&quot;]]
    }
  }else{
    if(CandidateLik[3,&quot;contracted&quot;]&gt;worstLik){
      newsimplex &lt;- candidates[[&quot;contracted&quot;]]
    }else{
      newsimplex &lt;- ShrinkIt(newsimplex)
    }
  }

  return(newsimplex)
}

# image(x=shapevec,y=scalevec,z=surface2D,zlim=c(-1000,-30),col=topo.colors(12))
# contour(x=shapevec,y=scalevec,z=surface2D,levels=c(-30,-40,-80,-500),add=T)
# addSimplex(oldsimplex,col=&quot;red&quot;)
# addSimplex(candidates$reflected,col=&quot;green&quot;)
# addSimplex(candidates$half,col=&quot;green&quot;)

# Visualize the simplex  ---------------------

oldsimplex &lt;- simplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)
image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-125),add=T)
addSimplex(oldsimplex,col=&quot;red&quot;)
addSimplex(newsimplex,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Let’s try another few moves</p>
<pre class="r"><code># Make another move  -------------

oldsimplex &lt;- newsimplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-125),add=T)
addSimplex(oldsimplex,col=&quot;red&quot;)
addSimplex(newsimplex,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>And again!</p>
<pre class="r"><code># Make another move  ----------------------

oldsimplex &lt;- newsimplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-125),add=T)
addSimplex(oldsimplex,col=&quot;red&quot;)
addSimplex(newsimplex,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Again:</p>
<pre class="r"><code># Make another move  ----------------

oldsimplex &lt;- newsimplex
newsimplex &lt;- MoveTheSimplex(oldsimplex)

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-125),add=T)
addSimplex(oldsimplex,col=&quot;red&quot;)
addSimplex(newsimplex,col=&quot;green&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>And another few times:</p>
<pre class="r"><code># Make another few moves  ----------------------

par(mfrow=c(2,2))

for(i in 1:4){
  oldsimplex &lt;- newsimplex
  newsimplex &lt;- MoveTheSimplex(oldsimplex)
  
  image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
  contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-125),add=T)
  addSimplex(oldsimplex,col=&quot;red&quot;)
  addSimplex(newsimplex,col=&quot;green&quot;)
}</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Now we can build a function and use the algorithm for optimizing!</p>
<pre class="r"><code># Build a simplex optimization function!  -----------------

SimplexMethod &lt;- function(firstguess,tolerance=0.00001){
  initsimplex &lt;- list()
  initsimplex[[&#39;vertex1&#39;]] &lt;- firstguess + c(5,0.5)
  initsimplex[[&#39;vertex2&#39;]] &lt;- firstguess + c(-5,-0.5)
  initsimplex[[&#39;vertex3&#39;]] &lt;- firstguess + c(5,-0.5)
  VertexLik &lt;- SimplexLik(initsimplex)
  oldbestlik &lt;- VertexLik[which.max(VertexLik)]
  deltalik &lt;- 100
  counter &lt;- 0
  oldsimplex &lt;- initsimplex
  while((counter&lt;250)&amp;(any(abs(diff(VertexLik))&gt;tolerance))){
    newsimplex &lt;- MoveTheSimplex(oldsimplex)
    VertexLik &lt;- SimplexLik(newsimplex)
    bestlik &lt;- VertexLik[which.max(VertexLik)]
    oldsimplex &lt;- newsimplex
    counter &lt;- counter+1
  }
  mle &lt;- list()
  mle$estimate &lt;- newsimplex[[1]]
  mle$likelihood &lt;- bestlik
  mle$iterations &lt;- counter
  return(mle)
}

SimplexMethod(firstguess = c(shape=39,rate=4))</code></pre>
<pre><code>## $estimate
##     shape      rate 
## 49.656948  7.172449 
## 
## $likelihood
##   vertex3 
## -37.66714 
## 
## $iterations
## [1] 41</code></pre>
<p>I like to call this the “amoeba” method of optimization!</p>
<p>In general, the simplex-based methods are less efficient than the
derivative-based methods at finding the MLE- especially as you near the
MLE.</p>
</div>
</div>
<div id="derivative-free-method-2-simulated-annealing-se."
class="section level2">
<h2>Derivative-free method 2: simulated annealing (SE).</h2>
<p>Simulated annealing is one of my favorite optimization techniques. I
think it serves as a good metaphor for problem-solving in general. When
solving a problem, the first step is to think big, try to imagine
whether we might be missing possible solutions. Then we settle (focus)
on a general solution, learn more about how that solution applies to our
problem, and ultimately get it done!</p>
<p>The temperature analogy is fun too! We start out “hot”- unfocused,
frenzied, bouncing around - and we end up “cold” - crystal clear and
focused on a solution!</p>
<div id="se-a-global-optimization-solution" class="section level3">
<h3>SE: A “global” optimization solution</h3>
<p>Simulated annealing is called a “global” optimization solution
because it can deal with false peaks and other strangenesses that can
arise in optimization problems (e.g., maximizing likelihood). The price
is in reduced efficiency!</p>
<div id="pseudocode-for-the-metropolis-simulated-annealing-routine"
class="section level4">
<h4>Pseudocode for the Metropolis simulated annealing routine</h4>
<p>Pick an initial starting point and evaluate the likelihood.</p>
<p>Continue the following steps until your answer is good enough:</p>
<blockquote>
<ul>
<li>Pick a new point at random near your old point and compute the (log)
likelihood<br />
</li>
<li>If the new value is better, accept it and start again<br />
</li>
<li>If the new value is worse, then<br />
- Pick a random number between zero and 1<br />
- Accept the new (worse) value anyway if the random number is less than
exp(change in log likelihood/k). Otherwise, go back to the previous
value<br />
</li>
<li>Periodically (e.g. every 100 iterations) lower the value of
<em>k</em> to make it harder to accept bad moves. Eventually, the
algorithm will “settle down” on a particular point in parameter
space.</li>
</ul>
</blockquote>
<p>A simulated annealing method is available in the “optim” function in
R (method = “SANN”)</p>
</div>
</div>
<div id="example-simulated-annealing" class="section level3">
<h3>Example: Simulated annealing!</h3>
<p>Let’s use the same familiar myxomatosis example!</p>
<pre class="r"><code># Simulated annealing!  -----------------------

startingvals &lt;- c(shape=80,rate=7)
startinglik &lt;- -GammaLikelihoodFunction(startingvals)
startinglik</code></pre>
<pre><code>## [1] -270.5487</code></pre>
<pre class="r"><code>k = 100   # set the &quot;temperature&quot;
 
     # function for making new guesses
newGuess &lt;- function(oldguess=startingvals){
  maxshapejump &lt;- 5
  maxratejump &lt;- 0.75
  jump &lt;- c(runif(1,-maxshapejump,maxshapejump),runif(1,-maxratejump,maxratejump))
  newguess &lt;- oldguess + jump
  return(newguess)
}
  # set a new &quot;guess&quot; near to the original guess

newGuess(oldguess=startingvals)     # each time is different- this is the first optimization procedure with randomness built in</code></pre>
<pre><code>##     shape      rate 
## 76.416983  7.329611</code></pre>
<pre class="r"><code>newGuess(oldguess=startingvals)</code></pre>
<pre><code>##     shape      rate 
## 80.837027  7.247608</code></pre>
<pre class="r"><code>newGuess(oldguess=startingvals)</code></pre>
<pre><code>##     shape      rate 
## 76.552051  7.611385</code></pre>
<p>Now let’s evaluate the difference in likelihood between the old and
the new guess…</p>
<pre class="r"><code># evaluate the difference in likelihood between the new proposal and the old point

LikDif &lt;- function(oldguess,newguess){
  oldLik &lt;- -GammaLikelihoodFunction(oldguess)
  newLik &lt;- -GammaLikelihoodFunction(newguess)
  return(newLik-oldLik)
}

newguess &lt;- newGuess(oldguess=startingvals)
loglikdif &lt;- LikDif(oldguess=startingvals,newguess)
loglikdif</code></pre>
<pre><code>## [1] -103.0489</code></pre>
<p>Now let’s look at the Metropolis routine:</p>
<pre class="r"><code># run and visualize a Metropolis simulated annealing routine -------------

k &lt;- 100
oldguess &lt;- startingvals
counter &lt;- 0
guesses &lt;- matrix(0,nrow=100,ncol=2)
colnames(guesses) &lt;- names(startingvals)
while(counter&lt;100){
  newguess &lt;- newGuess(oldguess)
  loglikdif &lt;- LikDif(oldguess,newguess)
  if(loglikdif&gt;0){ 
    oldguess &lt;- newguess
  }else{
    rand=runif(1)
    if(rand &lt;= exp(loglikdif/k)){
      oldguess &lt;- newguess   # accept even if worse!
    }
  }
  counter &lt;- counter + 1
  guesses[counter,] &lt;- oldguess
}

# visualize!

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-135),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Clearly this is the most inefficient, brute-force method we have seen
so far (aside from the actual brute force method). And also quite
clearly, in the context of this class, the best and most fun (and
dangerous?!).</p>
<p>NOTE: simulated annealing is still way more efficient than the brute
force method we saw earlier, especially with multiple dimensions!</p>
<p>Let’s run it for longer, and with a smaller value of k..</p>
<pre class="r"><code># Run it for longer!

k &lt;- 10
oldguess &lt;- startingvals
counter &lt;- 0
guesses &lt;- matrix(0,nrow=1000,ncol=2)
colnames(guesses) &lt;- names(startingvals)
while(counter&lt;1000){
  newguess &lt;- newGuess(oldguess)
  while(any(newguess&lt;0)) newguess &lt;- newGuess(oldguess)
  loglikdif &lt;- LikDif(oldguess,newguess)
  if(loglikdif&gt;0){ 
    oldguess &lt;- newguess
  }else{
    rand=runif(1)
    if(rand &lt;= exp(loglikdif/k)){
      oldguess &lt;- newguess   # accept even if worse!
    }
  }
  counter &lt;- counter + 1
  guesses[counter,] &lt;- oldguess
}

# visualize!

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-135),add=T)
lines(guesses,col=&quot;red&quot;)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>This looks better! The search algorithm is finding the
high-likelihood parts of parameter space pretty well!</p>
<p>Now let’s “cool” the temperature over time, let the algorithm settle
down on a likelihood peak</p>
<pre class="r"><code># cool the &quot;temperature&quot; over time and let the algorithm settle down

k &lt;- 100
oldguess &lt;- startingvals
counter &lt;- 0
guesses &lt;- matrix(0,nrow=10000,ncol=2)
colnames(guesses) &lt;- names(startingvals)
MLE &lt;- list(vals=startingvals,lik=-GammaLikelihoodFunction(startingvals),step=0)
while(counter&lt;10000){
  newguess &lt;- newGuess(oldguess)
  while(any(newguess&lt;0)) newguess &lt;- newGuess(oldguess)
  loglikdif &lt;- LikDif(oldguess,newguess)
  if(loglikdif&gt;0){ 
    oldguess &lt;- newguess
  }else{
    rand=runif(1)
    if(rand &lt;= exp(loglikdif/k)){
      oldguess &lt;- newguess   # accept even if worse!
    }
  }
  counter &lt;- counter + 1
  if(counter%%100==0) k &lt;- k*0.8
  guesses[counter,] &lt;- oldguess
  thislik &lt;- -GammaLikelihoodFunction(oldguess)
  if(thislik&gt;MLE$lik) MLE &lt;- list(vals=oldguess,lik=-GammaLikelihoodFunction(oldguess),step=counter)
}

# visualize!

image(x=shapevec,y=ratevec,z=surface2D,zlim=c(-500,-30),col=topo.colors(12))
contour(x=shapevec,y=ratevec,z=surface2D,levels=c(-30,-40,-80,-135),add=T)
lines(guesses,col=&quot;red&quot;)
points(MLE$vals[1],MLE$vals[2],col=&quot;green&quot;,pch=20,cex=3)</code></pre>
<p><img src="LECTURE5_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>MLE</code></pre>
<pre><code>## $vals
##     shape      rate 
## 49.914729  7.206274 
## 
## $lik
## [1] -37.66748
## 
## $step
## [1] 5777</code></pre>
<pre class="r"><code>optim(params,GammaLikelihoodFunction)$par</code></pre>
<pre><code>##     shape      rate 
## 49.608753  7.164569</code></pre>
<p>As you can see, the simulated annealing method did pretty well.
However, we needed thousands of iterations to do what other methods just
take a few iterations to do. But, we might feel better that we have
explored parameter space more thoroughly and avoided the potential
problem of false peaks (although there’s no guarantee that the simulated
annealing method will find the true MLE).</p>
</div>
</div>
<div id="other-methods" class="section level2">
<h2>Other methods</h2>
<p>As you can see, there are many ways to optimize- and the
<em>optimal</em> optimization routine is not always obvious!</p>
<p>You can probably use some creative thinking and imagine your own
optimization algorithm… For example, some have suggested combining the
simplex method with the simulated annealing method! Optimization is an
art!!</p>
</div>
<div id="what-about-the-confidence-interval" class="section level2">
<h2>What about the confidence interval??</h2>
<p>As you can see in the previous examples, most of the optimization
techniques we have looked at do not explore parameter space enough to
discern the shape of the likelihood surface around the maximum
likelihood estimate. Therefore, we do not have the information we need
to compute the confidence intervals around our parameter estimates. And
what good is a point estimate without a corresponding estimate of
uncertainty??</p>
<p>There are several techniques that are widely used to estimate and
describe parameter uncertainty:</p>
<ol style="list-style-type: decimal">
<li>Brute force (expose the entire likelihood surface!) [okay, this one
isn’t actually used very often]</li>
<li>Profile likelihood (the most accurate way!)</li>
<li>Evaluate curvature at the MLE and use that to estimate sampling
error (somewhat inexact but efficient- but generally performs pretty
well, and is the default for many MLE routines!)</li>
</ol>
<p><a href="LECTURE6.html">–go to next lecture–</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
