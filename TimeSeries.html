<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />


<title>Time Series Analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus2.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The Virtual Ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
    <li>
      <a href="LECTURE6.html">Bayesian #1: concepts</a>
    </li>
    <li>
      <a href="LECTURE7.html">Bayesian #2: mcmc</a>
    </li>
    <li>
      <a href="LECTURE8.html">Model Selection</a>
    </li>
    <li>
      <a href="LECTURE9.html">Performance Evaluation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final project overview</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
    <li>
      <a href="LAB4.html">Lab 4: Bayesian Inference</a>
    </li>
    <li>
      <a href="FigureDemo.html">Demo: Figures in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Student-led topics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TimeSeries.html">Time-series analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
    <li>
      <a href="ReedfrogPred.csv">Reed Frog Predation Data</a>
    </li>
    <li>
      <a href="ReedfrogFuncresp.csv">Reed Frog Func Resp</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Time Series Analysis</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>Fall 2018</em></h4>

</div>


<p>For those wishing to follow along with the R-based demo in class, <a href="TimeSeries.R">click here</a> for the companion R script for this lecture.</p>
<p>Presenters: V. Alaasam, Z. Bess, A. Tatarko, D. Picklum, D. Salcido</p>
<p><em>November 5, 2018</em></p>
<pre class="r"><code>#########
# Load packages

suppressWarnings(library(forecast))
suppressWarnings(library(tseries))
suppressWarnings(library(dtw))</code></pre>
<pre><code>## Loading required package: proxy</code></pre>
<pre><code>## 
## Attaching package: &#39;proxy&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     as.dist, dist</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     as.matrix</code></pre>
<pre><code>## Loaded dtw v1.20-1. See ?dtw for help, citation(&quot;dtw&quot;) for use in publication.</code></pre>
<div id="intro-to-the-topic" class="section level2">
<h2>Intro to the topic</h2>
<p>Linear regressions: autocorrelation is a major broken assumption. In time series there is a constant, unbroken frequency of measurement. Time series analysis allows you to make inference about change over time and allows for autocorrelation between datapoints, but not between errors. You can also use time series to forecast likely future values.</p>
</div>
<div id="lake-tahoe-clarity-data-worked-example" class="section level2">
<h2>Lake Tahoe Clarity Data: worked example</h2>
<p>We will build our first time series from 50 years (1968-2017) of Secchi disc (water clarity) data taken in the summer and winter in Lake Tahoe. To take these measurements, the Secchi disk is lowered into the water until it is no longer visible, and depth at which it can no longer be seen is recorded.</p>
<pre class="r"><code>## Set up the time series

year&lt;-seq(1968, 2017, by=1)
winter&lt;-c(33.4, 36.3, 30.3, 33.5, 26.1, 29.5, 29.7, 28.8, 27.6, 27.8, 26.7, 29.0, 27.7,
          24.9, 27.6, 29.0, 22.0, 27.3, 26.9, 23.2, 23.6, 26.7,
          25.8, 21.6, 22.1, 25.8, 21.8, 22.9, 26.9, 20.0, 23.2, 24.7, 21.5, 23.7, 23.9, 21.6, 25.4, 
          24.5, 23.4, 25.1, 26.0, 24.8, 22.2, 25.9, 26.9, 23.7, 24.1, 21.8, 25.4, 23.3)
summer&lt;-c(28.7, 22.8, 28.5, 26.3, 27.8, 22.9, 25.3, 23.7, 25.8,
          28.3, 25.0, 24.9, 22.8, 29.8, 19.7, 17.4, 22.7, 22.1,
          22.6, 26.1, 28.0, 23.0, 23.0, 22.2, 25.2, 19.9, 23.7, 17.7, 21.1,
          19.1, 18.2, 19.2, 19.5, 22.2, 24.7, 21.1, 22.3, 20.4, 17.5, 19.9, 15.4, 18.0, 
          15.8, 15.7, 19.7, 19.4, 23.4, 22.3, 17.2, 16.3)

tahoedatasummer&lt;-data.frame(year, summer)
tahoedatasummer$season&lt;-2
tahoedatawinter&lt;-data.frame(year, winter)
tahoedatawinter$season&lt;-1

names(tahoedatasummer) &lt;- c(&quot;year&quot;, &quot;clarity&quot;,&quot;season&quot;)
names(tahoedatawinter) &lt;- c(&quot;year&quot;, &quot;clarity&quot;, &quot;season&quot;)
tahoedata&lt;-as.data.frame(rbind(tahoedatasummer, tahoedatawinter))
tahoedata &lt;- tahoedata[order(tahoedata$year,tahoedata$season),] 
rownames(tahoedata) &lt;- c()
head(tahoedata)</code></pre>
<pre><code>##   year clarity season
## 1 1968    33.4      1
## 2 1968    28.7      2
## 3 1969    36.3      1
## 4 1969    22.8      2
## 5 1970    30.3      1
## 6 1970    28.5      2</code></pre>
<div id="creating-time-series" class="section level3">
<h3>Creating time series</h3>
<p>Frequency in the ts() function specifies the number of times that data was collected per year. For monthly time series data, frequency=12. For quarterly data, frequency=4. The start parameter specifies the first year that the data was collected, and the first interval in that year, if applicable (e.g. start=c(1958,2) means the second quarter (or month, depending on frequency) of the year 1958). R requires complete data set for time series, so in data sets with missing values, you can impute the missing values.</p>
<pre class="r"><code>#Create a time searies using the ts() command 
tahoetimeseries &lt;- ts(tahoedata$clarity, frequency=2, start=c(1968,1))
class(tahoetimeseries) #This confirms that the dataset is now a timeseries object</code></pre>
<pre><code>## [1] &quot;ts&quot;</code></pre>
<pre class="r"><code>#Create a time series plot
plot.ts(tahoetimeseries, main=&quot;Declining Lake Tahoe Clarity Over Time&quot;,
        ylab=&quot;Depth of H2O Clarity (meters)&quot;, xlab=&quot;Year&quot;, lwd=1)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
</div>
<div id="decompose-data" class="section level3">
<h3>Decompose data</h3>
<p>The decompose() function will separate the time series into the component sources of variation in the measured variable. In this case, we are separating the Secchi disk water clarity into estimated sources of variation due to seasonality, the overall trend, and the “Irregular” component (also called the random component, which is all other remaining variation unnacounted for by the seasonality and trend). To decompose, we have to specify that our model is additive or multiplicative. This time series is additive because the fluctuations are constant in size over time (rather than changing over the course of the time series).</p>
<ul>
<li>In an additive model, Value[t] = Trend[t] + Seasonal[t] + Irregular[t]</li>
<li>In a multiplicative model, Value[t] = Trend[t] * Seasonal[t] * Irregular[t]</li>
</ul>
<pre class="r"><code># Decompose the time series into trend, seasonal, and random components

tahoetimeseriescomponents &lt;- decompose(tahoetimeseries, type=&quot;additive&quot;) # specify an additive decomposition 

tahoetimeseriescomponents$trend #portion of variation in H20 Clarity due to the trend
tahoetimeseriescomponents$seasonal #portion of variation in H20 Clarity due to seasonal patterns
tahoetimeseriescomponents$random #portion of variation in H20 Clarity due to random effects</code></pre>
<pre class="r"><code># We can visualize each of the components

plot(tahoetimeseriescomponents)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<p>If we want to adjust for seasonality we can subtract the seasonal component from the original time series. Note that this plot matches the ‘trend’ plot made using the decompose() function. If a multiplicitive model had been specified, we would have used division instead of subtraction. If we want to adjust for seasonality, we can subtract the seasonal component from the original time series.</p>
<pre class="r"><code>par(mfrow=c(1,3))
plot.ts(tahoetimeseries, main=&quot;Declining Lake Tahoe \nClarity&quot;, xlab=&quot;Year&quot;, ylab=&quot;Clarity (meters)&quot;)

seasonallyadjusted &lt;- tahoetimeseries-tahoetimeseriescomponents$seasonal #Remove the seasonal component
plot(seasonallyadjusted, main=&quot;Seasonal Component \nRemoved&quot;, ylab=&quot;Clarity (meters)&quot;, xlab=&quot;Year&quot;)

trend&lt;- seasonallyadjusted-tahoetimeseriescomponents$random #With both seasonal and random components removed, only trend remains.
plot(trend, main=&quot;Seasonal and Random \nComponents Removed&quot;, xlab=&quot;Year&quot;, ylab=&quot;Clarity (meters)&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-7-1.png" width="864" /></p>
</div>
<div id="data-smoothing" class="section level3">
<h3>Data Smoothing</h3>
<p>The Holt-Winters function “smooths” our data and can be used for forecasting time series with seasonality. Smoothing is controlled by: 1) The parameter ‘alpha’ (0–&gt;1) for the estimate of the level (local average) at a particular time point. 2) The parameter ‘beta’ (0–&gt;1) for the estimate of the slope. 3) The parameter ‘gamma’ (0–&gt;1) for the estimate of the seasonality. These parameters will tell you if your estimates are due to newer or older time points in the time series. You can think of this as the “memory” of the model. For example, if alpha is near 0, little weight is placed on the most recent observations when making forecasts of future values, and more weight is placed on the older values.</p>
<pre class="r"><code>forecasted &lt;- HoltWinters(tahoetimeseries)
forecasted</code></pre>
<pre><code>## Holt-Winters exponential smoothing with trend and additive seasonal component.
## 
## Call:
## HoltWinters(x = tahoetimeseries)
## 
## Smoothing parameters:
##  alpha: 0.08808539
##  beta : 0.2522389
##  gamma: 0.2890812
## 
## Coefficients:
##           [,1]
## a  17.06350568
## b  -0.06755068
## s1  6.70075457
## s2  1.39632552</code></pre>
<p>We can plot our smoothed forecast. By default, this function just uses the range of years that exist in our data. In other words, it doesn’t forecast into the future. Thus, it can assess how good the forecast is by comparing with our actual data.</p>
<pre class="r"><code>plot(forecasted, xlab=&quot;Year&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-9-1.png" width="768" /></p>
</div>
<div id="forecasting-into-the-future" class="section level3">
<h3>Forecasting: Into the future</h3>
<p>Now, we can make forecasts for future time points by using the “forecast” function in R. This integrates the Holt-Winters smoothing function. The argument “h” specifies how many intervals in the future to forecast.</p>
<pre class="r"><code>future&lt;-forecast(HoltWinters(tahoetimeseries), h=40)
plot(future)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
<p>This will produce forecasted values for 20 years because we specified h=40 and there are two sampling events per year. The upper and lower 80 and 95% prediction intervals are also produced. The Holt-Winters model is used for an additive model with increasing or decreasing trends, and can accomodate seasonality.</p>
</div>
<div id="autocorrelation-corellogram-and-box-ljung-test" class="section level3">
<h3>Autocorrelation: Corellogram and Box-Ljung test</h3>
<div class="figure">
<img src="Auto.jpg" />

</div>
<p>To see if our model is OK, we can look at the residuals of the model. A Ljung-box test assesses autocorrelation of our residuals over time. Shift=Lag. A Lag is the time difference at which you make a comparison of the measurement at one point in time with a given. The null hypothesis is that data are independently distributed.</p>
<p>To test goodness of fit of our forecast, we can plot a correlogram of our residuals. If the residuals of our model are autocorrelated, the model can be improved (we will get into how to do this later). This is assessing the fit of the model within the time period that has already occurred.</p>
<p>Residual magnitude = ‘observed value’ - ‘expected value’</p>
<p>If Lag=1, you are comparing the observation at one time point with the observation at an adjacent time point. If Lag=2, you are comparing the observation at one time point with the observation at two intervals away.</p>
<pre class="r"><code>Box.test(future$residuals, lag=20, type=&quot;Ljung-Box&quot;)</code></pre>
<pre><code>## 
##  Box-Ljung test
## 
## data:  future$residuals
## X-squared = 13.99, df = 20, p-value = 0.831</code></pre>
<p>This correllogram helps us visualize the autocorrelation of the residuals at each lag time to see if our forecast is good. A high p-value indicates low autocorrelation, so this model works well with our timeseries. In other words, the magnitude of the residuals does not increase or decrease over time.</p>
<pre class="r"><code>acf(future$residuals, lag.max=20, na.action = na.pass)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-12-1.png" width="768" /></p>
</div>
<div id="choosing-model-parameters-a-function-for-model-assessment" class="section level3">
<h3>Choosing model parameters: a function for model assessment</h3>
<p>Inputs are a) a vector of forecasts and b) the number of lags desired for the correlogram and for the Ljung-Box Test. The function will produce:</p>
<ul>
<li>The test statistics and p-values for a Shapiro-Wilk Test (testing to see if the errors are normally distributed) and a Ljung-Box Test for autocorrelation</li>
<li>A histogram of the errors</li>
<li>A correlogram for the temporal correlation of the errors *An idealized normal distribution curve with a mean of 0 and a standard deviation equal to the standard deviation of the residuals.</li>
</ul>
<p>Perform a Shapiro-Wilk test on the residuals to determine normality. Perform a Ljung-Box test to assess temporal autocorrelation.</p>
<pre class="r"><code>## A function for building correlograms and testing errors

ErrorDistribution&lt;-function(forecastedvalues, lags){
  errors&lt;-na.omit(forecastedvalues$residuals) 
  par(mfrow=c(1,2))
  hist(errors, main=&quot;Distribution of Forecast \nErrors&quot;, freq=FALSE, col=&quot;royalblue1&quot;, 
       xlab=&quot;Error Magnitude&quot;, ylim=c(0,(dnorm(0, 0, sd(errors))+1))) #Histogram of the residual        distribution
  curve(dnorm(x, 0, sd(errors)), type=&#39;l&#39;, add=TRUE, lwd=3) #Histogram of residuals should follow this curve.
  legend(&quot;topleft&quot;, lwd=3, c(&quot;Ideal Normal Distribution&quot;), cex=0.5) #Create a legend
  acf(errors, main=&quot;Corellogram&quot;, lag=lags, na.action=na.pass, ylab=&quot;Correlation Coefficient&quot;) #Plot the corellogram 
  legend(&quot;topright&quot;, c(&quot;95% Confidence \nIntervals\n&quot;), lty=2, col=4, cex=0.6) 
  shapiro&lt;-shapiro.test(errors) 
  ljung.box&lt;-Box.test(errors, lag=lags, type=&quot;Ljung-Box&quot;) #Ljung-Box test: tests temporal autocorrelation
  matrix&lt;-matrix(data=c(shapiro$statistic, shapiro$p.value, 
                        ljung.box$statistic, ljung.box$p.value), nrow=2, ncol=2)
  colnames(matrix)&lt;-c(&quot;Shapiro-Wilk Test&quot;, &quot;Ljung-Box Test&quot;)
  rownames(matrix)&lt;-c(&quot;Test Statistic&quot;, &quot;P-Value&quot;) #Makes a matrix of the test results.
  return(matrix)
  
}</code></pre>
<pre class="r"><code>ErrorDistribution(forecastedvalues=future, lags=20)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-14-1.png" width="864" /></p>
<pre><code>##                Shapiro-Wilk Test Ljung-Box Test
## Test Statistic         0.9858364     13.9899467
## P-Value                0.3784925      0.8310053</code></pre>
<p>The output from a Sharpiro-Wilk test tells us that our errors are normally distributed. We can see from the graph on the left that the mean is approximately zero, meaning that the residuals are not positively- or negatively-biased. The Ljung-Box test tells us that the errors are not temporally autocorrelated within the timeframe that has already happened.</p>
<p>Let’s try using the first half of our data and “forecasting” the second half of our data. This will allow us to test the acccuracy of our predicting abilities. We can subset the time series using the window() function to look at just a chunk of time. Here, we are looking at the time series for only 1990 onward. Then, we can create a forecast from 1968 to 2009, using it to predict from 2010-2017; 16 intervals.</p>
<pre class="r"><code># Testing our forecasting ability

plot.ts(window(tahoetimeseries, start=c(1990,1), end=c(2017, 2)), main=&quot;Declining Lake Tahoe Clarity Over Time&quot;, ylab=&quot;Depth of H2O Clarity (meters)&quot;, xlab=&quot;Year&quot;, ylim=c(0, 40), lwd=2)
shortforecast&lt;-data.frame(forecast(HoltWinters(window(tahoetimeseries, start=c(1968, 1), end=c(2009, 2))), h=16)) #Shortened forecast (using 1968-2009 data)
predictionyears&lt;-seq(2010, 2017.5, by=0.5) #Prediction years

#Plot Forecast with 95% prediction intervals 
points(predictionyears, shortforecast$Point.Forecast, type=&#39;l&#39;, col=&quot;red&quot;, lwd=2) #Forecast
points(predictionyears, shortforecast$Lo.95, type=&#39;l&#39;, lty=2, col=&quot;red&quot;, lwd=1) #lower 95% PI
points(predictionyears, shortforecast$Hi.95, type=&#39;l&#39;, lty=2, col=&quot;red&quot;, lwd=1) #Upper 95% PI
legend(&quot;topleft&quot;, c(&quot;Observed Values (2000-2017)&quot;,&quot;Forecasted Values (2010-2017)&quot;, &quot;95% Prediction Intervals&quot;), lwd=c(2,2,1), col=c(1,2,2), lty=c(1,1,2), cex=0.75)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-15-1.png" width="768" /></p>
</div>
</div>
<div id="airpassengers-dataset-example" class="section level2">
<h2>Airpassengers Dataset Example</h2>
<p>We will use this dataset to explore different forecasting methods. The AirPassengers data set is a timseries provided by R. Unlike the Tahoe clarity data, this dataset consists of monthly measurements.</p>
<pre class="r"><code>AirPassengers</code></pre>
<pre><code>##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432</code></pre>
<pre class="r"><code>class(AirPassengers) </code></pre>
<pre><code>## [1] &quot;ts&quot;</code></pre>
<pre class="r"><code>## plot of Air Passengers between 1948 to 1960

plot.ts(AirPassengers)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-17-1.png" width="768" /></p>
<div id="non-stationarity" class="section level3">
<h3>Non-Stationarity</h3>
<p>Looking at the raw time series, it appears that the seasonal variance in the number of air passengers increases with time. This means that our time series is non-stationary. We can visualize the non-stationarity in our time series by applying the short forecasting method as we did with the Tahoe data. We will plot a shortened forecast (using 1949-1955 data) and overlay the forecast for the last 4 years (48 months) onto the observed data. As you can see, the additive Holt-Winters forecast does a poor job predicting, and the observed values often exceed the 95% confidence intervals. Also, the Ljung-Box test gives a significant result, telling us that the residuals are autocorrelated.</p>
<pre class="r"><code>## Create a Holt-Winters projection of the last 

forecastAirPassengers&lt;-forecast(HoltWinters(AirPassengers, seasonal=&quot;mult&quot;, gamma=), h=40)
plot.ts(AirPassengers, main=&quot;Air Passenger Data&quot;, ylab=&quot;# of Airline Passengers&quot;,xlab=&quot;Year&quot;, xlim=c(1950, 1960), ylim=c(100, 555), lwd=2)
shortforecast&lt;-data.frame(forecast(HoltWinters(window(AirPassengers, start=c(1950, 1), end=c(1955, 12))),h=48))
predictionyears&lt;-seq(1956, 1959+(11/12), by=1/12)

points(predictionyears, shortforecast$Point.Forecast, type=&#39;l&#39;, col=&quot;red&quot;, lwd=2)
points(predictionyears, shortforecast$Lo.95, type=&#39;l&#39;, lty=2, col=&quot;red&quot;, lwd=1)
points(predictionyears, shortforecast$Hi.95, type=&#39;l&#39;, lty=2, col=&quot;red&quot;, lwd=1)
legend(&quot;topleft&quot;, c(&quot;Observed Values (Jan,1949-Dec,1960)&quot;, &quot;Forecasted Values (Jan,1956-Jan,1960)&quot;, &quot;95% Confidence Intervals&quot;), lwd=c(2,2,1), col=c(1,2,2), lty=c(1,1,2), cex=0.75)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-18-1.png" width="768" /></p>
<pre class="r"><code>##Ljung-Box test for autocorrelation of residuals
Box.test(forecastAirPassengers$residuals, lag=20, type=&quot;Ljung-Box&quot;)</code></pre>
<pre><code>## 
##  Box-Ljung test
## 
## data:  forecastAirPassengers$residuals
## X-squared = 37.344, df = 20, p-value = 0.01064</code></pre>
</div>
<div id="choosing-forecasting-models-autoregressive-integrated-moving-average-arima" class="section level3">
<h3>Choosing forecasting models: Autoregressive Integrated Moving Average (ARIMA)</h3>
<p>In addition to using Holt-Winters forecasting methods, we can fit ARIMA models. While Holt-Winters models implemen exponential smoothing and are parameterized by 3 components (level, trend and seasonal), the ARIMA model is an autoregressive model with a moving average and differencing components. In ARIMA models, the forecast variable is dependent on lagged observations. ARIMA forecasting models make the assumption that your data is stationary (ie, the mean and variance is not changing over time). Therefore, we must check this assumption, and transform or difference our time series to meet this assumption, and incorporate this info into the ARIMA model.</p>
</div>
<div id="autocorrelation-and-partial-autocorrelation" class="section level3">
<h3>Autocorrelation and Partial Autocorrelation</h3>
<p>Autocorrelation is important to examine and define for the ARIMA model. You can define autocorrelation as 1st order, if data points are correlated with the immediate lag (eg. first shift in time series) and 2nd order if correlated with the first two lags. Patterns and characteristics of autocorrelation are interpreted from the correlogram. Specifically, we can determine if the correlation and its magnitude is due to lags or lagged errors or both. For example, in the correlogram below we see data is significantly correlated as the correlations at each lag are outside the 95% bounds. Further, it shows us evidence of seasonality with the cyclic behavior. In partial autocorrelation, the residuals of one point in a time series are similar to residuals of other points before or after it in time. A PACF visualizes partial autocorrelation. ARIMA accounts for both autocorrelation and partial autocorrelation.</p>
<div class="figure">
<img src="Pauto1.jpg" style="width:70.0%" />

</div>
<div class="figure">
<img src="Pauto2.jpg" />

</div>
<pre class="r"><code>## Correlogram and Ljung-Box test for the first 48 months (= lag of 4 years)

acf(AirPassengers, lag.max=48, na.action = na.pass,xlab=&quot;lag (years)&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-19-1.png" width="768" /></p>
<pre class="r"><code>pacf(AirPassengers, lag.max=48, na.action = na.pass,xlab=&quot;lag (years)&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-19-2.png" width="768" /></p>
<pre class="r"><code>adf.test(AirPassengers) #Augmented Dickey-Fuller Test</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  AirPassengers
## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>kpss.test(AirPassengers) #Kwiatkowski-Phillips-Schmidt-Shin: null hypothesis: x is level or trend stationary</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  AirPassengers
## KPSS Level = 4.3423, Truncation lag parameter = 2, p-value = 0.01</code></pre>
</div>
<div id="first-differencing-or-de-trend-the-data" class="section level3">
<h3>First: differencing, or de-trend the data</h3>
<p>This dataset is VERY non-stationary because the p-value ~0. If you start off with a non-stationary time series, you need to make it stationary. We can see that this timeseries is non-stationary in two ways: 1) The mean changes over time 2) the variance is not constant in size over time. We will address non-stationarity first by differencing our time series, removing the change in mean over time (de-trending!).</p>
<p>Original Time Series: Value [T1], Value [T2], Value [T3], …</p>
<p>Time Series Differenced by One Order: Value [T2-T1], Value [T3-T2], Value [T4-T3], …</p>
<pre class="r"><code># The diff() command de-trends the timeseries to make it more stationary
# This plot gives: 
# 1. overal shape of data
# 2. ACF (autocorrelation function: represents the correlation between consecutive data points in the time series)
# 3. pACF (partial autocorrelation fuction: partial correlation coefficients between the series and lags of itself) 

dAP&lt;-diff(AirPassengers, differences = 1)
tsdisplay(ts(dAP, freq=1),lag.max= 48, main=&quot;First order Difference&quot;, xlab=&quot;Months&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>adf.test(dAP)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  dAP
## Dickey-Fuller = -7.0177, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>kpss.test(dAP)</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  dAP
## KPSS Level = 0.011485, Truncation lag parameter = 2, p-value = 0.1</code></pre>
<p>Our time series looks much more stationary after differencing once. We can confirm this using the ndiffs() command, which will return the number of times the time series should be differenced.</p>
<pre class="r"><code>ndiffs(AirPassengers)</code></pre>
<pre><code>## [1] 1</code></pre>
</div>
<div id="next-log-transform-the-data" class="section level3">
<h3>Next: Log transform the data</h3>
<p>The data is now de-trended with a constant mean, but the data is not yet stationary because variance increases with time. We will log transform the data to change this.</p>
<pre class="r"><code># Use the diff() command on the logged data and create ACF and pACF plots

ldAP&lt;-diff(log(AirPassengers), differences = 1) #use diff function to take first order diffrence of logged time series
tsdisplay(ts(ldAP, freq=1),lag.max= 48, main=&quot;First order Difference&quot;,xlab=&quot;Months&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>adf.test(ldAP)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  ldAP
## Dickey-Fuller = -6.4313, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>kpss.test(ldAP)</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  ldAP
## KPSS Level = 0.022017, Truncation lag parameter = 2, p-value = 0.1</code></pre>
<pre class="r"><code># Compare differenced and logged time series and correlograms

par(mfrow=c(2,3),mar=c(4, 4, 1, 1) + 0.1)
acf(AirPassengers, lag.max=48, na.action = na.pass, main=&quot;Original Time Series&quot;,xlab=&quot;Lag (years)&quot;)
acf(dAP, lag.max=48, na.action = na.pass, main=&quot;Differenced Time Series&quot;,xlab=&quot;Lag (years)&quot;)
acf(ldAP, lag.max=48, na.action = na.pass, main=&quot;Differenced and Logged Time Series&quot;,xlab=&quot;Lag (years)&quot;)

plot.ts(AirPassengers, xlab=&quot;Year&quot;)
plot.ts(dAP, xlab=&quot;Year&quot;, ylab=&quot;logged # of Air Passengers&quot;)
plot.ts(ldAP, xlab=&quot;Year&quot;, ylab=&quot;Differenced and logged # of Air Passengers&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-23-1.png" width="912" /></p>
</div>
<div id="finally-seasonality-is-still-contributing-autocorrelation" class="section level3">
<h3>Finally: Seasonality is still contributing autocorrelation</h3>
<p>Lets take care of seasonality. Because cycles are still evident in the differenced and logged time series, we will now account for non-stationarity due to seasonality. The seasonal cycles can be differenced to remove the effect of seasonality by setting lag=12 in the differencing command.</p>
<pre class="r"><code>sldAP&lt;-diff(ldAP,lag=12, differences= 1) # Take first order difference of seasonal cycle (12mo)
suppressWarnings(tsdisplay(ts(sldAP,freq=1),main=&quot;Logged Raw Data \nDifferenced at lags 1 and 12&quot;,lag.max=40))</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-24-1.png" width="864" /></p>
<pre class="r"><code>par(mfrow=c(1,2))
acf(sldAP, lag.max=48, na.action = na.pass, main=&quot;Differenced Time Series&quot;,xlab=&quot;Lag (years)&quot;)
plot.ts(sldAP, xlab=&quot;Year&quot;, ylab=&quot;log ( # of Air Passengers )&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-24-2.png" width="864" /></p>
<pre class="r"><code>adf.test(sldAP)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  sldAP
## Dickey-Fuller = -5.1993, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<pre class="r"><code>kpss.test(sldAP)</code></pre>
<pre><code>## 
##  KPSS Test for Level Stationarity
## 
## data:  sldAP
## KPSS Level = 0.058569, Truncation lag parameter = 2, p-value = 0.1</code></pre>
</div>
</div>
<div id="building-arima-models-and-auto.arima" class="section level2">
<h2>Building ARIMA models and auto.arima()</h2>
<p>The basic structure of an ARIMA model takes into account the differencing and autocorrelation that we found while looking for stationarity in our time series. It includes both autocorrelation values (from our ACF plots) and partial autocorrelation values (from our pACF plots). This makes ARIMA models flexible and adaptable to a variety of kinds of time series. The basic ARIMA model is as follows:</p>
<p>ARIMA(p,d,q)(P,D,Q)[period]</p>
<ul>
<li>p: number of lags for partial ACF to drop below 95% conf int</li>
<li>d: order of differencing</li>
<li>q: number of lags before the ACF drops below the 95% conf int</li>
<li>P: For the seasonal component, number of lags for partial ACF to drop below 95% conf int</li>
<li>D: For the seasonal component, order of differencing</li>
<li>Q: For the seasonal component, number of lags before the ACF drops below the 95% conf int</li>
<li>Period: this is the number of seasons in a year</li>
</ul>
<p>eg: ARIMA(2,1,1)(1,0,0)[12] has differencing of order 1, with 2 lags and 1 partial lag. For the seasonal component, (1,0,0), there was no differencing performed, and there was 1 lag and 0 partial lags. [12] stands for the number of period in a year, so there are measurements for each month in this case. The auto.arima() function will optimize these parameters, and will allow us to make forecasts into the future.</p>
<pre class="r"><code>arima_model &lt;- auto.arima(log(AirPassengers))
summary(arima_model)</code></pre>
<pre><code>## Series: log(AirPassengers) 
## ARIMA(0,1,1)(0,1,1)[12] 
## 
## Coefficients:
##           ma1     sma1
##       -0.4018  -0.5569
## s.e.   0.0896   0.0731
## 
## sigma^2 estimated as 0.001371:  log likelihood=244.7
## AIC=-483.4   AICc=-483.21   BIC=-474.77
## 
## Training set error measures:
##                        ME       RMSE        MAE        MPE      MAPE
## Training set 0.0005730622 0.03504883 0.02626034 0.01098898 0.4752815
##                   MASE       ACF1
## Training set 0.2169522 0.01443892</code></pre>
<pre class="r"><code>forecastAP &lt;- forecast(arima_model, level = c(95), h = 40)
plot(forecastAP)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div id="errordistribution-function" class="section level3">
<h3>ErrorDistribution() Function</h3>
<p>Now we can use the ErrorDistribution() function we created to test how well our ARIMA forecast works.</p>
<pre class="r"><code>## Test for normality and autocorrelation of residuals

ErrorDistribution(forecastedvalues=forecastAP, lags=20)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-26-1.png" width="864" /></p>
<pre><code>##                Shapiro-Wilk Test Ljung-Box Test
## Test Statistic         0.9863690     17.6883515
## P-Value                0.1674294      0.6079297</code></pre>
</div>
</div>
<div id="dynamic-time-warping" class="section level2">
<h2>Dynamic Time Warping</h2>
<p>Dynamic Time Warping allows you to compare two different time series, even when the two are not aligned. The method stretches and compresses “time” to make one time series resemble the other as much as possible. It then computes the optimal (least cumulative distance) alignment between points of two time series.</p>
<pre class="r"><code>dat&lt;-seq(0,6.28,len=100)
query&lt;-sin(dat)+runif(100)/10 #create somewhat noisy data

## Create a template or baseline time series to compare our query time series to
template&lt;-cos(dat)
plot(template); lines(query,col=&quot;blue&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Now we can use the dtw function Function arguments: two time series or vectors of numbers keep=TRUE preserves the cumulative cost matrix, inputs, and other internal structures step=rabinerJuangStepPattern step pattern describing the warping steps, can be specialized type=“twoway” overlays the two time series together, type=“threeway” includes the alignment; a line with a slope of 1 means the two are well aligned</p>
<pre class="r"><code>## Find the best match with the canonical recursion formula
alignment&lt;-dtw(query,template,keep=TRUE) #will need the dtw package for this

plot(dtw(query,template,keep=TRUE, step=rabinerJuangStepPattern(6,&quot;c&quot;)),type=&quot;twoway&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r"><code>plot(dtw(query,template,keep=TRUE, step=rabinerJuangStepPattern(6,&quot;c&quot;)),type=&quot;threeway&quot;)</code></pre>
<p><img src="TimeSeries_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
</div>
<div id="challenge" class="section level2">
<h2>Challenge</h2>
<p>The Oceanic Niño Index (ONI) describes warm and cold periods, thereby influencing snow accumulation and snowmelt. See if this data aligns well with the Tahoe clarity data. What other factors may explain declining clarity?</p>
<p>You can load the data <a href="onidata.csv">here</a></p>
<pre class="r"><code>#View ONI data
onidat=read.csv(&quot;onidata.csv&quot;, header=T)
head(onidat)
onits &lt;- ts(onidat$ONI, frequency=2, start=c(1968,1)) #convert to time series
plot.ts(onits, main=&quot;ONI Over Time&quot;,
        ylab=&quot;Oceanic Niño Index (ONI)&quot;, xlab=&quot;Year&quot;, lwd=1)</code></pre>
</div>
<div id="useful-links" class="section level2">
<h2>Useful links</h2>
<ul>
<li><a href="https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/">Little book of R for Time Series</a></li>
<li><a href="https://www.youtube.com/watch?v=ZjaBn93YPWo">Autocorrelation video</a></li>
<li><a href="http://dtw.r-forge.r-project.org/">dynamic time warping</a></li>
<li><a href="http://node101.psych.cornell.edu/Darlington/series/series0.htm">Regression approach to time series</a></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
