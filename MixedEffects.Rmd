---
title: "Mixed Effects Models Using Sloth Data"
author: "Joe Brehm, Mariel Boldis, Steven Bristow, and Janyne Little"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE}

############################################################
####                                                    ####  
####  NRES 746, Student-led topic #5                    ####
####                                                    ####
############################################################


############################################################
####  Mixed Effects Models                              ####
############################################################


```


For those wishing to follow along with the R-based demo in class, [click here](MixedEffects.R) for the companion R script for this lecture.

Also, [click here](20181120_Mixed Effects.pptx) for the PPT slides!

#### Load the data!

Sloth data provided by Neam KD, Lacher Jr. TE (2018) Multi-scale effects of habitat structure and landscape context on a vertebrate with limited dispersal ability (the brown-throated sloth, Bradypus variegatus ). Biotropica 50(4): 684-693. https://doi.org/10.1111/btp.12540

Dryad data package:
Neam KD, Lacher Jr. TE (2018) Data from: Multi-scale effects of habitat structure and landscape context on a vertebrate with limited dispersal ability (the brown-throated sloth, Bradypus variegatus). Dryad Digital Repository. https://doi.org/10.5061/dryad.n8tt5

Download Zuur et al. data [here](http://highstat.com/Books/Book2/ZuurDataMixedModelling.zip)    
Download sloth data [here](https://www.dropbox.com/s/ewh3l1rjy8ivi9j/sloth_point5only_withPatchGroup.csv?dl=0).    

## Properties of mixed models
### Remember these Assumptions. 

The assumptions of generalised linear mixed models are a combination of the assumptions of GLMs and mixed models.

1. The observed *y* are independent, conditional on some predictors *x*

2. The response *y* come from a known distribution from the exponential family, with a known mean variance relationship

3. There is a uniform relationship between some known function of the mean of *y* and the predictors *x* and random effects *z*

4. Random effects *z* are independent of *y*

5. Random effects *z* are normally distributed


### Install packages
```{r results='hide', collapse=TRUE, message=FALSE}

library(Matrix)
library(lme4)
library(MASS)
library(arm)
library(sjstats)
library(ResourceSelection)

```

## Zuur et al. Data

First, let's start with some nice neat data to show you what mixed effect models can do for you.

```{r}

# load in
df.rikz <- read.table("RIKZ.txt",header=T)

# test for correlation
abs(cor(df.rikz[,c(2,5)]))

# define the model
lme.rikz <- lmer(Richness ~ NAP + (1 | Beach), data = df.rikz)

# residual vs fitted plot
plot(lme.rikz, xlab = "Fitted", ylab = "Residual")

```

```{r tidy=TRUE}

summary(lme.rikz)

```

## Sloth Data
Now... let's get into a real ecological dataset.

```{r}

rm(list=ls())

df.sloth <- read.csv("sloth_point5only_withPatchGroup.csv", header = TRUE, sep = ",")
str(df.sloth) # 25 variables with 25 observations
```

Before we can use this data, we need to subset the data to include numeric variables only. This subsetted data will be used to run a for loop with a Shapiro-Wilk test to test for normality.

```{r results='hide'}
# drop metadata, response, random effect, categorical data
df.sloth2 <- subset.data.frame(df.sloth, select = -c(SITE_ID, LONGITUDE, LATITUDE, LAND_USE, patch_grou, OCCURRENCE))
df.sloth2$PR <- as.numeric(df.sloth2$PR)
str(df.sloth2)

```

### Testing Data for Normality
Shapiro-Wilk test to test for normality. Variables that don't pass will need to be transformed before we can use them.

```{r}

IsItNormal <- function(df, save = F) { ### should be able to accept any dataframe of arbitrary size & names, if all columns contain data that Shapiro-Wilk test likes
  slothtest <- list()
  for(i in seq(dim(df)[2])) { 
    slothtest <- append(slothtest, shapiro.test(df[[i]]))
  }
  mx.slothtest <- matrix(slothtest, byrow = T, ncol = 4)
  df.slothtest <- data.frame(var = as.character(colnames(df)),
                             W = as.numeric(mx.slothtest[,1]),
                             p = as.numeric(mx.slothtest[,2]),
                             row.names = NULL)
  if(save == T) {write.table(df.slothtest, file = paste0("IsItNormal", round(as.numeric(Sys.time())), ".csv"), row.names = F, sep = ",")}
  return(df.slothtest)
}

IsItNormal(df.sloth2)

```

We will also transform the data three ways and run the normality test. This method allows us to see if non-normal data prior to transformtion can be normalized through transforming it.

```{r results='hold'}
df.sloth.log <- as.data.frame(apply(df.sloth2, 2, log))
df.sloth.sqrt <- as.data.frame(apply(df.sloth2, 2, sqrt))
df.sloth.squared <- as.data.frame(apply(df.sloth2, 2, function(x) x^2))

df.slothtest.transformed <- cbind(IsItNormal(df.sloth2),
                                  IsItNormal(df.sloth.log)[,2:3],
                                  IsItNormal(df.sloth.sqrt)[,2:3],
                                  IsItNormal(df.sloth.squared)[,2:3])

colnames(df.slothtest.transformed)[4:5] <- paste0("log.", c("W", "p"))
colnames(df.slothtest.transformed)[6:7] <- paste0("sqrt.", c("W", "p"))
colnames(df.slothtest.transformed)[8:9] <- paste0("squared.", c("W", "p"))

# Table defining the normality at different transformations saved to your drive

write.table(df.slothtest.transformed, file = paste0("AreTransformedSlothsNormal_", round(as.numeric(Sys.time())), ".csv"), row.names = F, sep = ",")

```

If you open the table we wrote above, you will find that 5 variables are never normal, 5 variables that are normal if log transformed, 7 variables that are normal without transformation, 1 variable that's normal if square rooted, and 1 variable that is normal if squared.

Below we have transformed the necessary variables and saved them as their own object to be combined in the subsequent code.

```{r}

# data subsetted removing "never normal columns" based on normality tests 
df.sloth3 <- subset.data.frame(df.sloth2, select = -c(CONTAG, DIST_SEC_F, PATCH_SHAP, PLAND_SF, SIEI))

# Log transform "PATCH_AREA", "DIST_ROAD","PD", "LPI","AREA_WM"
df.sloth4 <- subset.data.frame(df.sloth2, select = c(PATCH_AREA, DIST_ROAD,PD, LPI,AREA_WM))
df.sloth4.log <- log(df.sloth4)
colnames(df.sloth4.log) <- paste0("log", colnames(df.sloth4.log))

# data subset with only normal non-transformed variables
df.sloth.normal <- subset.data.frame(df.sloth3, select = c(DIST_RIP_F,ECON_AM,EDGE,GYRATE_WM,PATCH_GYRA,PR,SHAPE_WM))

# sqrt CWED
df.sloth3.sqrt <- sqrt(df.sloth3["CWED"])
colnames(df.sloth3.sqrt) <- paste0("sqrt", colnames(df.sloth3.sqrt))

# squared SIDI
sq <- function(x){
  x^2
}

df.sloth3sq <- sq(df.sloth3["SIDI"])
colnames(df.sloth3sq) <- paste0("square", colnames(df.sloth3sq))

```

Combine all the transformed and normal data into one along with the categorical variables and metadata we removed originally in the beginning. The "allsloth" data will be the dataset we work from.

```{r}

# Working Dataset
allsloth <- cbind(df.sloth[,1:6], df.sloth4.log, df.sloth3.sqrt, df.sloth.normal)

str(allsloth)

```

### Let's visualize our data

Boxplots for all variables in relation to Sloth Presence/Absence

```{r warning=FALSE}

par(mfrow=c(3,3))
boxplot(logPATCH_AREA ~ OCCURRENCE, data = allsloth, xlab='Absence/Presence', ylab = 'Patch Area')
boxplot(logDIST_ROAD ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Distance to road") 
boxplot(logPD ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Patch Density") #per 100 ha
boxplot(logLPI ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Prop of landscape in patch") #Proportion of landscape in largest patch (%)
boxplot(logAREA_WM ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Mean Area") #Mean area of all patches, weighted by area
boxplot(sqrtCWED ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Density of edge (contrast)") #Density of edge weighted by edge contrast; approaches 0 when contrast is low (m/ha)
boxplot(DIST_RIP_F ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Dist to nearest riparian") #Distance to nearest patch of riparian forest
boxplot(ECON_AM ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "relative prop of edge contrast")
boxplot(EDGE ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Length of edge") #Total length of edge in landscape (m)


par(mfrow=c(3,3))
boxplot(GYRATE_WM ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Mean extent all patches") # Mean extent of all patches, weighted by area
boxplot(PATCH_GYRA ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Exent of patch") #Extent of patch (m)
boxplot(PR ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "# of patch types") #Number of different patch types
boxplot(SHAPE_WM ~ OCCURRENCE, data = allsloth, xlab= "Absence/Presence", ylab = "Mean shape complexity") #Mean shape complexity of all patches, weighted by area

```

### Using GLM (General Linear Model)

We want to know if any of these variables have an effect on presence/absence of sloths. Let's start with a general linear model to explore how sloth presence is a function of all our numeric variables.

```{r warning=FALSE}

# GLM
glm.sloth <- glm(OCCURRENCE ~ logPATCH_AREA + LAND_USE + logDIST_ROAD + logPD + logLPI + logAREA_WM + sqrtCWED + DIST_RIP_F + ECON_AM + EDGE + GYRATE_WM + PATCH_GYRA + PR + SHAPE_WM, data=allsloth, family=binomial)

summary(glm.sloth)
par(mfrow=c(2,2))
plot(glm.sloth)

```

### Hosmer-Lemeshow Goodness of Fit Test

This test will allow us to evaluate our glm and defines how well our model fits depending on the difference between the model and the observed data.

This is one approach for binary data. Based on our p-value, this model appears to be "ok" and shows that some of our covariates could potentially be significant. But, we haven't accounted for random effects to explain any variance in the response, and the way we can do this with our binary sloth data is to implement a mixed effects model.

```{r}

# Test Fit of Selected GLM
hoslem.test(allsloth$OCCURRENCE, fitted(glm.sloth))

```

To fit a model for the presence or absence of sloths, we will use glmer with family=binomial from the "lmer" package. Using a lmer function would not allow us to define family as "binomial"-- which is what we are looking at with sloth presence/absence. But first, we need to scale the data and identify any collinearity among the variables.

```{r results='hide'}

# Sloth Data is scaled to make sure models run

allslothnotscaled <- allsloth
str(allsloth)

sc <- scale(allsloth[7:19])
allsloth <- cbind(allsloth[1:6], sc)

```

Identifying collinearity for our numeric variables only

```{r results='hold'}

headers <- colnames(allsloth)[7:19]
cor.m <- abs(cor(allsloth[,headers]))
cor.m.above75 <- cor.m > 0.75
cor.m.above75

```

### Bootstrapping

Because we only have 25 observations per variable, we bootstrap the data to get more data to account for singularity.

> Singularity is often a problem with mixed effect models with highly correlated data or not enough data.

```{r}

# bootstrap an additional 100 samples

set.seed(537)
vec.boots <- sample(1:25, 100, replace = T)
allsloth <- allsloth[vec.boots,]

```

Now we run a loop for all of the following iterations 

```{r}

# create formulae to define models

ls.formulae.aov <- paste("OCCURRENCE ~", c( # prefix for all model formulae
                        "logDIST_ROAD * DIST_RIP_F",
                        "logDIST_ROAD + DIST_RIP_F",
                        "logDIST_ROAD * DIST_RIP_F + logAREA_WM",
                        "logDIST_ROAD * DIST_RIP_F * logAREA_WM",
                        "logDIST_ROAD * DIST_RIP_F + EDGE",
                        "logDIST_ROAD * DIST_RIP_F * GYRATE_WM",
                        "logDIST_ROAD * DIST_RIP_F + GYRATE_WM",
                        "logDIST_ROAD * DIST_RIP_F * PATCH_GYRA",
                        "logDIST_ROAD * DIST_RIP_F + PATCH_GYRA",
                        "logDIST_ROAD * DIST_RIP_F * PR",
                        "logDIST_ROAD * DIST_RIP_F + PR",
                        "logDIST_ROAD * DIST_RIP_F * SHAPE_WM",
                        "logDIST_ROAD * DIST_RIP_F + SHAPE_WM"
                      ))

ls.formulae <- paste(ls.formulae.aov, "+ (1 | LAND_USE)")

```

We create a function to run all of our interested formulas. Warning for running this formula-- if data comes back with errors, it will crash the loop and need to address those formulas or write them out.

>Keep in mind, this is considered "data mining." Investigating this many models at once will usually return variables that are "significant." This is bad practice in science.

```{r results='hide', warning=FALSE}

# make the glmer

ls.glmer <- lapply(ls.formulae, function(x) {print(x); glmer(x, data = allsloth, family = binomial)})

```

### AIC Model Comparison

Extract AIC values to identify best fit models from our iteration.

```{r}

# extract AIC
ls.aic <- sapply(ls.glmer, function(x) summary(x)$AICtab[1])

```

Convert the list to a dataframe for ease of use and identification of model.

```{r}

# convert to df, pull out the good models

df.modeleval <- data.frame(model = ls.formulae.aov, aic = ls.aic, index = 1:length(ls.formulae.aov), random = "PatchGroup")
df.modeleval$model <- as.character(df.modeleval$model)

df.modeleval$daic <- df.modeleval$aic - min(df.modeleval$aic)
df.modeleval <- df.modeleval[order(df.modeleval$daic),]
head(df.modeleval)

```

So what does the best model look like?

```{r}

topmodel <- ls.glmer[[10]]
nexttopmodel <- ls.glmer[[6]]

summary(topmodel)
summary(nexttopmodel)

plot(topmodel)
plot(nexttopmodel)

```









