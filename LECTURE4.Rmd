---
title: "Likelihood!"
author: "NRES 746"
date: "September 18, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


In the last class, we talked about simulating data from models. This is often called *forward* modeling- that is, we take a model and use it to predict emergent patterns. The process flow goes something like this:

$Model \rightarrow Data$

In this class, we will talk about *inference* using the method of **maximum likelihood**. Inference is in some ways the opposite process (sometimes called *inverse* modeling). We are using the data to say something about the model. 

$Data \rightarrow Model$

In many ways, these two processes -- *simulation* and *inference* -- are inter-related. Simulation can help us to make better inferences, and inference can help us to construct better simulation models!

Let's see how simulation can help us to make inference. This leads directly into the core idea of maximum likelihood!

## Using data simulation to make inferences

Let's use the "mtcars" data for this example: (note: some code borrowed from [here](http://stats.stackexchange.com/questions/142443/simple-non-linear-regression-problem))

```{r echo=FALSE}
data(mtcars)

plot(mpg~disp, data = mtcars, las = 1, pch = 16, xlab = "Displacement", ylab = "Miles/Gallon")



```


Looks nonlinear, with relatively constant variance across parameter space. So let's see if we can build a model that could possibly produce these data!!

Since this looks a little like an exponential decline, let's first build a function that can generate data that follows that deterministic function:

$mpg = N\left \{  intercept\cdot e^{slope\cdot displacement} ,Variance\right \}$

Or, if we package the parameters simply as params a, b, and c:

$mpg = N\left \{  a\cdot e^{b\cdot displacement} ,c\right \}$

```{r}

Deterministic_component <- function(xvals,a,b){
  yexp <- a*exp(b*xvals)        # deterministic exponential decay
  return(yexp)
}

DataGenerator_exp <- function(xvals,params){
  yexp <- Deterministic_component(xvals,params$a,params$b)  # get signal
  yvals <- rnorm(length(yexp),yexp,sqrt(params$c))     # add noise
  return(yvals)
}

```

Let's test this function to see if it does what we want:

```{r}

xvals=mtcars$disp    # xvals same as data (this is a "fixed effect", so there is no random component here- we can't really "sample" x values)
params <- list()  
params$a=30
params$b=-0.005   # = 1/200
params$c=1

yvals <- DataGenerator_exp(xvals,params)

plot(yvals~xvals)


```


Okay, looks reasonable. Now, let's write a function to generate multiple replicate datasets from a particular model and make boxplots describing the plausible data produced by the model across measured parameter space.

```{r}

PlotRangeOfPlausibleData <- function(xvals,params,reps){ 
  samplesize <- length(xvals)
  results <- array(0,dim=c(samplesize,reps))   # storage array for results
  for(i in 1:reps){
    yvals <- DataGenerator_exp(xvals,params)
    results[,i] <- yvals
  }
      # now make a boxplot of the results
  boxplot(lapply(1:nrow(results), function(i) results[i,]),at=xvals, xaxt="n",main="Plausible data under this model",ylab="mpg",xlab="Displacement",boxwex=6)
  cleanseq <- (seq(0,max(round(xvals/100)),length=(max(round(xvals/100)))+1))*100
  axis(1,at=cleanseq,labels = cleanseq)    # label the x axis properly
  
}

```


Let's try it out!

```{r}
reps <- 1000    # number of replicate datasets to generate

PlotRangeOfPlausibleData(xvals,params,reps)

```


Now we can overlay the data and see how well we did!

```{r}
real_yvals <- mtcars$mpg
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col="green")
```


Okay, not very good yet. Let's see if we can improve this by changing the parameters. Let's increase the intercept and reduce the slope:

```{r}
params$a=40       # was 30
params$b=-0.001   # was 0.005

    
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col="green")    # overlay the real data

```


Oops- we overshot!! Let's find something in the middle!

```{r}
params$a=33       # was 40
params$b=-0.002   # was 0.001

    
PlotRangeOfPlausibleData(xvals,params,reps)
points(xvals,real_yvals,pch=20,cex=3,col="green")    # overlay the real data
```


Much better! This model could plausibly generate most of these data!

So, we have used simulation, along with trial and error, to infer parameter values for our model!


## Computing data *likelihood* 

First of all, what does "data likelihood" really mean?  Formally,

$\pounds (Model|obs.data) \equiv  Prob(obs.data|Model)$

### Definition, in plain English!

The likelihood of a set of parameters $\theta $ given some observed data is equal to the *probability* of observing these data given those particular parameter values. In this way, likelihood is a quantitative measure of *goodness-of-fit*. Higher likelihoods correspond to a higher probability of the model producing the observed data.  

### Worked example

Let's go through an example! For simplicity, let's stick with the cars example for now. 

For simplicity, let's consider only the first observation:

```{r}

obs.data <- mtcars[1,c("mpg","disp")]
obs.data

```


Remember, we are considering the following data generating model:

$mpg = N\left \{  a\cdot e^{b\cdot displacement} ,c\right \}$

Let's assume for a second that the parameters we selected in our trial-and-error exercise above are the true parameters. What is the expected value of our observation under this data generating model.

```{r}
############
# "best fit" parameters from above
############

params$a=33       # was 40
params$b=-0.002   # was 0.001
params$c=1

params

expected_val <- Deterministic_component(obs.data$disp,params$a,params$b)
expected_val

```


Okay we now know our expected (mean) value for mpg for a car with displacement of 160 cubic inches. We also know the observed mpg for a car with a displacement of 160 cubic inches: it was 21 mpgs. Since the model also specifies the variance (1) we can compute the probability of observing a car with 21 mpgs under our model.

```{r}
mean = expected_val   # 23.96
stdev = sqrt(params$c)

curve(dnorm(x,mean,stdev),10,30,xlab="mpg",ylab="probability density")   # probability density
abline(v=obs.data$mpg,col="red",lwd=2)

```


Now it is straightforward to compute the likelihood. We just need to know the probability density where the red line (observed data) intersects with the normal density curve above:

```{r}
likelihood = dnorm(obs.data$mpg,mean,stdev)
likelihood
```


Now let's consider a second observation as well!

```{r}

obs.data <- mtcars[c(1,3),c("mpg","disp")]
obs.data

par(mfrow=c(1,2))  # set up graphics!

for(i in 1:nrow(obs.data)){
  curve(dnorm(x,Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c)),10,30,xlab="mpg",ylab="probability density")   # probability density
  abline(v=obs.data$mpg[i],col="red",lwd=2)
}

```


What is the likelihood of observing both of these data points???

$Prob(obs.data_{1}|Model])\cdot Prob(obs.data_{2}|Model])$

```{r}
Likelihood <- dnorm(obs.data$mpg[1],Deterministic_component(obs.data$disp[1],params$a,params$b),sqrt(params$c)) *
              dnorm(obs.data$mpg[2],Deterministic_component(obs.data$disp[2],params$a,params$b),sqrt(params$c))  
Likelihood
```


Let's consider four observations:

```{r}
obs.data <- mtcars[c(1,3,4,5),c("mpg","disp")]
obs.data

par(mfrow=c(2,2))  # set up graphics!

for(i in 1:nrow(obs.data)){
  curve(dnorm(x,Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c)),10,30,xlab="mpg",ylab="probability density")   # probability density
  abline(v=obs.data$mpg[i],col="red",lwd=2)
}

```


What is the combined likelihood of all of these four data points, assuming each observation is independent...

```{r}
Likelihood <- 1     # initialize the likelihood
for(i in 1:nrow(obs.data)){
  Likelihood <- Likelihood * dnorm(obs.data$mpg[i],Deterministic_component(obs.data$disp[i],params$a,params$b),sqrt(params$c))
}
Likelihood
```

Alternatively, we can use the "prod" function in R:

```{r}

Likelihood <- prod(dnorm(obs.data$mpg,Deterministic_component(obs.data$disp,params$a,params$b),sqrt(params$c)))
Likelihood
```


Okay, so it should be fairly obvious how we might get the likelihood of the entire dataset. Assuming independence of observations of course!

```{r}
full.data <- mtcars[,c("mpg","disp")]
Likelihood <- prod(dnorm(full.data$mpg,Deterministic_component(full.data$disp,params$a,params$b),sqrt(params$c)))
Likelihood
```

You may notice that that's a pretty small number. When you multiply lots of really small numbers together, we get much smaller numbers. This can be very undesirable, especially when you run into computational errors (e.g., arithmetic overflow). For this reason, and because we generally like sums rather than products, we generally log-transform likelihoods. As you recall,

$log(a\cdot b\cdot c)=log(a)+log(b)+log(c)$

Log transformations just make it easier to work with likelihoods! 

In fact, the probability distribution functions in R make it extra easy for us to work with log likelihoods, using the 'log=TRUE' option!

```{r}
Log.Likelihood <- sum(dnorm(full.data$mpg,Deterministic_component(full.data$disp,params$a,params$b),sqrt(params$c),log=TRUE)) 
Log.Likelihood  
exp(Log.Likelihood)   # we can convert back to likelihood if we want...
```

## Maximum Likelihood Estimation (MLE)

Maximum likelihood estimation is a general, flexible and generally robust method for drawing inference about models from data.

The basic idea is simple: given a data generating model, search for the set of parameters that maximizes the data likelihood!

The steps of a typical MLE analysis are as follows:

1. Build a likelihood function
2. Use numerical optimization algorithms to find the parameters that maximize the likelihood function
3. Use the shape of the likelihood function to make inference about parameter uncertainty (e.g., confidence intervals)

Okay, so what is a *likelihood function*? We basically already have this for the cars example: it is just a function that produces a joint likelhood value, given (1) a **dataset** and (2) a **data generating function**.

```{r}
LogLikFunction <- function(params,df,yvar,xvar){
  LogLik <- sum(dnorm(df[,yvar],Deterministic_component(df[,xvar],params['a'],params['b']),sqrt(params['c']),log=TRUE))
  return(LogLik)
}
LogLikFunction(unlist(params),df=mtcars,yvar="mpg",xvar="disp")
```

Now that we have a likelihood function, we need to search parameter space for the parameter set that maximizes the log likelhood. Luckily, there are lots of *computational algorithms* that can do this. We will look at this in detail in the next lecture. For now, we just need to know that they exist, and that they can be harnessed using the 'optim' function in R.

Let's find the MLE for the three parameters in the cars example!!

```{r}
MLE <- optim(fn=LogLikFunction,par=unlist(params),df=mtcars,yvar="mpg",xvar="disp",control=list(fnscale=-1))  # note, the control param is set so that "optim" maximizes rather than minimizes the Log-likelihood. 
```

Now, we can get the MLEs for the three parameters:

```{r}
MLE$par
```

We can also get the log likelihood for the best model

```{r}
MLE$value
```

Let's look at goodness-of-fit for the best model!

```{r}
bestParams <- as.list(MLE$par)

xvals=mtcars$disp
yvals <- mtcars$mpg
PlotRangeOfPlausibleData(xvals,bestParams,1000)
points(xvals,yvals,pch=20,cex=3,col="green")

```


Now this isn't such a bad looking model!


### Generating initial values

[not finished!]




























