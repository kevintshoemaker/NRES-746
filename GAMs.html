<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Israel Borokini" />

<meta name="date" content="2016-11-20" />

<title>Generalized Additive Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus2.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Generalized Additive Models</h1>
<h4 class="author"><em>Israel Borokini</em></h4>
<h4 class="date"><em>November 20, 2016</em></h4>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Regression analysis is used to investigate the relationships between and among response and predictor variables. Once a reliable relationship has been established among the variables, the analysis can be used to estimate how changes in an independent variable affect the response variable.</p>
<p>Regression models have three components: 1. Response, Y [also called regressand, criterion, or endogenous dependent variables] 2. Predictors, X [also called regressors, independent, exogenous or explanatory variables] 3. Unknown parameters, ?? The typical relationship among the three is: Y ~ f(X, ??) or E(Y|X) = f(X, ??) While function f, must be specified.</p>
<p>Regression models are used for describing the correlation and directionality of the variables (parametric, semi-parametric or non-parametric), making inference on the relationship between predictors and response variables, evaluating the relative strength (or influence) of each predictor on the response, and predicting the future effect of changes in response variables in relation to changes in predictor variables. Hence, a good model should perform three functions: description, inference and prediction.</p>
<div id="linear-regression" class="section level2">
<h2>Linear regression</h2>
<p>There are many extensions of linear regression: Linear regression is used to describe the relationship between dependent variables and one or many independent variables. If only one dependent variable (interval or ratio) is used with one independent variable, it is called simple linear regression, given as: Y = ?? + ??*X + ??</p>
<p>If more than one independent variable is used, but for only one dependent variable (interval or ratio), then it is called multiple linear regression, given as: Y = ?? + ??_1*X_1 + ??_2*X_2 + . + ??_n*X_n + ?? Where ?? ~ N (0,??^2) ?? and ?? are intercept and slope respectively, to be determined at CI = 95% using ordinary least squares (OLS) method; ?? - residual error, to cover unexplained information, assumed to be normally distributed with mean of 0 and ??^2; N is the sample size However, multiple linear regression [also called multivariable linear models] is different from multivariate linear regression [also called general linear models], which involves prediction of two or more correlated dependent variables using the same set of independent variables.</p>
<p>Logistic regression is used when one binary (or categorical) dependent variable is used with two or more independent variables; while ordinal regression is used when the dependent variable is ordinal data. Multinominal regression or discriminant analysis are used for only one dependent nominal variable. Probit model is a special case of logistic regression where dependent variable has only two values, for example, present or absent.</p>
<p>There are several other extensions of the linear models including generalized linear models (GLMs), heteroscedastic models or hierarchical linear models, etc. some of which are discussed later.</p>
</div>
<div id="assumptions-of-linear-models-parametric-models" class="section level2">
<h2>Assumptions of linear models (parametric models)</h2>
<ol style="list-style-type: decimal">
<li>Linearity: predictors and response variables must have linear relationship; hence linear regressions are sensitive to outliers and data inaccuracy. This can be tested with scatterplots</li>
<li>Normality: all variables to be used must be normally distributed. This can be checked using histogram, Q-Q plot, partial residual plots, Shapiro-Wilk test (for parametric regression), or Anderson-Darling test.</li>
<li>Little or no multicollinearity: multicollinearity means there is strong correlations among predictors, indicated as higher r values closer to 1.0. Singularity is when r = 1.0. It increased dimensionality of models while adding no new information, increases noise and leads to bias predictions. Multicollinearity can be detected using:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Correlation matrix: (correlation values &gt;1 indicates multicollinearity)</li>
<li>Tolerance measures: T = 1 - R^2 (T &lt; 0.1 indicates multicollinearity)</li>
<li>Variance inflation factor: VIF = 1/T (VIF &gt;100 indicates multicollinearity)</li>
<li>Condition index (values ???10 indicates multicollinearity) Depending on the studied system, some of the correlated variables should be removed from the model.</li>
</ol>
<ol start="4" style="list-style-type: decimal">
<li>Autocorrelation: This is statistical dependence among residuals, given as: y(x+1) = y(x). This can be detected using scatter plots, or running Durbin-Watson’s d test, in which d values &gt; 2.5 indicates autocorrelation.</li>
<li>Homoscedasticity: if the residual plots have the same width for all values of the response variable, then the data is homoscedastic. In other words, variance should be constant for the errors of all response variable data points, regardless of the predictors. See Figure 1. This can be detected using scatter plots or Goldfeld-Quandt test.</li>
<li>The error is assumed to be normally distributed with mean of 0 and ??^2 for predictors.</li>
<li>The general rule of thumb is that there should be 20 dependent values to 1 independent value. Model transformations (a little digression here.) Transformations are used if the data is non-normal. See Figure 2 for types of transformations. Square root transformation is used when the moderately and positively skewed. Substantial non-normal positively skewed data is corrected by log transformation. Inverse transformation is used to correct severe non-normal positively skewed data. Data reflection should be done to negatively skewed data before transformation. In case of heteroscedastic data, unknown parameters should be estimated using general least squares (GLS) or iteratively weighted least squares (IRLS).</li>
</ol>
</div>
<div id="types-of-regression-models" class="section level2">
<h2>Types of regression models</h2>
<ol style="list-style-type: decimal">
<li>Parametric model: These models adhere strictly to the linear model assumptions described earlier, such that average change in response variables is proportional to change in predictors. All linear regression models and their extensions are examples of parametric models.</li>
<li>Non-parametric model: this type of models assumes no relationship among predictors and response variables. In order words, plots show a curved relationship between variables. Non-parametric models include kernel regression and non-parametric multiplicative regression. This becomes the last resort when transformation does not ensure that data meets assumptions of linear regression. Unknown parameters are estimated using nonlinear least squares.</li>
<li>Semi-parametric model: This is a case where the shape of relationship among variables are determined by the data, hence the relationship is not restricted to any shape. Examples of this include additive models, generalized additive models.</li>
</ol>
</div>
<div id="generalized-linear-models" class="section level2">
<h2>Generalized linear models</h2>
<p>GLM is a generalization of linear regression to allow for modelling of response variables that have non-normal error distribution. This is operated by relating the predictor variables to the mean of response variable, via a link function g. In this way, different forms of linear regression are unified into one model. The developers, Drs. John Nelder and Robert Wedderburn, recommended using maximum likelihood estimation for the unknown parameters. Kindly note that there are many types of link function, depending on the distribution family, but the predictor variables must be linear.</p>
<p>Do not confused generalized linear model with general linear model. While both models use least square method to estimate unknown parameters, residual distribution in general linear model is assumed to be Gaussian, but in GLM, residual distribution can belong to any exponential distribution family, e.g. poisson, logistic, linear, binomial, negative binomial or gamma. In general linear model, Y must be continuous data, while in GLM, it can be categorical, continuous, interval, etc. In general linear model, residual distribution must be linear, but GLM accommodates non-linear residual distribution, as in binomial response. Also, in general linear model, OLS is not relaxed as errors are fit to linear distribution, but in GLM, OLS is relaxed and errors can be modeled to fit the model.</p>
</div>
</div>
<div id="additive-models" class="section level1">
<h1>Additive models</h1>
<p>This was developed by Dr. Stone in 1985, to estimate additive approximation to multivariate regression function. Therefore, it helps reduce the effect of dimensionality in the model by using univariate smoother. Relationships among variables are explained by individual terms estimates.</p>
</div>
<div id="generalized-additive-models" class="section level1">
<h1>Generalized additive models</h1>
<p>GAM is a semi-parametric extension of GLM, which makes assumptions that link functions are additive and components are smooth. Statistically, the main difference between GLM and GAM is that in GAM, linear predictor is not forced to be linear, but is the sum of smoothing functions. GAM can handle non-linear, linear and non-monotonic relationships between response and predictor variables. GAM was developed by Drs. Hastie and Tibshirani, who were students of the professors who developed GLM. Essentially, shape of predictor functions is determined by the data, therefore no assumption is made on the specific link function for error distribution. GAM is given as: Y = ?? + f(X) + ?? Where ??*X is replaced by a smoothing curve f(X) which is derived from the model. In the classical GAM, the response variable is scalar, but Yee and Wild (1996) developed vector smoothing for model classes including multiple logistic regression for nominal response variables, bivariate probit, bivariate logistic, and continuation ratio model, etc. This extension of GAM is called vector generalized additive model. (VGAM).</p>
<div id="how-gam-works" class="section level2">
<h2>How GAM works</h2>
<p>The predictors are first separated into knot, k (sections), low order polynomials or spline functions are used to fit the data into each section independently. Then, functions of all k are added to predict the link function (smoothing), which is why it is called additive model. Smoothing of knots is done using “loess” and “splines”, depending on R package used. Model fitting is based on likelihood methods (e.g. AIC scores).</p>
</div>
<div id="uniqueness-of-gam" class="section level2">
<h2>Uniqueness of GAM</h2>
<ol style="list-style-type: decimal">
<li>The non-parametric (unspecified) function f of the predictor variables x</li>
<li>GAM is flexible and provide excellent fit for both linear and nonlinear relationships (multiple link functions)</li>
<li>Just like GLM, GAM can be applied to any exponential distribution family</li>
<li>Regularization of predictor functions helps to avoid over-fitting. Application and advantages of GAM</li>
<li>Very powerful for prediction and interpolation</li>
<li>Highly used in SDMs and ENMs (Elith et al. 2006)</li>
<li>Analogous to hinge feature of maxent algorithm (Phillips et al. 2006)</li>
<li>Building optimization models</li>
<li>Comparatively GAMs shows lower AIC scores and explained higher deviance than GLMs</li>
<li>Applied in Genetics, epidemiology, molecular biology, air quality and medicine (Dominici et al. 2002).</li>
</ol>
</div>
<div id="r-packages-that-implement-gam" class="section level2">
<h2>R packages that implement GAM</h2>
<ol style="list-style-type: decimal">
<li>gdxrrw (can read or write GDX files)</li>
<li>mgcv - requires package “nlme”</li>
<li>gam (original GAM package developed by Hastie and Tibshirani) - requires “splines” package</li>
<li>mda - “bruto” function</li>
<li>gamstools</li>
</ol>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>Example and exercise are given in the R codes provided below</p>
<pre class="r"><code>library(mgcv)</code></pre>
<pre><code>## Warning: package &#39;mgcv&#39; was built under R version 3.3.2</code></pre>
<pre><code>## Loading required package: nlme</code></pre>
<pre><code>## Warning: package &#39;nlme&#39; was built under R version 3.3.2</code></pre>
<pre><code>## This is mgcv 1.8-16. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;.</code></pre>
<pre class="r"><code>library(nlme)

data(&quot;airquality&quot;)

#setwd(&quot;C:\\Users\\tbisrael\\Documents\\Academic Classes\\Fall 2016\\NRES 746\\GAMs&quot;)
ozone.data&lt;-  airquality  #read.csv(&quot;ozone.csv&quot;, header = T)
head(ozone.data) # ozone is the response variable while the others are predictors</code></pre>
<pre><code>##   Ozone Solar.R Wind Temp Month Day
## 1    41     190  7.4   67     5   1
## 2    36     118  8.0   72     5   2
## 3    12     149 12.6   74     5   3
## 4    18     313 11.5   62     5   4
## 5    NA      NA 14.3   56     5   5
## 6    28      NA 14.9   66     5   6</code></pre>
<pre class="r"><code>ozone.data$rad &lt;- ozone.data$Solar.R
ozone.data$temp &lt;- ozone.data$Temp
ozone.data$wind &lt;- ozone.data$Wind
ozone.data$ozone &lt;- ozone.data$Ozone

ozone.data &lt;- ozone.data[,c(&quot;ozone&quot;,&quot;rad&quot;,&quot;wind&quot;,&quot;temp&quot;)]

pairs(ozone.data)</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code># we can see that the relationships among the variables are not linear

plot(ozone.data$ozone)</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code># and the response variable does not obey linear regression assumption</code></pre>
</div>
<div id="lets-first-play-around-linear-regression" class="section level2">
<h2>Let’s first play around linear regression</h2>
<pre class="r"><code>model1&lt;-lm(ozone~rad+temp+wind, data = ozone.data)
summary(model1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = ozone ~ rad + temp + wind, data = ozone.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.485 -14.219  -3.551  10.097  95.619 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -64.34208   23.05472  -2.791  0.00623 ** 
## rad           0.05982    0.02319   2.580  0.01124 *  
## temp          1.65209    0.25353   6.516 2.42e-09 ***
## wind         -3.33359    0.65441  -5.094 1.52e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.18 on 107 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6059, Adjusted R-squared:  0.5948 
## F-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>summary.aov(model1)</code></pre>
<pre><code>##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## rad           1  14780   14780   32.94 8.95e-08 ***
## temp          1  47378   47378  105.61  &lt; 2e-16 ***
## wind          1  11642   11642   25.95 1.52e-06 ***
## Residuals   107  48003     449                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 42 observations deleted due to missingness</code></pre>
<pre class="r"><code>opar &lt;- par(mfrow = c(2, 2))
plot(model1)  </code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># the results suggests non-normality of residuals.

model2&lt;-glm(ozone~rad+temp+wind, data = ozone.data)
summary(model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = ozone ~ rad + temp + wind, data = ozone.data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -40.485  -14.219   -3.551   10.097   95.619  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -64.34208   23.05472  -2.791  0.00623 ** 
## rad           0.05982    0.02319   2.580  0.01124 *  
## temp          1.65209    0.25353   6.516 2.42e-09 ***
## wind         -3.33359    0.65441  -5.094 1.52e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 448.6242)
## 
##     Null deviance: 121802  on 110  degrees of freedom
## Residual deviance:  48003  on 107  degrees of freedom
##   (42 observations deleted due to missingness)
## AIC: 998.72
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<pre class="r"><code>opar &lt;- par(mfrow = c(2, 2))
plot(model2)  </code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code># the results suggests non-normality of residuals.

## Both LM and GLM analyses show that the predictors are significantly different, with AIC = 998.63, in spite of the fact that the predictors do not obey linear regression assumptions. This can lead to serious misinterpretation and prediction of relationships.</code></pre>
</div>
<div id="lets-go-to-gams" class="section level2">
<h2>Let’s go to GAMs</h2>
<pre class="r"><code>model3&lt;-gam(ozone ~ s(rad)+s(temp)+s(wind), data = ozone.data)
summary(model3)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad) + s(temp) + s(wind)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.663   25.32   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df      F  p-value    
## s(rad)  2.760  3.447  3.967  0.00849 ** 
## s(temp) 3.833  4.753 11.613 8.45e-09 ***
## s(wind) 2.910  3.657 13.695 1.59e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.723   Deviance explained = 74.7%
## GCV =  338.9  Scale est. = 306.83    n = 111</code></pre>
<pre class="r"><code>anova.gam(model3)  # kindly note: this gives the same results as in the summary() function</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad) + s(temp) + s(wind)
## 
## Approximate significance of smooth terms:
##           edf Ref.df      F  p-value
## s(rad)  2.760  3.447  3.967  0.00849
## s(temp) 3.833  4.753 11.613 8.45e-09
## s(wind) 2.910  3.657 13.695 1.59e-08</code></pre>
<pre class="r"><code>AIC(model3)</code></pre>
<pre><code>## [1] 962.596</code></pre>
<pre class="r"><code>## gave lower AIC of 962.298 compared to that of GLM
opar &lt;- par(mfrow = c(2, 2))
plot(model3,residuals = TRUE)  # plots model output, showing partial residuals
plot(model3,seWithMean = TRUE) # plots model output, with intercept confidence intervals</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>plot(model3,scheme = 1,unconditional = TRUE)  # another plotting method. Note that &quot;scheme=2&quot; is the default shown in previous plots</code></pre>
<pre><code>## Warning in plot.gam(model3, scheme = 1, unconditional = TRUE): Smoothness
## uncertainty corrected covariance not available</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<pre class="r"><code>gam.check(model3)  # checking the smoothing basis dimensions by running some diagnostic checks</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-3-3.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-3-4.png" width="672" /></p>
<pre><code>## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 5 iterations.
## The RMS GCV score gradient at convergence was 0.0004966112 .
## The Hessian was positive definite.
## Model rank =  28 / 28 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##            k&#39;   edf k-index p-value
## s(rad)  9.000 2.760   0.963    0.32
## s(temp) 9.000 3.833   0.788    0.00
## s(wind) 9.000 2.910   1.036    0.64</code></pre>
<pre class="r"><code># K&#39; was observed to be 9.0 and K&#39; = K - 1. So, even though we didn&#39;t set K, the default k used by the algorithm was 10

# the default method for estimating smoothness is GCV, which is a cross-validation method. Let&#39;s change it to a likelihood method using REML

model3a&lt;-gam(ozone ~ s(rad)+s(temp)+s(wind), data = ozone.data, method = &quot;REML&quot;)
summary(model3a)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad) + s(temp) + s(wind)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.677    25.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df      F  p-value    
## s(rad)  1.668  2.087  5.248  0.00611 ** 
## s(temp) 3.415  4.249 12.773 6.03e-09 ***
## s(wind) 3.383  4.211 12.746 7.56e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.718   Deviance explained =   74%
## -REML = 473.63  Scale est. = 312.35    n = 111</code></pre>
</div>
<div id="lets-add-interaction-terms-as-we-would-expect-in-natural-systems" class="section level2">
<h2>let’s add interaction terms, as we would expect in natural systems</h2>
<pre class="r"><code>wt&lt;-ozone.data$wind*ozone.data$temp
model4&lt;-gam(ozone ~ s(rad)+s(temp)+s(wind)+s(wt), data = ozone.data)
summary(model4)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad) + s(temp) + s(wind) + s(wt)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.358      31   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F  p-value    
## s(rad)  2.650  3.304 3.168   0.0299 *  
## s(temp) 5.493  6.614 7.590 4.99e-07 ***
## s(wind) 7.562  8.170 5.417 1.15e-05 ***
## s(wt)   6.754  7.685 5.808 6.92e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.815   Deviance explained = 85.3%
## GCV = 259.65  Scale est. = 204.78    n = 111</code></pre>
<pre class="r"><code>anova.gam(model4)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad) + s(temp) + s(wind) + s(wt)
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F  p-value
## s(rad)  2.650  3.304 3.168   0.0299
## s(temp) 5.493  6.614 7.590 4.99e-07
## s(wind) 7.562  8.170 5.417 1.15e-05
## s(wt)   6.754  7.685 5.808 6.92e-06</code></pre>
<pre class="r"><code>AIC(model4) # gave AIC value of 927.247</code></pre>
<pre><code>## [1] 928.3013</code></pre>
<pre class="r"><code>plot(model4,residuals = TRUE)  # plots model output, showing partial residuals</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-4-1.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-4-2.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-4-3.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-4-4.png" width="672" /></p>
<pre class="r"><code># the results showed &quot;rad&quot; predictor to be less significantly different under non-linear assumptions, and the plot also confirms it.
plot(model4,seWithMean = TRUE) # plots model output, with intercept confidence intervals</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-4-5.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-4-6.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-4-7.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-4-8.png" width="672" /></p>
<pre class="r"><code>gam.check(model4)</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-4-9.png" width="672" /></p>
<pre><code>## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 8 iterations.
## The RMS GCV score gradient at convergence was 0.001522209 .
## The Hessian was positive definite.
## Model rank =  37 / 37 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##            k&#39;   edf k-index p-value
## s(rad)  9.000 2.650   0.902    0.16
## s(temp) 9.000 5.493   0.760    0.00
## s(wind) 9.000 7.562   1.142    0.90
## s(wt)   9.000 6.754   1.134    0.92</code></pre>
<pre class="r"><code># the results from this diagnostic test shows that the default k is too low.

# compare between the two models
anova(model3,model4, test = &quot;F&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: ozone ~ s(rad) + s(temp) + s(wind)
## Model 2: ozone ~ s(rad) + s(temp) + s(wind) + s(wt)
##   Resid. Df Resid. Dev     Df Deviance      F    Pr(&gt;F)    
## 1    98.143      30836                                     
## 2    84.227      17926 13.916    12910 4.5301 5.357e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># results showed significant difference between the two models, which could be due to the interaction term added to &quot;model 4&quot; 
AIC(model3,model4) # could produce AIC for all the models at the same time. Clearly, the interaction produced better results for model selection</code></pre>
<pre><code>##              df      AIC
## model3 11.50254 962.5960
## model4 24.45903 928.3013</code></pre>
</div>
<div id="now-we-set-smoothing-parameter-k-for-both-additive-and-interaction-models-models-3-and-4-respectively." class="section level2">
<h2>Now, we set smoothing parameter k for both additive and interaction models (models 3 and 4 respectively).</h2>
</div>
<div id="recall-that-the-default-k-was-10-and-the-gam.check-indicates-that-k-was-too-low-so-we-specify-k-and-check-the-results" class="section level2">
<h2>Recall, that the default k was 10 and the gam.check() indicates that k was too low, so we specify k and check the results</h2>
<pre class="r"><code>model3b&lt;-gam(ozone ~ s(rad, k=20)+s(temp, k=20)+s(wind, k=20), data = ozone.data)
summary(model3b)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad, k = 20) + s(temp, k = 20) + s(wind, k = 20)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.659   25.38   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df      F  p-value    
## s(rad)  2.761  3.461  3.943   0.0087 ** 
## s(temp) 4.101  5.138 10.928 8.94e-09 ***
## s(wind) 2.938  3.708 13.559 1.55e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.724   Deviance explained = 74.9%
## GCV = 338.47  Scale est. = 305.53    n = 111</code></pre>
<pre class="r"><code>gam.check(model3b)</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre><code>## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 7 iterations.
## The RMS GCV score gradient at convergence was 0.0001559654 .
## The Hessian was positive definite.
## Model rank =  58 / 58 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##             k&#39;    edf k-index p-value
## s(rad)  19.000  2.761   0.963    0.30
## s(temp) 19.000  4.101   0.792    0.00
## s(wind) 19.000  2.938   1.040    0.59</code></pre>
<pre class="r"><code>AIC(model3b)</code></pre>
<pre><code>## [1] 962.3911</code></pre>
<pre class="r"><code>## Based on AIC score, k = 20 didn&#39;t improve the results

model3c&lt;-gam(ozone ~ s(rad, k=25)+s(temp, k=25)+s(wind, k=25), data = ozone.data)
summary(model3c)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad, k = 25) + s(temp, k = 25) + s(wind, k = 25)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.404   29.99   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##            edf Ref.df      F  p-value    
## s(rad)   3.007  3.762  3.759   0.0103 *  
## s(temp)  3.605  4.526 16.777 1.81e-11 ***
## s(wind) 22.999 23.851  5.612 2.03e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.802   Deviance explained = 85.6%
## GCV =    302  Scale est. = 218.72    n = 111</code></pre>
<pre class="r"><code>gam.check(model3c)</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<pre><code>## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 10 iterations.
## The RMS GCV score gradient at convergence was 0.0008188915 .
## The Hessian was positive definite.
## Model rank =  73 / 73 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##             k&#39;    edf k-index p-value
## s(rad)  24.000  3.007   0.885    0.14
## s(temp) 24.000  3.605   0.753    0.00
## s(wind) 24.000 22.999   1.152    0.91</code></pre>
<pre class="r"><code>AIC(model3c)  ## a significant reduction in AIC value</code></pre>
<pre><code>## [1] 940.4544</code></pre>
<pre class="r"><code>plot(model3c) ## there is terrible overfitting of &quot;wind&quot; predictor, also, the edf value is too close to k&#39;, so we try again by reducing k for wind</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-5-3.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-5-4.png" width="672" /><img src="GAMs_files/figure-html/unnamed-chunk-5-5.png" width="672" /></p>
<pre class="r"><code>model3d&lt;-gam(ozone ~ s(rad, k=25)+s(temp, k=25)+s(wind, k=10), data = ozone.data)
summary(model3d)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## ozone ~ s(rad, k = 25) + s(temp, k = 25) + s(wind, k = 10)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   42.099      1.659   25.37   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df      F  p-value    
## s(rad)  2.762  3.463  3.941  0.00871 ** 
## s(temp) 4.088  5.126 10.943 8.89e-09 ***
## s(wind) 2.919  3.667 13.706 1.52e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.724   Deviance explained = 74.8%
## GCV = 338.47  Scale est. = 305.63    n = 111</code></pre>
<pre class="r"><code>gam.check(model3d)</code></pre>
<p><img src="GAMs_files/figure-html/unnamed-chunk-5-6.png" width="672" /></p>
<pre><code>## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 7 iterations.
## The RMS GCV score gradient at convergence was 0.001249363 .
## The Hessian was positive definite.
## Model rank =  58 / 58 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##             k&#39;    edf k-index p-value
## s(rad)  24.000  2.762   0.963    0.28
## s(temp) 24.000  4.088   0.791    0.02
## s(wind)  9.000  2.919   1.039    0.60</code></pre>
<pre class="r"><code>AIC(model3d) ## AIC values shot up again</code></pre>
<pre><code>## [1] 962.3976</code></pre>
<pre class="r"><code>AIC(model3, model3a, model3b, model3c, model3d) # compare AIC values for all fitted models</code></pre>
<pre><code>##               df      AIC
## model3  11.50254 962.5960
## model3a 12.54705 967.8031
## model3b 11.80063 962.3911
## model3c 31.61090 940.4544
## model3d 11.76890 962.3976</code></pre>
</div>
</div>
<div id="exercise" class="section level1">
<h1>Exercise</h1>
<p>Can you fit a GAM with best AIC value, highest deviance explained and less overfitting of the predictors? Hint: use different k values and try different additive and interaction terms (backward model selection). After trying different models, recommend to the researcher what is your recommendation for the researcher on what predictors to measure?</p>
</div>
<div id="suggested-readings" class="section level1">
<h1>Suggested readings</h1>
<ol style="list-style-type: decimal">
<li>Granadeiro JP, Andrade J and Palmeirim JM (2004). Modelling the distribution of shorebirds in estuarine areas using generalized additive models. Journal of Sea Research 52: 227-240</li>
<li>Hale SS, Hughes MH, Strobel CJ, Buffum HW, Copeland JL and Paul JF (2002). Coastal ecological data from the Virginian Biogeographic Province, 1990-1993. Ecology 83 (10): 2942, and Ecological Archives E083-057</li>
<li>Leathwick JR, Elith J and Hastie T (2006). Comparative performance of generalized additive models and multivariate adaptive regression splines for statistical modelling of species distributions. Ecological Modelling 199: 188-196</li>
<li>Bellido JM, Pierce GJ and Wang J (2001). Modelling intra-annual variation in abundance of squid, Loligo forbesi in Scottish waters using generalized additive models. Fisheries Research 52: 23-39</li>
<li>Suarez-Seoane S, Osborne PE and Alonso JC (2002). Large-scale habitat selection by agricultural steppe birds in Spain: identifying species-habitat responses using generalized additive models. Journal of Applied Ecology 39: 755-771</li>
<li>Yee T and Mitchell ND (1991). Generalized additive models in plant ecology. Journal of Vegetation Science 2:587-602</li>
<li>Yee TW and Wild CJ (1996). Vector Generalized Additive models. Journal of Royal Statistical Society B 58 (3): 481-493</li>
<li>Hastie T and Tibshirani R (1986). Generalized additive models (with discussion). Statistical Science 1: 297-318.</li>
<li>Wood S (2002). Package ‘mgcv’. [Online]. Available: <a href="http://cran.rproject.org/web/packages/mgcv/mgcv.pdf" class="uri">http://cran.rproject.org/web/packages/mgcv/mgcv.pdf</a></li>
</ol>
</div>
<div id="references-weblinks-with-r-codes" class="section level1">
<h1>References (Weblinks): with R codes</h1>
<ol style="list-style-type: decimal">
<li><a href="https://stat.ethz.ch/R-manual/Rdevel/library/mgcv/html/summary.gam.html" class="uri">https://stat.ethz.ch/R-manual/Rdevel/library/mgcv/html/summary.gam.html</a></li>
<li><a href="http://multithreaded.stitchfix.com/blog/2015/07/30/gam/" class="uri">http://multithreaded.stitchfix.com/blog/2015/07/30/gam/</a></li>
<li><a href="https://support.sas.com/rnd/app/stat/topics/gam/gam.pdf" class="uri">https://support.sas.com/rnd/app/stat/topics/gam/gam.pdf</a></li>
<li><a href="http://plantecology.syr.edu/fridley/bio793/gam.html" class="uri">http://plantecology.syr.edu/fridley/bio793/gam.html</a></li>
<li><a href="http://geog.uoregon.edu/GeogR/topics/gamex1.html" class="uri">http://geog.uoregon.edu/GeogR/topics/gamex1.html</a></li>
<li><a href="https://rpubs.com/ryankelly/GAMs" class="uri">https://rpubs.com/ryankelly/GAMs</a></li>
<li><a href="http://www.statisticssolutions.com/assumptions-of-linear-regression/" class="uri">http://www.statisticssolutions.com/assumptions-of-linear-regression/</a></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
