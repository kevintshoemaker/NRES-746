<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-08-31" />

<title>Working with Probabilities</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Schedule
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="schedule.html">Course Schedule</a>
    </li>
    <li>
      <a href="labschedule.html">Lab Schedule</a>
    </li>
    <li>
      <a href="Syllabus.pdf">Syllabus</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
    <li>
      <a href="LECTURE3.html">The virtual ecologist</a>
    </li>
    <li>
      <a href="LECTURE4.html">Likelihood</a>
    </li>
    <li>
      <a href="LECTURE5.html">Optimization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Algorithms in R</a>
    </li>
    <li>
      <a href="LAB2.html">Lab 2: Virtual ecologist</a>
    </li>
    <li>
      <a href="LAB3.html">Lab 3: Likelihood</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Working with Probabilities</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>August 31, 2016</em></h4>

</div>


<p>Note: some materials borrowed from White and Morgan’s “STATISTICAL MODELS IN ECOLOGY USING R” course</p>
<p>In the last class, we reviewed basic programming in R, using basic constructs like the programming loop (e.g., “for”, “while”) and random sampling (e.g., using the ‘sample’ function in R) to build our own analyses from first principles. We began the course this way because the ability to understand, use, and build algorithms is absolutely fundamental to modern data analysis.</p>
<p>Also fundamental to modern data analysis is an ability to work with probabilities. Again, many of you will find this a very basic review, but I really want to ensure that all of us are working with solid foundations before we venture into more advanced topics.</p>
<p>The central points:</p>
<ul>
<li>Most traditional statistics utilize tricks &amp; assumptions to ensure the data follow a particular distribution (usually normal)</li>
<li>With more computational power, we are much less limited and can model alternative distributions more easily</li>
</ul>
<div id="basic-probability-rules" class="section level2">
<h2>Basic probability rules</h2>
<div id="classic-urn-example" class="section level3">
<h3>Classic Urn Example</h3>
<p>Consider an Urn filled with blue, red, and green spheres. To make the example more concrete, assume the following:</p>
<ul>
<li>red: 104</li>
<li>blue: 55</li>
<li>green: 30</li>
</ul>
<pre class="r"><code>n_red &lt;- 104
n_blue &lt;- 55
n_green &lt;- 30

allSpheres &lt;- c(n_red,n_blue,n_green)
names(allSpheres) &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;)</code></pre>
<p>What is the probability of drawing a blue sphere?</p>
<pre class="r"><code>P_blue &lt;- allSpheres[&quot;blue&quot;]/sum(allSpheres)
P_blue</code></pre>
<pre><code>##      blue 
## 0.2910053</code></pre>
<p>Let’s generate a vector of probabilities for drawing each type of sphere…</p>
<pre class="r"><code>Prob &lt;- allSpheres/sum(allSpheres)
Prob</code></pre>
<pre><code>##       red      blue     green 
## 0.5502646 0.2910053 0.1587302</code></pre>
<p>What is the probability of drawing a blue <strong>OR</strong> a red sphere?</p>
<pre class="r"><code>as.numeric( Prob[&quot;blue&quot;] + Prob[&quot;red&quot;] )</code></pre>
<pre><code>## [1] 0.8412698</code></pre>
<p>What is the probability of drawing a blue <strong>OR</strong> a red sphere <strong>OR</strong> a green sphere?</p>
<pre class="r"><code>as.numeric( Prob[&quot;blue&quot;] + Prob[&quot;red&quot;] + Prob[&quot;green&quot;] )</code></pre>
<pre><code>## [1] 1</code></pre>
<p>What would it mean if this didn’t sum to 1?</p>
<p>What is the probability of drawing a blue <strong>AND THEN</strong> a red sphere? NOTE: in the next examples, assume that objects are replaced and that the urn is re-randomized before any subsequent draws! This is called “<em>sampling with replacement</em>”</p>
<pre class="r"><code>as.numeric( Prob[&quot;blue&quot;] * Prob[&quot;red&quot;] )</code></pre>
<pre><code>## [1] 0.1601299</code></pre>
<p>What is the probability of drawing a blue and a red in two consecutive draws?</p>
<pre class="r"><code>as.numeric( (Prob[&quot;blue&quot;] * Prob[&quot;red&quot;]) + (Prob[&quot;red&quot;] * Prob[&quot;blue&quot;]) )</code></pre>
<pre><code>## [1] 0.3202598</code></pre>
</div>
<div id="less-classic-urn-example" class="section level3">
<h3>Less-classic urn example</h3>
<p>Now consider an urn filled with blue &amp; red objects of two types: spheres &amp; cubes. To make the example more concrete, assume the following:</p>
<ul>
<li>red sphere: 39</li>
<li>blue sphere: 76</li>
<li>red cube: 101</li>
<li>blue cube: 25</li>
</ul>
<pre class="r"><code>n_red_sphere &lt;- 39
n_blue_sphere &lt;- 76
n_red_cube &lt;- 101
n_blue_cube &lt;- 25

allSpheres &lt;- c(n_red_sphere,n_blue_sphere)
allCubes &lt;- c(n_red_cube,n_blue_cube)
allTypes &lt;- c(allSpheres,allCubes)
allTypes &lt;- matrix(allTypes,nrow=2,ncol=2,byrow=T)
rownames(allTypes) &lt;- c(&quot;sphere&quot;,&quot;cube&quot;)
colnames(allTypes) &lt;- c(&quot;red&quot;,&quot;blue&quot;)
allTypes</code></pre>
<pre><code>##        red blue
## sphere  39   76
## cube   101   25</code></pre>
<pre class="r"><code>Prob_Shape &lt;- apply(allTypes,1,sum)/sum(allTypes)  # note the &#39;apply&#39; function
Prob_Shape</code></pre>
<pre><code>##    sphere      cube 
## 0.4771784 0.5228216</code></pre>
<pre class="r"><code>Prob_Color &lt;- apply(allTypes,2,sum)/sum(allTypes)
Prob_Color</code></pre>
<pre><code>##       red      blue 
## 0.5809129 0.4190871</code></pre>
<p>What is the <em>marginal probability</em> of drawing a red object (and why do we call it a “marginal” probability?)</p>
<pre class="r"><code>Prob_Color[&quot;red&quot;]</code></pre>
<pre><code>##       red 
## 0.5809129</code></pre>
<p>What is the <em>joint probability</em> of drawing an object that is both blue AND a cube?</p>
<pre class="r"><code>as.numeric( Prob_Color[&quot;blue&quot;] * Prob_Shape[&quot;cube&quot;])  </code></pre>
<pre><code>## [1] 0.2191078</code></pre>
<p>Is this correct? If not, why?</p>
<p>Under what circumstances would this be correct?</p>
<p>What is the correct answer?</p>
<pre class="r"><code>(allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;]   </code></pre>
<pre><code>## [1] 0.1037344</code></pre>
<p>What is the probability of drawing an object that is blue <strong>OR</strong> a cube? <span class="math inline">\(Prob(blue\bigcup cube)\)</span></p>
<pre class="r"><code>as.numeric( Prob_Color[&quot;blue&quot;] + Prob_Shape[&quot;cube&quot;])  </code></pre>
<pre><code>## [1] 0.9419087</code></pre>
<p>Is this correct? If not, why?</p>
<p>What is the correct answer?</p>
<pre><code>## [1] 0.7228009</code></pre>
<pre><code>## [1] 0.7228009</code></pre>
<p>What is the <strong>conditional probability</strong> of getting a blue object, given that it is a cube? <span class="math inline">\(Prob(blue|cube)\)</span></p>
<p>This can be expressed as: <span class="math inline">\(Prob(blue|cube) = Prob(blue,cube) / Prob(cube)\)</span></p>
<pre class="r"><code>(allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;] / Prob_Shape[&quot;cube&quot;]</code></pre>
<pre><code>##      cube 
## 0.1984127</code></pre>
<p>Can we now express the joint probability of drawing a blue cube in terms of conditional probabilities?</p>
<p><span class="math inline">\(Prob(blue\bigcap cube) = Prob(blue) * Prob(cube|blue)\)</span></p>
<p>Does this now give us the correct answer?</p>
<pre class="r"><code>as.numeric( Prob_Color[&quot;blue&quot;] * (allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;] / Prob_Color[&quot;blue&quot;] )</code></pre>
<pre><code>## [1] 0.1037344</code></pre>
<p>What is an unconditional probability?</p>
<p>What is the unconditional probability of drawing a blue item, regardless of shape?</p>
<p><span class="math inline">\(Prob(blue|cube)\cdot Prob(cube) + Prob(blue|not cube) \cdot Prob(not cube)\)</span></p>
<pre class="r"><code>as.numeric( ((allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;] / Prob_Shape[&quot;cube&quot;]) * Prob_Shape[&quot;cube&quot;] + 
              ((allTypes/sum(allTypes))[&quot;sphere&quot;,&quot;blue&quot;] / Prob_Shape[&quot;sphere&quot;]) * Prob_Shape[&quot;sphere&quot;]  )</code></pre>
<pre><code>## [1] 0.4190871</code></pre>
<div id="short-exercise-1" class="section level4">
<h4>Short exercise #1</h4>
<p>Can you interpret the above equation in words? Take a moment to try! How does this relate to the marginal probability?</p>
</div>
<div id="short-exercise-2" class="section level4">
<h4>Short exercise #2</h4>
<p>What does it mean if the conditional probability of drawing a blue object (e.g., given it is a cube) is equal to the unconditional probability of drawing a blue item? Can we say anything about the relationship or dependency among color and shape in this example?</p>
</div>
</div>
<div id="bolkers-medical-example" class="section level3">
<h3>Bolker’s medical example</h3>
<p>Suppose the infection rate (prevalence) for a rare disease is one in a million:</p>
<pre class="r"><code>Prob_Disease &lt;- c(1,999999)     # disease prevalence 
Prob_Disease &lt;- Prob_Disease/sum(Prob_Disease)
names(Prob_Disease) &lt;- c(&quot;yes&quot;,&quot;no&quot;)
Prob_Disease</code></pre>
<pre><code>##      yes       no 
## 0.000001 0.999999</code></pre>
<p>Suppose there is a test that never gives a false negative (if you’ve got it you will test positive) but very rarely gives a false positive result (if you ain’t got it, you might still test positive for the disease). Let’s imagine the false positive rate is 1%.</p>
<p>Medical professionals (and patients!) often want to know the probability that a positive-testing patient actually has the disease. This quantity is known as the <em>Positive Predictive Value</em> or PPV. How can we compute this?</p>
<p>Stated another way, we want to know the <em>conditional probability</em> of having the disease given a positive test result.</p>
<p><span class="math inline">\(Prob(Disease|+test)\)</span></p>
<p>What do we have already?</p>
<p>First of all, we know the conditional probability of having a positive test result given the patient has the disease</p>
<p><span class="math inline">\(Prob(+test|Disease) = 1\)</span></p>
<p>Secondly, we know the conditional probability of having a positive test result given the patient doesn’t have the disease</p>
<p><span class="math inline">\(Prob(+test|no Disease) = 0.01\)</span></p>
<p>Third, we know the unconditional probability of having the disease</p>
<p><span class="math inline">\(Prob(Disease) = 0.000001\)</span></p>
<p>Now, can we use basic probability rules to compute the PPV?</p>
<p>What is the unconditional probability of testing positive?</p>
<p><span class="math inline">\(Prob(+test)\)</span></p>
<p>We can either test positive and have the disease or we can test positive and not have the disease…</p>
<p><span class="math inline">\(Prob(+test\bigcap Disease) + Prob(+test\bigcap no Disease)\)</span></p>
<p>Stated another way,</p>
<p><span class="math inline">\(Prob(+test|Disease)*Prob(Disease) + Prob(+test|no Disease)*Prob(no Disease)\)</span></p>
<pre class="r"><code>as.numeric( 1*Prob_Disease[&quot;yes&quot;] + 0.01*Prob_Disease[&quot;no&quot;] )</code></pre>
<pre><code>## [1] 0.01000099</code></pre>
<p>So now we have <span class="math inline">\(Prob(+test|Disease)\)</span>, <span class="math inline">\(Prob(Disease)\)</span>, and <span class="math inline">\(Prob(+test)\)</span>. How can we use these components to compute <span class="math inline">\(Prob(Disease|+test)\)</span>?</p>
<div id="short-exercise-3" class="section level4">
<h4>Short exercise 3</h4>
<p>What’s the the <em>joint probability</em> of being infected and testing positive?</p>
<p>What’s the PPV??</p>
<p>Re-structure your PPV equation so that you consider the positive test result to be the “Data” and the positive disease status to be the “Hypothesis”</p>
<p>The equation you just generated is known as <strong>Bayes’ theorem</strong> (or Bayes’ rule). It forms the basis for <strong>Bayesian statistics</strong></p>
<p>How does this simple rule of probability relate to Bayesian statistics??</p>
<p>As you can see, Bayes’ theorem can be derived by simple probability rules. Why is it so controversial?</p>
</div>
</div>
<div id="frequentism-vs.bayesianism-an-aside" class="section level3">
<h3>Frequentism vs. Bayesianism (an aside)</h3>
<div id="what-is-frequentism" class="section level4">
<h4>What is Frequentism?</h4>
<p>Under this paradigm, the true answers are hidden behind a veil of sampling variability. That is, if we had perfect knowledge (infinite sample size) we would know the answers we seek. Random sampling errors prevent this level of certainty. However, if we know the <em>frequency</em> with which random sampling yields anomalies of various magnitudes, then we can understand and control for the effects of sampling variability. For example, we can set an <span class="math inline">\(\alpha\)</span> level (false-positive rate that we can live with) and then make a “positive” conclusion about a test only if random sampling variability could account for the observed effect size with a frequency at or below the pre-determined <span class="math inline">\(\alpha\)</span> level.</p>
</div>
<div id="what-is-bayesianism" class="section level4">
<h4>What is Bayesianism</h4>
<p>In many ways, Bayesian analyses can report uncertainty in the way that many of us intuitively <em>want</em> to interpret uncertainty– as degrees of belief about alternative possibilities (e.g., possible model structures or parameter values). Given a set of plausible models for describing a system, Bayesian statistics can tell us the degree to which we can believe model A generated the observed data versus the other models in the candidate set. If we fit a regression model, Bayesian statistics can tell us the degree to which we can trust that the true regression parameter <span class="math inline">\(\beta_{1}\)</span> is above zero. That said, Bayesian analyses require specifying a <em>prior</em> probability on all fitted parameters and models. This can pose a philosophical problem: what if you don’t have any prior knowledge? Furthermore, the interpretation of <em>probability</em> itself can get us in philosophical hot water; because Bayesian probabilities are best interpreted as “degree of belief” (scaled to sum to 1), your conclusions from the same data could be very different from mine… But maybe this is okay – at least these differences can be formalized in terms of different prior distributions!</p>
</div>
</div>
<div id="which-paradigm-is-better" class="section level3">
<h3>Which paradigm is better?</h3>
<p>The pragmatic analyst admits that they are both useful, and uses both methods freely!!</p>
<div id="short-exercise-4" class="section level4">
<h4>Short exercise 4</h4>
<p><strong>Likelihood</strong> is defined as: <span class="math inline">\(Prob(data|model)\)</span>. Is the notion of a likelihood inherently Bayesian or Frequentist?</p>
<div class="figure">
<img src="frequentists_vs_bayesians.png" />

</div>
</div>
</div>
</div>
<div id="lets-make-a-deal" class="section level2">
<h2>Let’s make a deal!</h2>
<p>This is a classic example for introducing Bayes rule…</p>
<p>The setup: you are in a game show, called <em>Let’s Make a Deal</em>! There are three doors in front of you. One hides a fancy prize and the other two hide goats.</p>
<div class="figure">
<img src="montyhall.jpg" alt="Monty Hall problem, setup" />
<p class="caption">Monty Hall problem, setup</p>
</div>
<p>You pick door A. Before you see what’s behind door A, the host, Monty Hall, opens door C to reveal a goat. Now you can stay with A or switch to door B. Should you switch? NOTE: no matter which door you choose at first, Monty will always open one of the other doors, and will never open the door with the prize (he knows where the prize is). ALSO NOTE: you can’t keep the goat even if you want it!</p>
<p>Given the new info, we now know that the prize isn’t behind door C. We want to know <span class="math inline">\(Prob(A|info)\)</span> and <span class="math inline">\(Prob(B|info)\)</span></p>
<p>As for the priors (knowledge about which door the prize is behind at the beginning of the game), let’s assign a uniform distribtuion: <span class="math inline">\(Prob(A) = Prob(B) = Prob(C) = 1/3\)</span></p>
<p>The data (<span class="math inline">\(info\)</span>) is that Monty opened door C to reveal a goat.</p>
<p>What is the likelihood of the data under hypothesis 1, that is that the prize is behind door A, <span class="math inline">\(Prob(info|A)\)</span>?</p>
<p>What is the likelihood of the data under hypothesis 2, that is that the prize is behind door B, <span class="math inline">\(Prob(info|B)\)</span>?</p>
<p>The last thing we need in order to compute the degree to which we can believe hypothesis A is true vs hypothesis B (or vice versa): we need to compute the denominator for Bayes theorem – in this case, <span class="math inline">\(Prob(info)\)</span>. One way to get this unconditional probability is to use the conditional probabilities of the data under each model, multiply each by the unconditional model probabilities (priors), and sum up across all possible models:</p>
<p><span class="math inline">\(Prob(info) = Prob(info|A)\cdot Prob(A) + Prob(info|B)\cdot Prob(B) + Prob(info|C)\cdot Prob(C)\)</span></p>
<p>(either A is true <em>or</em> B is true <em>or</em> C is true, and these possibilities are mutually exclusive).</p>
<p>So what’s our degree of belief about the prize being behind door A (“staying”)?</p>
<p>What’s our degree of belief about door B (“switching”)?</p>
<p>What should we do if we want to maximize our probability of winning the game??</p>
<p>Does this probability calculus convince you? Another way to convince yourself would be to simply simulate this result to convince ourselves that switching is the right move. Here is an R function for doing this:</p>
<pre class="r"><code>##### Monty Hall simulation code (code by Corey Chivers 2012)
#####################################################
# Simulation of the Monty Hall Problem
# Demonstrates that switching is always better
# than staying with your initial guess
#
# Corey Chivers, 2012
#####################################################
 
monty&lt;-function(strat=&#39;stay&#39;,N=1000,print_games=TRUE){
  doors&lt;-1:3 #initialize the doors behind one of which is a good prize
  win&lt;-0 #to keep track of number of wins
  
  for(i in 1:N){
    prize&lt;-floor(runif(1,1,4)) #randomize which door has the good prize
    guess&lt;-floor(runif(1,1,4)) #guess a door at random
    
    ## Reveal one of the doors you didn&#39;t pick which has a bum prize
    if(prize!=guess)
      reveal&lt;-doors[-c(prize,guess)]
    else
      reveal&lt;-sample(doors[-c(prize,guess)],1)
    
    ## Stay with your initial guess or switch
    if(strat==&#39;switch&#39;)
      select&lt;-doors[-c(reveal,guess)]
    if(strat==&#39;stay&#39;)
      select&lt;-guess
    if(strat==&#39;random&#39;)
      select&lt;-sample(doors[-reveal],1)
    
    ## Count up your wins
    if(select==prize){
      win&lt;-win+1
      outcome&lt;-&#39;Winner!&#39;
    }else
      outcome&lt;-&#39;Loser!&#39;
    
    if(print_games)
      cat(paste(&#39;Guess: &#39;,guess,
          &#39;\nRevealed: &#39;,reveal,
          &#39;\nSelection: &#39;,select,
          &#39;\nPrize door: &#39;,prize,
          &#39;\n&#39;,outcome,&#39;\n\n&#39;,sep=&#39;&#39;))
  }
  cat(paste(&#39;Using the &#39;,strat,&#39; strategy, your win percentage was &#39;,win/N*100,&#39;%\n&#39;,sep=&#39;&#39;)) #Print the win percentage of your strategy
}</code></pre>
<p>Now we can test out the different strategies.</p>
<pre class="r"><code>monty(strat=&quot;stay&quot;,print_games=FALSE)</code></pre>
<pre><code>## Using the stay strategy, your win percentage was 34.9%</code></pre>
<p>What if we had more prior information – say, there was a strong smell of goat coming from door A? The priors would be different ( <span class="math inline">\(Prob(A) &gt; Prob(B)\)</span> ). The likelihoods would remain exactly the same, but the posterior for A would be greater than 1/3. How strong would the odor have to be to convince you to stay rather than switch?</p>
<p>That’s one benefit of the Bayesian approach – you can integrate all the available information! Think about the XKCD comic above… Does it make sense that the Bayesian approach might have less of a tendency to produce “ridiculous” answers?</p>
</div>
<div id="probability-distributions" class="section level2">
<h2>Probability distributions</h2>
<div id="discrete-vs.continuous" class="section level3">
<h3>Discrete vs. continuous</h3>
<p>In <em>discrete distributions</em>, each outcome has a specific probability (like the probability of flipping a coin 10 times and getting 4 heads). For example, let’s consider the Poisson distribution</p>
<pre class="r"><code>mean &lt;- 5
rpois(10,mean)    # the random numbers have no decimal component</code></pre>
<pre><code>##  [1] 10  6  8  4  5  2  3  6  7  4</code></pre>
<pre class="r"><code>             # plot a discrete distribution!
xvals &lt;- seq(0,15,1)
probs &lt;- dpois(xvals,lambda=mean)
names(probs) &lt;- xvals
               
barplot(probs,ylab=&quot;Probability&quot;,main=&quot;Poisson distribution (discrete)&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>barplot(cumsum(probs),ylab=&quot;Cumulative Probability&quot;,main=&quot;Poisson distribution (discrete)&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
<pre class="r"><code>sum(probs)   # just to make sure it sums to 1!  Does it??? </code></pre>
<pre><code>## [1] 0.999931</code></pre>
<p>In <em>continuous distributions</em>, the height of the curve corresponds to <em>probability density</em>, <span class="math inline">\(f(x)\)</span>, not probability <span class="math inline">\(Prob(x)\)</span>. This is because the probability of getting exactly one value in a continuous distribution is effectively zero. This arises from the problem of precision. The sum of the probability distribution must be 1 (there is only 100% of probability to go around). In a continuous distribution, there are an infinite number of possible values of x. So any individual probability is always divided by infinity, which makes it zero. Therefore we have to talk about probability density, unless we want to specify a particular range of values – we can’t calculate <span class="math inline">\(Prob(x = 5)\)</span>, but we can calculate <span class="math inline">\(Prob(4 &lt; x &lt; 6)\)</span> or <span class="math inline">\(Prob(x &gt; 5)\)</span>. Let’s consider the beta distribution:</p>
<pre class="r"><code>shape1 = 0.5
shape2 = 0.5

rbeta(10,shape1,shape2)</code></pre>
<pre><code>##  [1] 0.04905359 0.90018409 0.88566342 0.97566484 0.70167072 0.29310416
##  [7] 0.92949169 0.25136098 0.21749407 0.02440683</code></pre>
<pre class="r"><code>curve(dbeta(x,shape1,shape2))   # probability density</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>curve(pbeta(x,shape1,shape2))   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dbeta,lower=0,upper=1,shape1=shape1,shape2=shape2)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 3e-06</code></pre>
<div id="some-other-probability-distribution-terms" class="section level4">
<h4>Some other probability distribution terms:</h4>
<p><strong>Moments</strong> – descriptions of the distribution. For a bounded probability distribution, the collection of all the moments (of all orders, from 0 to ∞) uniquely determines the distribution.</p>
<ul>
<li>The zeroth central moment (<span class="math inline">\(\int \left ( x-\mu \right )^{0}Prob(x)\partial x\)</span>) is the total probability (i.e. one),</li>
<li>The first central moment (<span class="math inline">\(\int \left ( x-\mu \right )^{1}Prob(x)\partial x\)</span>) is the mean.<br />
</li>
<li>The second central moment (<span class="math inline">\(\int \left ( x-\mu \right )^{2}Prob(x)\partial x\)</span>) is the variance.<br />
</li>
<li>The third central moment (<span class="math inline">\(\int \left ( \left (x-\mu \right )/\sigma \right )^{3}Prob(x)\partial x\)</span>) is the skewness.</li>
</ul>
<p><strong>Parameters</strong> – the values in the probability distribution function, describing the exact shape and location of the distribution. <em>Parametric statistics</em> require assuming certain things about distributions &amp; parameters, while <em>nonparametric stats</em> do not require these assumptions.</p>
</div>
</div>
<div id="some-probability-distributions" class="section level3">
<h3>Some probability distributions</h3>
<p>The Bolker book goes through the main distributions we will be using in this course. Pay particular attention to the type of <em>process</em> described by each distribution. The key to using these correctly is to figure out which statistical process best matches the ecological process you’re studying, then use that distribution. e.g., am I counting independent, random events occurring in a fixed window of time or space (like sampling barnacles in quadrats on an intertidal bench)? Then the distribution of their occurrence probably follows a Poisson distribution.</p>
<div id="binomial" class="section level4">
<h4>Binomial</h4>
<pre class="r"><code>size &lt;- 10
prob &lt;- 0.3
rbinom(10,size,prob)</code></pre>
<pre><code>##  [1] 1 5 5 6 1 7 3 2 6 2</code></pre>
<pre class="r"><code>xvals &lt;- seq(0,size,1)
probs &lt;- dbinom(xvals,size,prob)
names(probs) &lt;- xvals
               
barplot(probs,ylab=&quot;Probability&quot;,main=&quot;Binomial distribution&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>barplot(cumsum(probs),ylab=&quot;Cumulative Probability&quot;,main=&quot;Binomial distribution&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
<pre class="r"><code>sum(probs)   # just to make sure it sums to 1!  Does it???</code></pre>
<pre><code>## [1] 1</code></pre>
</div>
<div id="normal" class="section level4">
<h4>Normal</h4>
<pre class="r"><code>mean = 7.1
stdev = 1.9

rnorm(10,mean,stdev)</code></pre>
<pre><code>##  [1] 5.363801 8.800719 5.608123 5.868485 4.918817 5.767650 5.441302
##  [8] 7.841936 8.182000 5.990847</code></pre>
<pre class="r"><code>curve(dnorm(x,mean,stdev),0,15)   # probability density</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>curve(pnorm(x,mean,stdev),0,15)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dnorm,lower=-Inf,upper=Inf,mean=mean,sd=stdev)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 1.1e-05</code></pre>
</div>
<div id="exercise" class="section level4">
<h4>Exercise:</h4>
<p>Visualize the following distributions as above: Gamma, Exponential, Lognormal, Negative Binomial.</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
