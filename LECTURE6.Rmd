---
title: "Bayesian Analysis #1: Concepts"
author: "NRES 746"
date: "September 27, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

**Bayesian analysis** is also likelihood-based, and follows naturally from our previous discussions. The difference is that we are no longer interested in the maximum likelhood estimate and the properties of maximum likelhood estimators. We are now interested in computing our degree of believe in all possible values across parameter space. Effectively, we want a *probability distribution* that gives us a probability density (or mass) for all possible parameter combinations, that gives us our degree of belief in a particular model (in this case, the set of parameter values) given the observed data.    

### Play with binomial/beta (conjugate prior)

One of the most intuitive ways to get into Bayesian inference is to start with the binomial distribution. Let's imagine we know *N* (*N*, the number of independent trials, is fixed), but we want to estimate *p*. Let's assume we have no prior information, so that any value of p is equally likely.

#### Set the prior

To set the prior, let's assume a uniform distribution between 0 and 1:

```{r}

curve(dunif(x),ylim=c(0,2),col="red")

hist(runif(10000),freq=F,ylim=c(0,2),col="red")

```

An alternative way to specify this uniform (flat) prior is to use the beta distribution, with both shape parameters set to 1


```{r}
curve(dbeta(x,1,1),ylim=c(0,2),col="red")

hist(rbeta(10000,1,1),freq=F,ylim=c(0,2),col="red")

```

### Conjugate prior

Why choose the beta distribution here? The answer is that the beta is the **conjugate prior** for the *p* parameter in the binomial distribution. This makes Bayesian estimation easy, as we will see!

#### Definition: conjugate prior

A conjugate prior is a distribution that matches the data-generating model such that it has the same form as the likelihood function. In this way, the distributional form of the posterior distribution for a parameter is the same as the prior distribution for that parameter (although the shape of the distribution will change). We will come back to this!

### Worked example 

Let's work through an example. Let's imagine the same frog-call survey we have imagined before. We know the site is occupied. After visiting the site 10 times, we detected the frog (heard its call) 3 times out of 10. We are interested in determining the detection probability. 

We know the likelihood of the data across parameter space

```{r}
data = 3
param.space <- seq(0,1,by=0.001)
likelihood <- dbinom(data,size=10,prob=param.space)
par(mai=c(1,1,0,1))
curve(dbeta(x,1,1),ylim=c(0,2),col="blue",ylab="Probability density",xlab="param.space")
points(param.space,likelihood*5,type="l",col="red",lwd=2)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext("Likelihood", side=4, col="red",line=3)

```

Recall that the likelihood curve is NOT a probability distribution. It does not necessarily sum to 1! In Bayesian analyses, we translate the likelihood to a probability using Bayes rule!!

$Prob(Model|Data) = \frac{Prob(Data|Model)\cdot Prob(Model))}{Prob(Data)}$

The likelihood is just the $Prob(Data|Model)$ term...

What is the probability of the data? Well, it's just the sum of the probability of the data across parameter space. Really, $Prob(Data)$ can be seen as a normalizing constant that is used to convert the numerator of Bayes rule into a probability distribution. Let's do it first by brute force...

```{r}
prior <- dbeta(param.space,shape1=1,shape2=1)
#prior

## weight the data likelihood by the prior

weighted.likelihood <- likelihood*prior      # Numerator for Bayes rule

## compute normalization constant

normalization.constant <- sum(weighted.likelihood)

## Posterior!!
posterior <- weighted.likelihood/normalization.constant   # this is Bayes' rule!

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,5),type="l",lwd=1,lty=2,col="blue",ylab="Probability Density",xlab="param.space")
points(param.space,posterior*length(param.space),type="l",col="blue",lwd=2,lty=1)  # convert posterior to probability density
points(param.space,likelihood*5,type="l",col="red",lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext("Likelihood", side=4, col="red",line=3)

```

Notice that the shape of the posterior looks a lot like the shape of the likelhood surface. What this says to us is that the prior has been *overwhelmed* by the information content of the data (as summarized by the likelihood surface).

What if we have a more informative prior?

```{r}
prior <- dbeta(param.space,shape1=15,shape2=5)
#prior

## weight the data likelihood by the prior

weighted.likelihood <- likelihood*prior

## compute normalization constant

normalization.constant <- sum(weighted.likelihood)

## Posterior!!

posterior <- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,5),type="l",lwd=1,lty=2,col="blue",ylab="Probability Density",xlab="param.space")
points(param.space,posterior*length(param.space),type="l",col="blue",lwd=2,lty=1)
points(param.space,likelihood*5,type="l",col="red",lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext("Likelihood", side=4, col="red",line=3)
```

What does this tell us?

What about a super informative prior??

```{r}
prior <- dbeta(param.space,shape1=150,shape2=50)
#prior

## weight the data likelihood by the prior

weighted.likelihood <- likelihood*prior

## compute normalization constant

normalization.constant <- sum(weighted.likelihood)

## Posterior!!

posterior <- weighted.likelihood/normalization.constant

## Plot it out!
par(mai=c(1,1,0,1))
plot(param.space,prior,ylim=c(0,15),type="l",lwd=1,lty=2,col="blue",ylab="Probability Density",xlab="param.space")
points(param.space,posterior*length(param.space),type="l",col="blue",lwd=2,lty=1)
points(param.space,likelihood*5,type="l",col="red",lwd=1)
axis(4,at=seq(0,2,by=0.4),labels = seq(0,0.5,by=.1))
mtext("Likelihood", side=4, col="red",line=3)
```


Okay, now let's do it the more mathematically elegant way! When we work with a conjugate prior, the updating process is easy. The posterior distribution for the *p* term in the above example can be computed by:

$Beta(shape1=prior+k,shape2=prior+(N-k))$

Let's do the same thing, now using the conjugate prior method...

```{r}

### PRIOR

curve(dbeta(x,1,1),ylim=c(0,5),ylab="Prob Density",col="blue",lwd=1,lty=2,xlab="param.space")

### POSTERIOR

curve(dbeta(x,1+data,1+(10-data)),ylim=c(0,4),ylab="Prob Density",col="blue",lwd=2,lty=1,xlab="param.space",add=T)

```

And again, this time with an informative prior!

```{r}
### PRIOR

curve(dbeta(x,15,5),ylim=c(0,5),ylab="Prob Density",col="blue",lwd=1,lty=2,xlab="param.space")

### POSTERIOR

curve(dbeta(x,15+data,5+(10-data)),ylim=c(0,4),ylab="Prob Density",col="blue",lwd=2,lty=1,xlab="param.space",add=T)
```

```{r echo=FALSE}
graphics.off()

```

And the super informative prior??

```{r}
### PRIOR

curve(dbeta(x,150,50),ylim=c(0,15),ylab="Prob Density",col="blue",lwd=1,lty=2,xlab="param.space")

### POSTERIOR

curve(dbeta(x,150+data,50+(10-data)),ylim=c(0,4),ylab="Prob Density",col="blue",lwd=2,lty=1,xlab="param.space",add=T)

```

```{r echo=FALSE}
graphics.off()

```

### Bayesian point estimate

One of the differences between the MLE and the Bayesian paradigm (although both use likelihood as a way to summarize the information content of the data) is that the point estimate is not the maximum of the posterior distribution (in MLE, we by definition try to find the parameter value that maximizes the likelihood function) but the **mean** of the posterior distribution. That is, it is the expected value of that parameter... 

Imagine you had a skewed posterior distribution that looked something like this:

```{r}
curve(dlnorm(x,4,1),from=0.001,to=200,ylab="prob density")

```

Where is the mode? Where is the mean?

```{r}
param.space2 <- seq(0.001,200,length=10000)
skewed.posterior <- dlnorm(param.space2,4,1)
mean <- mean(rlnorm(10000,4,1))
mode <- param.space2[which.max(skewed.posterior)]
plot(param.space2,skewed.posterior,type="l",ylab="prob density")
abline(v=c(mean,mode),col=gray(0.5),lwd=3,lty=2)   # add to plot
```

What about for the other example? Is there a big difference between the mean and the mode?

```{r}
graphics.off()
### POSTERIOR
posterior <- dbeta(param.space,1+data,1+(10-data))
mean <- mean(rbeta(10000,1+data,1+(10-data)))
mode <- param.space[which.max(posterior)]
plot(param.space,posterior,type="l",col="blue",lwd=2)
abline(v=c(mean,mode),col=gray(0.5),lwd=3,lty=2)   # add to plot
```

```{r echo=FALSE}
graphics.off()

```

#### Discussion question

In MLE, we by definition try to find the parameter value that maximizes the likelihood function. Why don't we use the mean of the likelihood function? In Bayesian analysis, why don't we use the mode of the posterior distribution?

### Bayesian parameter uncertainty

We often call Bayesian confidence intervals *credible intervals* to distinguish from their frequentist analog. Bayesian (e.g., 95%) credible intervals can be interpreted in the way you probably have always wanted to interpret frequentist (95%) confidence intervals. It will probably feel satisfying, but a little dirty at the same time! 

> You are 95% sure that the true parameter value is between the lower and upper bound!!!

Let's try this: 

```{r}

### POSTERIOR

curve(dbeta(x,1+data,1+(10-data)),ylim=c(0,4),ylab="Prob Density",col="blue",lwd=2,lty=1,xlab="param.space")

### CREDIBLE INTERVAL

credible.interval <- qbeta(c(0.025,0.975),1+data,1+(10-data))     # get the credible interval

abline(v=credible.interval,col=gray(0.5),lwd=3,lty=2)   # add to plot

```

```{r echo=FALSE}
graphics.off()

```


### What if there is no nice easy conjugate prior?

One of the reasons Bayesian analysis was less common historically was that there were no mathematically straightforward ways to do the analysis. *There still are not* BUT we have fast computers and computational algorithms. Basically, we can use various forms of more-or-less brute force computation to do Bayesian analyses.

Let's start with the most brute of brute-force methods:

### The brute force method

For continuity we will continue the myxomatosis dataset. Recall that we are estimating the shape and scale parameters of the Gamma distribution that describes the virus titer in Australian rabbits for the lowest-grade infections. 

```{r}
library(emdbook)

MyxDat <- MyxoTiter_sum
Myx <- subset(MyxDat,grade==1)
head(Myx)
```

Recall the histogram looks like this:

```{r}
hist(Myx$titer,freq=FALSE)
```


And we are trying to fit a Gamma distribution to these data:

```{r}
hist(Myx$titer,freq=FALSE)
curve(dgamma(x,shape=40,scale=0.15),add=T,col="red")
```


We already have a likelihood function for this problem! Note that we are now looking at real likelihoods and not log likelihoods! 

```{r}
GammaLikelihoodFunction <- function(params){
  prod(dgamma(Myx$titer,shape=params['shape'],scale=params['scale']))
}

params <- c(40,0.15) 
names(params) <- c("shape","scale")
params
GammaLikelihoodFunction(params)

```

And we recall that the 2D likelihood surface looks something like this:

```{r echo=FALSE}
##############
# define 2-D parameter space!
##############

shapevec <- seq(10,100,by=0.1)   
scalevec <- seq(0.01,0.3,by=0.001)

##############
# define the likelihood surface across this grid within parameter space
##############

likelihood2D <- matrix(nrow=length(shapevec),ncol=length(scalevec))   # initialize storage variable

newparams <- params
for(i in 1:length(shapevec)){
  newparams['shape'] <- shapevec[i]
  for(j in 1:length(scalevec)){
    newparams['scale'] <- scalevec[j]
    likelihood2D[i,j] <- GammaLikelihoodFunction(newparams) 
  }
}

############
# Visualize the likelihood surface
############

image(x=shapevec,y=scalevec,z=likelihood2D,zlim=c(1e-70,1e-17),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=likelihood2D,levels=c(1e-18,1e-17),add=T)

```


Now let's use this 2-D likelihood surface as a jumping-off point for a brute-force Bayesian solution to this problem!

First we need to set priors for the shape and scale parameters... For example, we could set uniform distributions for these parameters. For this example, let's imagine a prior in which all pixels in the above image are equally likely:

```{r}

pixelArea <- 0.0001  # for determining probability densities

##############
# define the prior probability surface across this grid within parameter space
##############

prior2D <- matrix(1, nrow=length(shapevec),ncol=length(scalevec))   # initialize prior
prior2D <- prior2D/length(prior2D)

############
# Visualize the 2-D prior distribution
############

image(x=shapevec,y=scalevec,z=prior2D,zlim=c(0,0.001),col=rainbow(10))



```

Okay, not very interesting! 

But now we have the raw information we need to apply Bayes' rule!

```{r}

weighted.likelihood <- prior2D * likelihood2D    # numerator of Bayes rule
normalization.constant <- sum(weighted.likelihood)    # denominator of Bayes rule

posterior2D <- weighted.likelihood/normalization.constant

############
# Visualize the 2-D posterior distribution
############

image(x=shapevec,y=scalevec,z=(posterior2D/pixelArea),zlim=c(0,5),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=(posterior2D/pixelArea),levels=c(1:4),add=T,drawlabels=FALSE)

```

Now we can use this posterior distribution to get our point estimates and parameter uncertainty estimates. We could just take the 2d posterior distribution surface and draw the contour containing 95% of the probability density:

First let's find the contour line below which only 5% of the probability density is contained:

```{r}
possible.contours <- data.frame(contour = seq(0.13e-4,1e-4,length=100), quantile = NA)
i=1
for(i in 1:nrow(possible.contours)){
  ndx <- which(posterior2D<possible.contours$contour[i],arr.ind = T)
  possible.contours$quantile[i] <- sum(posterior2D[ndx])
}

head(possible.contours,10)
```

From here we can see that the posterior probability 1.739394e-05 encloses 95% of the probability density

```{r}

q95 <- 1.739394e-05
image(x=shapevec,y=scalevec,z=posterior2D,zlim=c(0.5e-11,5e-4),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=posterior2D,levels=q95,add=T,lwd=3,col="red",drawlabels=FALSE)

```

Where is our point estimate? Let's find the posterior mean and mode!

```{r}
image(x=shapevec,y=scalevec,z=posterior2D,zlim=c(0.5e-11,5e-4),col=topo.colors(12))
contour(x=shapevec,y=scalevec,z=posterior2D,levels=q95,add=T,lwd=3,col="red",drawlabels=FALSE)
meanshape <- sum(shapevec*posterior2D) 
meanscale <- sum(scalevec*posterior2D)
points(meanshape,meanscale,pch=20,cex=2,col="red")

```



We can also plot out the marginal posterior distributions for the parameters separately. For example, the probability that the shape parameter falls within a given range, regardless of the other variable:

```{r}

marginal.dist.shape <- apply(posterior2D,1,mean)
plot(shapevec,(marginal.dist.shape/sum(marginal.dist.shape))/0.1,type="l",lwd=2,col="blue",ylab="probability density",main="Posterior probability")
abline(v=meanshape)

marginal.dist.scale <- apply(posterior2D,2,mean)
plot(scalevec,(marginal.dist.scale/sum(marginal.dist.scale))/0.001,type="l",lwd=2,col="blue",ylab="probability density",main="Posterior probability")
abline(v=meanscale)

meanshape
meanscale
```

And from here we can compute the posterior mean and Bayesian credible intervals. 

How do our Bayesian estimates compare with the maximum likelihood estimates?? (shape = 49.3666607 scale = 0.1402629)


### Bayes Factor



### Markov Chain Monte Carlo

Now in many cases, we simply won't have the computational power to partition our parameter space into discrete pixels and completely evaluate the posterior probability for all *n*-dimensional pixels in that space. In these cases, we tend to harness ingenious algorithms known as Markov-Chain Monte Carlo
















